{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33e74357-703f-4a47-b365-74e6beac612b",
   "metadata": {},
   "source": [
    "# Tutorial 1: Loading and Sampling Trajectory Data\n",
    "\n",
    "Real-world mobility files vary widely in structure and formatting. Timestamps may be recorded as UNIX integers or ISO-formatted strings, with or without timezone offsets. Coordinate columns may follow different naming conventions, and files may be stored either as flat CSVs or as partitioned Parquet directories. This notebook demonstrates how `nomad.io.base` standardizes data loading across these variations using two example datasets: a CSV file (`gc-data.csv`) and a partitioned Parquet directory (`gc-data/`).\n",
    "\n",
    "## Inspecting schemas\n",
    "Let's start by inspecting the schemas of the datasets we will use with the nomad helper function `table_columns` from the `io` module. This method reports column names for both flat files and partitioned datasets without reading the full content into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d2a70-4f52-44e9-8e8f-106426addea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nomad.io import base as loader\n",
    "\n",
    "print(loader.table_columns(\"gc-data.csv\", format=\"csv\"))\n",
    "print(loader.table_columns(\"gc-data/\", format=\"parquet\")) # <<< SHOULD BE A PARTITIONED CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f802bd-950f-4c64-a309-766590d7e4e9",
   "metadata": {},
   "source": [
    "## Loading data \n",
    "\n",
    "Reading data with `pandas` or Parquet readers does not enforce any particular schema, but spatiotemporal data often contains columns that must follow specific formats. The `from_file` function applies consistent type casting, converting temporal fields to `datetime` objects, ensuring coordinates are floats, unix timestamps are integers, and optionally creating a `tz_offset` column to store timezone offsets when parsing datetime strings. This enables compatibility with engines like Spark, in which `Timestamp` objects cannot store timezone information.\n",
    "\n",
    "To make reproducing the code as easy as possible, we want to abstract away the different possible column names, understanding that in most cases we get the same columns. While renaming is an option in some cases, `nomad` can handle different column names in most methods by simply storing a a `traj_cols` dictionary mapping default column names to the actual column names in the dataset.This also allows downstream functions to know where to find required spatial, temporal, or even tessellation columns without excessive argument-passing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf535a-b1f7-41e2-993c-56c40c768b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_cols = {\"user_id\": \"identifier\", \"timestamp\": \"unix_timestamp\", \"latitude\": \"device_lat\", \"longitude\": \"device_lon\", \"datetime\": \"local_datetime\", \"date\": \"date\"}\n",
    "df_mapped = loader.from_file(\"gc-data.csv\", format=\"csv\", traj_cols=traj_cols)\n",
    "df_mapped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b27b36-605c-40c5-83b4-adb30c58d594",
   "metadata": {},
   "source": [
    "This mapping makes the dataset compatible with nomad tools without modifying its original structure. However, in the case in which a dataset has the default names, possibly due to the columns being renamed, many `nomad` methods will work without passing any mappings or excessive arguments. After inspecting the default column names, we see that the second dataset uses those, and thus the casting of column types and parsing of dates identifies (and is applied) on the appropriate columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341ed5b6-b375-4069-9a51-0f6d6ef069e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nomad.constants import DEFAULT_SCHEMA\n",
    "DEFAULT_SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d82bfd-b126-4346-81f7-e0b738ccfe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset has default column names, so no traj_cols argument is necessary\n",
    "df_pq = loader.from_file(\"gc-data/\", format=\"parquet\", parse_dates=True)\n",
    "df_pq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69253a5-5ed2-4bf5-b9a4-ad21f89f4898",
   "metadata": {},
   "source": [
    "Even when GPS data is stored in partitioned directories (e.g. date=2024-01-01/), `from_file` can handle it using PyArrow's file reader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbfe5f4-c481-4816-8136-bca26e48cf3c",
   "metadata": {},
   "source": [
    "## Working on smaller samples and persistence\n",
    "\n",
    "Large mobility datasets should typically not be fully loaded into the memory of a machine during interactive analysis, so subsampling by user is a common step in early analyses. nomad's `sample_users` selects a reproducible subset of user IDs, and `sample_from_file` filters the input dataset to include only those records. The resulting sample can be written to disk using `to_file`, partitioned by date in `hive` format to preserve compatibility with distributed engines. Reading the output back with `from_file` confirms that the sample was saved correctly and remains compatible with the same loading functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565115f0-1e3c-4b34-8eca-3abeb1695f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users = loader.sample_users(\"gc-data/\", format=\"parquet\", size=10, seed=300)\n",
    "sample_df = loader.sample_from_file(\"gc-data/\", users=users, format=\"parquet\", frac_records=0.25)\n",
    "\n",
    "loader.to_file(sample_df, \"/tmp/nomad_sample\", format=\"parquet\", partition_by=[\"date\"], existing_data_behavior='delete_matching')\n",
    "\n",
    "round_trip = loader.from_file(\"/tmp/nomad_sample\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15ae992-931c-484e-abd2-dc5e096a3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The amount of data in the working sample is much smaller, and sufficient for prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd42c69-c932-428e-ab9c-30b46a1aecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- Value counts for sample of data:\\n\")\n",
    "print(round_trip.user_id.value_counts())\n",
    "print(\"\\n---------------------------------\\n\")\n",
    "print(\"- Value counts for original data:\\n\")\n",
    "print(df_pq.user_id.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bb6f24-e1d9-492b-8164-1e7c047ec8d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "conda_py_310_env"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      },
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "conda_py_310_env"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    },
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 5,
           "op": "addrange",
           "valuelist": "6"
          },
          {
           "key": 5,
           "length": 2,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3 (ipykernel)"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      },
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "python3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    },
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
