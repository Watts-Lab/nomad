{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e43946-4af5-4cfe-8b33-21aa16a5cbd0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Outline\n",
    "explain how nomad can address different challenges in the data ingestion process:\n",
    "1) the data can be partitioned, complicating things for users most familiar with pandas, a simple wrapper function from_file simplifies things, same function for simple csvs or partitioned data\n",
    "2) column names and formats might vary from dataset to dataset, but spatial analysis functions require spatial and temporal columns, sometimes the time is expected as a unix timestamp, sometimes as a datetime with the local timezone. Similarly, an algorithm might require the latitude and longitude. Users always have the alternative of renaming the data so that those column names match the defaults of those functions, or they can input the right column names (or pass the relevant columns) on functions that have such flexibility. Nothing wrong with that. However, it could be preferrable to not alter the data permanently for analysis, specially if one will perform some filtering or produce a derivative dataset that is expected to be joined later on with the original data. Passing the correct column names every time to processing functions can be burdensome and verbose, and makes code less reusable when applied to a dataset with different column names. nomad addresses this by using an auxiliary dictionary of column names which other processing methods can use to find the appropriate columns. This is somewhat equivalent to passing the column names as keyword arguments, but functions also have a fallback to default column names for certain expected columns (like latitude, longitude, user_id, timestamp, etc).\n",
    "3) We can demonstrate the flexibiilty that this auxiliary dictionary offers, by loading some device-level data fond in `gc-data.csv`. Beyond being a wrapper for the pandas or pyarrow reader functions, the `io` reader method, `from_file`, also ensures the trajectory columns (coordinates and time columns) are cast to the correct data types, issues warnings when unix timestamps are not in seconds, and raises errors when the data seems to lack spatial or temporal columns that will likely be required in downstream processing. This can be evidenced by comparing the output of simply using `pandas.read_csv` with that of `from_file`, where we see that the right columns have been cast to the right data types:\n",
    "\n",
    "4) Of particular importance is the standardized handling of datetime strings in iso8601 formats. These can be timezone naive, have timezone information, and even have mixed timezones. For instance, when a trajectory spans multiple regions, or when there are daylight savings changes. nomad tries to simplify the parsing of dates in such cases, with three cases: [code explaining]\n",
    "\n",
    "5) This last case is important because distributed algorithms relying on Spark do not store timezone information in the timestamp format. This presents a challenge in which analysis related to local datetime is required, but this information is lost. Switching to utc time is always an option which makes naive datetimes comparable, but it makes analysis of day-time, night-time behaviors more complicated when there are mixed timezones. A standard way to deal with timezone data is to strip the timezone information from timestamps and represent it in a separate column as the offset from UTC time in seconds. Thus, for compatibility with Spark workflows, setting `mixed_timezone_behavior = \"naive\"` will create a `tz_offset` column (when one does not already exist).\n",
    "\n",
    "6) The flexibility provided by nomad to easily switch between small analyses using a small example of data, which could be stored in a .csv file, for testing code, and then using the same (or similar functions) to scale up in a distributed environment, facilitates a common (and recommended) workflow in which users can easily read data from some users from a large dataset and use standard pandas functionalities, benchmark their code, etc, and then scale up using more resources once they are certain their code is in good shape. This can easily be done with io methods like `sample_users`, `sample_from_file` (which may optionally take a sample of users drawn from somewhere else). This is shown as follows:\n",
    "\n",
    "7) Finally, a user might want to persist such a sample with care for the data types and, perhaps, recovering the date string format with timezone, which is possible even when this information was saved in the tz_offset column. Notice that this writer function can also seamlessly switch between csv and parquet formats, leveraging pyarrow and pandas. FOr example: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e74357-703f-4a47-b365-74e6beac612b",
   "metadata": {},
   "source": [
    "# Tutorial 1: Loading and Sampling Trajectory Data\n",
    "\n",
    "Real-world mobility files vary widely in structure and formatting. Timestamps may be recorded as UNIX integers or ISO-formatted strings, with or without timezone offsets. Coordinate columns may follow different naming conventions, and files may be stored either as flat CSVs or as partitioned Parquet directories. This notebook demonstrates how `nomad.io.base` standardizes data loading across these variations using two example datasets: a CSV file (`gc-data.csv`) and a partitioned Parquet directory (`gc-data/`).\n",
    "\n",
    "## Inspecting schemas\n",
    "Let's start by inspecting the schemas of the datasets we will use with the nomad helper function `table_columns` from the `io` module. This method reports column names for both flat files and partitioned datasets without reading the full content into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4d2a70-4f52-44e9-8e8f-106426addea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['identifier', 'unix_timestamp', 'device_lat', 'device_lon', 'date',\n",
      "       'offset_seconds', 'local_datetime'],\n",
      "      dtype='object')\n",
      "Index(['user_id', 'timestamp', 'latitude', 'longitude', 'tz_offset',\n",
      "       'datetime', 'date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nomad.io import base as loader\n",
    "\n",
    "print(loader.table_columns(\"gc-data.csv\", format=\"csv\"))\n",
    "print(loader.table_columns(\"gc-data/\", format=\"parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f802bd-950f-4c64-a309-766590d7e4e9",
   "metadata": {},
   "source": [
    "## Loading data \n",
    "\n",
    "Reading data with `pandas` or Parquet readers does not enforce any particular schema, but spatiotemporal data often contains columns that must follow specific formats. The `from_file` function applies consistent type casting, converting temporal fields to `datetime` objects, ensuring coordinates are numeric, and optionally creating a `tz_offset` column to store timezone offsets when parsing datetime strings. This enables compatibility with engines like Spark, in which `Timestamp` objects cannot store timezone information. When column names differ from expected defaults, `from_file` accepts a `traj_cols` dictionary that maps standard names to the datasetâ€™s column names, allowing downstream functions to locate required fields without renaming or altering the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eacf535a-b1f7-41e2-993c-56c40c768b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>unix_timestamp</th>\n",
       "      <th>device_lat</th>\n",
       "      <th>device_lon</th>\n",
       "      <th>date</th>\n",
       "      <th>offset_seconds</th>\n",
       "      <th>local_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wizardly_joliot</td>\n",
       "      <td>1704119340</td>\n",
       "      <td>38.321711</td>\n",
       "      <td>-36.667334</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-01 14:29:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wizardly_joliot</td>\n",
       "      <td>1704119700</td>\n",
       "      <td>38.321676</td>\n",
       "      <td>-36.667365</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-01 14:35:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wonderful_swirles</td>\n",
       "      <td>1704121560</td>\n",
       "      <td>38.321017</td>\n",
       "      <td>-36.667869</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>-7200</td>\n",
       "      <td>2024-01-01 13:06:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>youthful_galileo</td>\n",
       "      <td>1704098820</td>\n",
       "      <td>38.321625</td>\n",
       "      <td>-36.666612</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-01 08:47:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>youthful_galileo</td>\n",
       "      <td>1704103140</td>\n",
       "      <td>38.321681</td>\n",
       "      <td>-36.666841</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-01 09:59:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          identifier  unix_timestamp  device_lat  device_lon        date  \\\n",
       "0    wizardly_joliot      1704119340   38.321711  -36.667334  2024-01-01   \n",
       "1    wizardly_joliot      1704119700   38.321676  -36.667365  2024-01-01   \n",
       "2  wonderful_swirles      1704121560   38.321017  -36.667869  2024-01-01   \n",
       "3   youthful_galileo      1704098820   38.321625  -36.666612  2024-01-01   \n",
       "4   youthful_galileo      1704103140   38.321681  -36.666841  2024-01-01   \n",
       "\n",
       "   offset_seconds      local_datetime  \n",
       "0               0 2024-01-01 14:29:00  \n",
       "1               0 2024-01-01 14:35:00  \n",
       "2           -7200 2024-01-01 13:06:00  \n",
       "3               0 2024-01-01 08:47:00  \n",
       "4               0 2024-01-01 09:59:00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj_cols = {\n",
    "    \"user_id\": \"identifier\",\n",
    "    \"timestamp\": \"unix_timestamp\",\n",
    "    \"latitude\": \"device_lat\",\n",
    "    \"longitude\": \"device_lon\",\n",
    "    \"datetime\": \"local_datetime\",\n",
    "    \"tz_offset\": \"offset_seconds\",\n",
    "    \"date\": \"date\"\n",
    "}\n",
    "df_mapped = loader.from_file(\"gc-data.csv\", format=\"csv\", traj_cols=traj_cols)\n",
    "df_mapped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b27b36-605c-40c5-83b4-adb30c58d594",
   "metadata": {},
   "source": [
    "This mapping makes the dataset compatible with nomad tools without modifying its original structure. Algorithms expecting standard names like timestamp, latitude, or user_id will work correctly, thanks to the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61d82bfd-b126-4346-81f7-e0b738ccfe21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>tz_offset</th>\n",
       "      <th>datetime</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wizardly_joliot</td>\n",
       "      <td>1704119340</td>\n",
       "      <td>38.321711</td>\n",
       "      <td>-36.667334</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-01 14:29:00</td>\n",
       "      <td>2024-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wizardly_joliot</td>\n",
       "      <td>1704119700</td>\n",
       "      <td>38.321676</td>\n",
       "      <td>-36.667365</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-01 14:35:00</td>\n",
       "      <td>2024-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wonderful_swirles</td>\n",
       "      <td>1704121560</td>\n",
       "      <td>38.321017</td>\n",
       "      <td>-36.667869</td>\n",
       "      <td>-7200</td>\n",
       "      <td>2024-01-01 13:06:00</td>\n",
       "      <td>2024-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>youthful_galileo</td>\n",
       "      <td>1704098820</td>\n",
       "      <td>38.321625</td>\n",
       "      <td>-36.666612</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-01 08:47:00</td>\n",
       "      <td>2024-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>youthful_galileo</td>\n",
       "      <td>1704103140</td>\n",
       "      <td>38.321681</td>\n",
       "      <td>-36.666841</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-01-01 09:59:00</td>\n",
       "      <td>2024-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_id   timestamp   latitude  longitude  tz_offset  \\\n",
       "0    wizardly_joliot  1704119340  38.321711 -36.667334          0   \n",
       "1    wizardly_joliot  1704119700  38.321676 -36.667365          0   \n",
       "2  wonderful_swirles  1704121560  38.321017 -36.667869      -7200   \n",
       "3   youthful_galileo  1704098820  38.321625 -36.666612          0   \n",
       "4   youthful_galileo  1704103140  38.321681 -36.666841          0   \n",
       "\n",
       "             datetime        date  \n",
       "0 2024-01-01 14:29:00  2024-01-01  \n",
       "1 2024-01-01 14:35:00  2024-01-01  \n",
       "2 2024-01-01 13:06:00  2024-01-01  \n",
       "3 2024-01-01 08:47:00  2024-01-01  \n",
       "4 2024-01-01 09:59:00  2024-01-01  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This dataset has default column names, so no traj_cols argument is necessary\n",
    "df_pq = loader.from_file(\"gc-data/\", format=\"parquet\", parse_dates=True)\n",
    "df_pq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69253a5-5ed2-4bf5-b9a4-ad21f89f4898",
   "metadata": {},
   "source": [
    "Even when GPS data is stored in partitioned directories (e.g. date=2024-01-01/), from_file seamlessly handles it, allowing users familiar with Pandas to simplify the inspection of partitioned datasets in parquet formats without worrying about data casting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbfe5f4-c481-4816-8136-bca26e48cf3c",
   "metadata": {},
   "source": [
    "## Working on smaller samples and persistence \n",
    "Large mobility datasets should typically not be fully loaded into the memory of a machine during preliminary analysis, so subsampling by user is a common step in early analyses. nomad's `sample_users` selects a reproducible subset of user IDs, and `sample_from_file` filters the input dataset to include only those records. The resulting sample can be written to disk using `to_file`, partitioned by date in `hive` format to preserve compatibility with distributed engines. Reading the output back with `from_file` confirms that the sample was saved correctly and remains compatible with the same loading functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "854bee03-b4ee-4812-87f4-03140a5f3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"gc-data/\"\n",
    "output_path = \"/tmp/nomad_sample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "565115f0-1e3c-4b34-8eca-3abeb1695f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uid_col' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m users \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39msample_users(data_path, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m sample_df \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43musers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m loader\u001b[38;5;241m.\u001b[39mto_file(sample_df, output_path,\n\u001b[0;32m      7\u001b[0m                \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m                partition_by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      9\u001b[0m                existing_data_behavior\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelete_matching\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m loader\u001b[38;5;241m.\u001b[39mfrom_file(output_path, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\Documents\\repositories\\nomad-repo\\nomad\\io\\base.py:925\u001b[0m, in \u001b[0;36msample_from_file\u001b[1;34m(filepath, users, format, traj_cols, frac_users, frac_records, seed, parse_dates, mixed_timezone_behavior, fixed_format, **kwargs)\u001b[0m\n\u001b[0;32m    922\u001b[0m         df \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mto_table()\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    924\u001b[0m         df \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mto_table(\n\u001b[1;32m--> 925\u001b[0m             \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39mds\u001b[38;5;241m.\u001b[39mfield(\u001b[43muid_col\u001b[49m)\u001b[38;5;241m.\u001b[39misin(users), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(column_names)\n\u001b[0;32m    926\u001b[0m         )\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;241m<\u001b[39m frac_records \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m    929\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39mfrac_records, random_state\u001b[38;5;241m=\u001b[39mseed)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'uid_col' is not defined"
     ]
    }
   ],
   "source": [
    "users = loader.sample_users(data_path, format=\"parquet\", size=40)\n",
    "sample_df = loader.sample_from_file(data_path,\n",
    "                                    users=users,\n",
    "                                    format=\"parquet\")\n",
    "\n",
    "loader.to_file(sample_df, output_path,\n",
    "               format=\"parquet\",\n",
    "               partition_by=[\"date\"],\n",
    "               existing_data_behavior='delete_matching')\n",
    "\n",
    "loader.from_file(output_path, format=\"parquet\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd42c69-c932-428e-ab9c-30b46a1aecf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "conda_py_310_env"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      },
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "conda_py_310_env"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    },
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 5,
           "op": "addrange",
           "valuelist": "6"
          },
          {
           "key": 5,
           "length": 2,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3 (ipykernel)"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      },
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "python3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    },
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
