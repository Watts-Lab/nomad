{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9fd0c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-muted')\n",
    "from matplotlib import cm\n",
    "import geopandas as gpd\n",
    "\n",
    "from pyproj import Transformer\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import numpy.random as npr\n",
    "import matplotlib.dates as mdates\n",
    "from itertools import product\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nomad.io.base as loader\n",
    "import nomad.city_gen as cg\n",
    "from nomad.city_gen import City, Building\n",
    "import nomad.traj_gen as tg\n",
    "from nomad.traj_gen import Agent, Population\n",
    "import nomad.stop_detection.ta_dbscan as DBSCAN\n",
    "import nomad.stop_detection.lachesis as Lachesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79a7061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  type\n",
      "0  p-x13-y11  park\n",
      "1    h-x8-y8  home\n",
      "2    h-x9-y8  home\n",
      "3   h-x10-y8  home\n",
      "4   h-x11-y8  home\n"
     ]
    }
   ],
   "source": [
    "city_geojson = gpd.read_file('garden_city.geojson')\n",
    "city = cg.load('garden-city.pkl')\n",
    "print(city.building_types.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22f6e1",
   "metadata": {},
   "source": [
    "### Generate N agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af174e5",
   "metadata": {},
   "source": [
    "The following code maps our Garden City coordinates to a location in the Atlantic Ocean (Atlantis?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b6dc6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def garden_city_to_lat_long(agent, sparse_traj=True, full_traj=False, diaries=True):\n",
    "    def project_city_blocks_to_web_mercator(df):\n",
    "        \"\"\"Convert (x, y) from 15m block units to Web Mercator meters via affine shift and projection.\"\"\"\n",
    "        transformer = Transformer.from_crs(\"EPSG:3857\", \"EPSG:4326\", always_xy=True)\n",
    "        df['x'] = 15 * df['x'] - 4265699\n",
    "        df['y'] = 15 * df['y'] + 4392976\n",
    "        if 'ha' in df:\n",
    "            df['ha'] = 15 * df['ha']\n",
    "        df['longitude'], df['latitude'] = transformer.transform(df['x'].values, df['y'].values)\n",
    "        if 'datetime' in df.columns:\n",
    "            df['date'] = df['datetime'].dt.date\n",
    "        elif 'local_timestamp' in df.columns:\n",
    "            df['date'] = df['local_timestamp'].dt.date\n",
    "        return df\n",
    "\n",
    "    def finalize(df):\n",
    "        front = ['identifier', 'unix_timestamp', 'longitude', 'latitude', 'x', 'y', 'date']\n",
    "        cols = [col for col in front if col in df] + [col for col in df.columns if col not in front]\n",
    "        return df[cols].rename(columns={'identifier': 'uid', 'unix_timestamp': 'timestamp'}).reset_index(drop=True)\n",
    "\n",
    "    if sparse_traj:\n",
    "        agent.sparse_traj = finalize(project_city_blocks_to_web_mercator(agent.sparse_traj))\n",
    "    if full_traj:\n",
    "        agent.trajectory = finalize(project_city_blocks_to_web_mercator(agent.trajectory))\n",
    "        \n",
    "    if diaries:\n",
    "        diary = agent.diary.copy()\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for loc in diary[\"location\"]:\n",
    "            if loc is None:\n",
    "                xs.append(None)\n",
    "                ys.append(None)\n",
    "            else:\n",
    "                pt = agent.city.buildings[loc].geometry.centroid\n",
    "                xs.append(pt.x)\n",
    "                ys.append(pt.y)\n",
    "        diary[\"x\"] = xs\n",
    "        diary[\"y\"] = ys\n",
    "        agent.diary = finalize(project_city_blocks_to_web_mercator(diary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a5abe",
   "metadata": {},
   "source": [
    "Initiate $N$ empty agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60fd6023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 300  # can modify\n",
    "\n",
    "population = Population(city)\n",
    "population.generate_agents(N=N, seed=1, name_count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27199b",
   "metadata": {},
   "source": [
    "## Simple trajectory generation\n",
    "\n",
    "For simple trajectory generation tasks that don't require too much computation power and can be done on a personal laptop, the following code generates a trajectory for each agent and saves it to a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4e1beaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_agent_trajectory(agent_id, agent, seed):\n",
    "    \n",
    "    beta_duration = npr.uniform(25, 140)\n",
    "    beta_start = max(npr.uniform(45, 480), beta_duration)\n",
    "    beta_ping = min(npr.uniform(3, 15), beta_duration//2)\n",
    "    print(beta_ping, beta_start, beta_duration)\n",
    "    # agent.generate_trajectory(\n",
    "    #     datetime = \"2024-01-01T08:00 +01:00\",\n",
    "    #     end_time = pd.Timestamp('2024-01-21T08:30:00 +01:00'),\n",
    "    #     seed=1,\n",
    "    #     dt=1)\n",
    "    agent.generate_trajectory(\n",
    "        end_time=pd.Timestamp(\"2024-01-14T08:30:00 +01:00\"),\n",
    "        dt=1,\n",
    "        seed=1,\n",
    "        local_timestamp=pd.Timestamp(\"2024-01-01T08:00:00 +01:00\"),\n",
    "        location=agent.home,   # ensures a valid building start\n",
    "    )\n",
    "    print('finished generating trajectory')\n",
    "    print(\"Trajectory columns:\", agent.trajectory.columns)\n",
    "    print(agent.trajectory.head())\n",
    "    print(\"Diary empty?\", agent.diary.empty)\n",
    "    print(agent.diary.head())\n",
    "    agent.sample_trajectory(\n",
    "        beta_start=beta_start,\n",
    "        beta_durations=beta_duration,\n",
    "        beta_ping = beta_ping,\n",
    "        seed=seed,\n",
    "        ha=13/15,\n",
    "        replace_sparse_traj=True)\n",
    "    \n",
    "    garden_city_to_lat_long(agent,\n",
    "                            sparse_traj=True,\n",
    "                            full_traj=False)\n",
    "    \n",
    "    return agent_id, copy.deepcopy(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828c0340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'admiring_allen'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_id, agent = [(agent_id, agent) for agent_id, agent in population.roster.items()][0]\n",
    "agent_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01e588c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.726069542349697 179.43585824467377 110.38604091300256\n",
      "finished generating trajectory\n",
      "Trajectory columns: Index(['x', 'y', 'local_timestamp', 'unix_timestamp', 'identifier'], dtype='object')\n",
      "          x          y           local_timestamp  unix_timestamp  \\\n",
      "0  9.382740   8.455714 2024-01-01 08:01:00+01:00      1704092460   \n",
      "1  8.611431  10.729973 2024-01-01 08:02:00+01:00      1704092520   \n",
      "2  6.464773  12.191183 2024-01-01 08:03:00+01:00      1704092580   \n",
      "3  5.000000  12.500000 2024-01-01 08:04:00+01:00      1704092640   \n",
      "4  5.000000  12.500000 2024-01-01 08:05:00+01:00      1704092700   \n",
      "\n",
      "       identifier  \n",
      "0  admiring_allen  \n",
      "1  admiring_allen  \n",
      "2  admiring_allen  \n",
      "3  admiring_allen  \n",
      "4  admiring_allen  \n",
      "Diary empty? False\n",
      "            local_timestamp  unix_timestamp  duration  location  \\\n",
      "0 2024-01-01 08:01:00+01:00      1704092460         4      None   \n",
      "1 2024-01-01 08:05:00+01:00      1704092700       236  w-x6-y12   \n",
      "2 2024-01-01 12:01:00+01:00      1704106860         3      None   \n",
      "3 2024-01-01 12:04:00+01:00      1704107040        12  r-x3-y11   \n",
      "4 2024-01-01 12:16:00+01:00      1704107760         2      None   \n",
      "\n",
      "       identifier  \n",
      "0  admiring_allen  \n",
      "1  admiring_allen  \n",
      "2  admiring_allen  \n",
      "3  admiring_allen  \n",
      "4  admiring_allen  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('admiring_allen', <nomad.traj_gen.Agent at 0x14c65da00>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_agent_trajectory(agent_id, agent, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd2c0ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating trajectories: 100%|████████████████| 300/300 [07:16<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "def generate_trajectory_data(agent_id, agent, seed):\n",
    "    beta_duration = npr.uniform(25, 140)\n",
    "    beta_start = max(npr.uniform(45, 480), beta_duration)\n",
    "    beta_ping = min(npr.uniform(3, 15), beta_duration//2)\n",
    "\n",
    "    agent.reset_trajectory()\n",
    "    agent.generate_trajectory(\n",
    "        end_time=pd.Timestamp(\"2024-01-14T08:30:00 +01:00\"),\n",
    "        dt=1,\n",
    "        seed=1,\n",
    "        local_timestamp=pd.Timestamp(\"2024-01-01T08:00:00 +01:00\"),\n",
    "        location=agent.home,   # ensures a valid building start\n",
    "    )\n",
    "    agent.sample_trajectory(\n",
    "        beta_start=beta_start,\n",
    "        beta_durations=beta_duration,\n",
    "        beta_ping=beta_ping,\n",
    "        seed=seed,\n",
    "        ha=13/15,\n",
    "        replace_sparse_traj=True)\n",
    "\n",
    "    garden_city_to_lat_long(agent, sparse_traj=True, full_traj=False)\n",
    "    return None\n",
    "\n",
    "# Generate trajectories with progress bar\n",
    "for agent_id, agent in tqdm(population.roster.items(), desc=\"Generating trajectories\"):\n",
    "    generate_trajectory_data(agent_id, agent, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bc723ca-634d-4db9-8d6f-35ec1ae74a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.diary.location.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4286bf1-b6f9-4fd0-a284-b39fcbfb613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_cols = {\n",
    "    \"user_id\": \"uid\",\n",
    "    \"timestamp\": \"timestamp\",\n",
    "    \"latitude\": \"latitude\",\n",
    "    \"longitude\": \"longitude\",\n",
    "    \"x\": \"x\",\n",
    "    \"y\": \"y\",\n",
    "    \"duration\": \"duration\",\n",
    "    \"datetime\": \"datetime\"}\n",
    "# Save only sparse trajectories and diaries\n",
    "population.save_pop(\n",
    "    save_full_traj = False,\n",
    "    save_sparse_traj = True,\n",
    "    save_diaries = True,\n",
    "    partition_cols={\"sparse_traj\": [\"date\"]},\n",
    "    bucket='catalog-raghav',\n",
    "    prefix='temp/'\n",
    "    #traj_cols=traj_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79e81d81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "output/sparse_traj/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sparse_df \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mfrom_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/sparse_traj/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, traj_cols\u001b[38;5;241m=\u001b[39mtraj_cols,\n\u001b[1;32m      2\u001b[0m                       parse_dates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m diaries_df \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mfrom_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/diaries/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, traj_cols\u001b[38;5;241m=\u001b[39mtraj_cols,\n\u001b[1;32m      4\u001b[0m                        parse_dates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Dropbox/nomad/hdbscan/nomad/nomad/io/base.py:794\u001b[0m, in \u001b[0;36mfrom_file\u001b[0;34m(filepath, format, traj_cols, parse_dates, mixed_timezone_behavior, fixed_format, sep, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;124;03mLoad and cast trajectory data from a specified file path or list of paths.\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;124;03m- Checks before loading that files have required trajectory columns for analysis. \u001b[39;00m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 794\u001b[0m column_names \u001b[38;5;241m=\u001b[39m table_columns(filepath, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m, sep\u001b[38;5;241m=\u001b[39msep)\n\u001b[1;32m    795\u001b[0m traj_cols \u001b[38;5;241m=\u001b[39m _parse_traj_cols(column_names, traj_cols, kwargs)\n\u001b[1;32m    797\u001b[0m _has_spatial_cols(column_names, traj_cols)\n",
      "File \u001b[0;32m~/Dropbox/nomad/hdbscan/nomad/nomad/io/base.py:710\u001b[0m, in \u001b[0;36mtable_columns\u001b[0;34m(filepath, format, include_schema, sep)\u001b[0m\n\u001b[1;32m    708\u001b[0m         schema \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mUnionDataset(schema\u001b[38;5;241m=\u001b[39mdatasets[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mschema, children\u001b[38;5;241m=\u001b[39mdatasets)\u001b[38;5;241m.\u001b[39mschema\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 710\u001b[0m         schema \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mdataset(filepath, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mfile_format_obj, partitioning\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhive\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mschema\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m schema \u001b[38;5;28;01mif\u001b[39;00m include_schema \u001b[38;5;28;01melse\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mIndex(schema\u001b[38;5;241m.\u001b[39mnames)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyarrow/dataset.py:794\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    783\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    784\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[1;32m    785\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m     selector_ignore_prefixes\u001b[38;5;241m=\u001b[39mignore_prefixes\n\u001b[1;32m    791\u001b[0m )\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(source):\n\u001b[0;32m--> 794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _filesystem_dataset(source, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, FileInfo) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyarrow/dataset.py:476\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    474\u001b[0m         fs, paths_or_selector \u001b[38;5;241m=\u001b[39m _ensure_multiple_sources(source, filesystem)\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 476\u001b[0m     fs, paths_or_selector \u001b[38;5;241m=\u001b[39m _ensure_single_source(source, filesystem)\n\u001b[1;32m    478\u001b[0m options \u001b[38;5;241m=\u001b[39m FileSystemFactoryOptions(\n\u001b[1;32m    479\u001b[0m     partitioning\u001b[38;5;241m=\u001b[39mpartitioning,\n\u001b[1;32m    480\u001b[0m     partition_base_dir\u001b[38;5;241m=\u001b[39mpartition_base_dir,\n\u001b[1;32m    481\u001b[0m     exclude_invalid_files\u001b[38;5;241m=\u001b[39mexclude_invalid_files,\n\u001b[1;32m    482\u001b[0m     selector_ignore_prefixes\u001b[38;5;241m=\u001b[39mselector_ignore_prefixes\n\u001b[1;32m    483\u001b[0m )\n\u001b[1;32m    484\u001b[0m factory \u001b[38;5;241m=\u001b[39m FileSystemDatasetFactory(fs, paths_or_selector, \u001b[38;5;28mformat\u001b[39m, options)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyarrow/dataset.py:441\u001b[0m, in \u001b[0;36m_ensure_single_source\u001b[0;34m(path, filesystem)\u001b[0m\n\u001b[1;32m    439\u001b[0m     paths_or_selector \u001b[38;5;241m=\u001b[39m [path]\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filesystem, paths_or_selector\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: output/sparse_traj/"
     ]
    }
   ],
   "source": [
    "sparse_df = loader.from_file(\"output/sparse_traj/\", format=\"parquet\", traj_cols=traj_cols,\n",
    "                      parse_dates=True)\n",
    "diaries_df = loader.from_file(\"output/diaries/\", format=\"parquet\", traj_cols=traj_cols,\n",
    "                       parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e29eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "diaries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0be1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f762faed",
   "metadata": {},
   "source": [
    "## Parallelized Trajectory Generation\n",
    "\n",
    "For larger trajectory generation tasks that require a lot of compute power, we can parallelize the trajectory generation using the following code. We generate ground-truth trajectories in agent-month \"chunks\", sparsify each chunk, then reset the ground-truth trajectory field to lessen the memory usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e81ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using parallel processing (e.g., using a cluster)\n",
    "%%time\n",
    "\n",
    "def generate_trajectory_data(agent_id, agent, seed):\n",
    "    \n",
    "    beta_duration = npr.uniform(15, 180)\n",
    "    beta_start = max(npr.uniform(60, 1200), beta_duration*3)\n",
    "    beta_ping = npr.uniform(1.5, 30)\n",
    "    \n",
    "    param = (beta_start, beta_duration, beta_ping)\n",
    "    \n",
    "    for month in range(1,13):\n",
    "        days = calendar.monthrange(2024, month)[1]\n",
    "        population_n.generate_trajectory(agent, \n",
    "                                         T=datetime(2024, month, days, hour=23, minute=59), \n",
    "                                         seed=seed)\n",
    "    \n",
    "        agent.sample_traj_hier_nhpp(*param, \n",
    "                                    seed=seed,\n",
    "                                    reset_traj=True)\n",
    "    \n",
    "    garden_city_to_lat_long(agent,\n",
    "                            sparse_traj=True,\n",
    "                            full_traj=False)\n",
    "    \n",
    "    return agent_id, copy.deepcopy(agent)\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "shared_roster = manager.dict(population_n.roster)\n",
    "\n",
    "start = 6001  # 12001  # can modify\n",
    "end = 12001   # 18001  # can modify\n",
    "roster = dict(population_n.roster)\n",
    "batch = islice(roster.items(), start, end)\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    with tqdm(total=(end-start), desc=\"Processing agents\") as pbar:\n",
    "        futures = [\n",
    "            executor.submit(generate_trajectory_data, agent_id, agent, i+15000)\n",
    "            for i, (agent_id, agent) in enumerate(batch, start=start)\n",
    "        ]\n",
    "        results = []\n",
    "        for future in futures:\n",
    "            results.append(future.result())\n",
    "            pbar.update(1)\n",
    "\n",
    "for agent_id, agent in results:\n",
    "    population_n.roster[agent_id] = agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee64f31",
   "metadata": {},
   "source": [
    "This code saves the generated trajectories in a parquet file, using the date as the partition column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdffd98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "partition_cols = {\n",
    "    'sparse_traj': ['date'],\n",
    "    'diaries': ['identifier']\n",
    "}\n",
    "\n",
    "roster = dict(islice(population_n.roster.items(), start, end))\n",
    "\n",
    "population_n.save_pop(bucket=\"synthetic-raw-data\",\n",
    "                      prefix=f\"agents-{start+15000}-{end+15000-1}/\",\n",
    "                      save_full_traj=False,\n",
    "                      save_sparse_traj=True,\n",
    "                      save_homes=True,\n",
    "                      save_diaries=True,\n",
    "                      partition_cols=partition_cols,\n",
    "                      roster=roster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478a261b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the Parquet files\n",
    "s3_path = \"s3://synthetic-raw-data/agents-1-1001/sparse_trajectories.parquet/\"\n",
    "df1 = pd.read_parquet(s3_path)\n",
    "s3_path = \"s3://synthetic-raw-data/agents-1001-2000/sparse_trajectories.parquet/\"\n",
    "df2 = pd.read_parquet(s3_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3 (ipykernel)"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    },
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3.10 (daphme)"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    },
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 3,
           "op": "addrange",
           "valuelist": "0"
          },
          {
           "key": 3,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
