{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9fd0c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-muted')\n",
    "from matplotlib import cm\n",
    "import geopandas as gpd\n",
    "import math\n",
    "\n",
    "from pyproj import Transformer\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import numpy.random as npr\n",
    "import matplotlib.dates as mdates\n",
    "from itertools import product\n",
    "import copy\n",
    "import boto3\n",
    "import pickle\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nomad.io.base as loader\n",
    "import nomad.city_gen as cg\n",
    "from nomad.city_gen import City, Building\n",
    "import nomad.traj_gen as tg\n",
    "from nomad.traj_gen import Agent, Population\n",
    "import nomad.stop_detection.ta_dbscan as DBSCAN\n",
    "import nomad.stop_detection.lachesis as Lachesis\n",
    "from nomad.generation.sparsity import gen_params_target_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79a7061",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# garden city\n",
    "\n",
    "city_geojson = gpd.read_file('garden_city.geojson')\n",
    "\n",
    "city = cg.load('garden-city.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e50195a-7819-46e3-919d-a1a84105dea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# synthetic philly\n",
    "\n",
    "city_geojson = gpd.read_file('philly.geojson')\n",
    "\n",
    "s3 = boto3.client('s3', region_name=\"us-east-2\")\n",
    "pickle_buffer = io.BytesIO()\n",
    "s3.download_fileobj(\"synthetic-philly\", \"philadelphia-city.pkl\", pickle_buffer)\n",
    "pickle_buffer.seek(0)\n",
    "city = pickle.load(pickle_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22f6e1",
   "metadata": {},
   "source": [
    "### Generate N agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af174e5",
   "metadata": {},
   "source": [
    "The following code maps our Garden City coordinates to a location in the Atlantic Ocean (Atlantis?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04b6dc6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def garden_city_to_lat_long(agent, sparse_traj=True, full_traj=False, diaries=True):\n",
    "    def project_city_blocks_to_web_mercator(df):\n",
    "        \"\"\"Convert (x, y) from 15m block units to Web Mercator meters via affine shift and projection.\"\"\"\n",
    "        transformer = Transformer.from_crs(\"EPSG:3857\", \"EPSG:4326\", always_xy=True)\n",
    "        df['x'] = 15 * df['x'] - 4265699\n",
    "        df['y'] = 15 * df['y'] + 4392976\n",
    "        if 'ha' in df:\n",
    "            df['ha'] = 15 * df['ha']\n",
    "        df['longitude'], df['latitude'] = transformer.transform(df['x'].values, df['y'].values)\n",
    "        df['date'] = df['datetime'].dt.date\n",
    "        return df\n",
    "\n",
    "    def finalize(df):\n",
    "        front = ['identifier', 'timestamp', 'longitude', 'latitude', 'x', 'y', 'date']\n",
    "        cols = [col for col in front if col in df] + [col for col in df.columns if col not in front]\n",
    "        return df[cols].rename(columns={'identifier': 'uid', 'timestamp': 'timestamp'}).reset_index(drop=True)\n",
    "\n",
    "    if sparse_traj:\n",
    "        agent.sparse_traj = finalize(project_city_blocks_to_web_mercator(agent.sparse_traj))\n",
    "    if full_traj:\n",
    "        agent.trajectory = finalize(project_city_blocks_to_web_mercator(agent.trajectory))\n",
    "\n",
    "    if diaries:\n",
    "        diary = agent.diary.copy()\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for loc in diary[\"location\"]:\n",
    "            if loc is None:\n",
    "                xs.append(None)\n",
    "                ys.append(None)\n",
    "            else:\n",
    "                pt = agent.city.buildings[loc].geometry.centroid\n",
    "                xs.append(pt.x)\n",
    "                ys.append(pt.y)\n",
    "        diary[\"x\"] = xs\n",
    "        diary[\"y\"] = ys\n",
    "        agent.diary = finalize(project_city_blocks_to_web_mercator(diary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f3f437e-1bf3-4069-96e9-0a9da90b950f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def philly_to_lat_long(agent, sparse_traj=True, full_traj=False, diaries=True):\n",
    "    def project_point_to_web_mercator(x, y):\n",
    "        \"\"\"\n",
    "        Project a fractional (x, y) block coord to Web Mercator using affine interpolation.\n",
    "        philly_grid_map is the grid_map produced by RealCityGenerator in virtual_philly.ipynb\n",
    "           import it into this notebook through pkl\n",
    "        \"\"\"\n",
    "        i, j = int(math.floor(x)), int(math.floor(y))\n",
    "        poly = philly_grid_map.get((i, j))\n",
    "\n",
    "        if poly is None:\n",
    "            raise ValueError(f\"No polygon found at grid cell ({i}, {j})\")\n",
    "\n",
    "        # Bounds of the 1x1 block polygon in EPSG:3857\n",
    "        minx, miny, maxx, maxy = poly.bounds\n",
    "\n",
    "        dx = x - i\n",
    "        dy = y - j\n",
    "\n",
    "        X = minx + dx * (maxx - minx)\n",
    "        Y = miny + dy * (maxy - miny)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def apply_projection_to_df(df):\n",
    "        \"\"\"Apply Web Mercator projection to a DataFrame with 'x' and 'y' columns.\"\"\"\n",
    "        def safe_project(row):\n",
    "            try:\n",
    "                return project_point_to_web_mercator(row['x'], row['y'])\n",
    "            except Exception:\n",
    "                return (None, None)\n",
    "\n",
    "        projected = df.apply(safe_project, axis=1)\n",
    "        df[['x', 'y']] = pd.DataFrame(projected.tolist(), index=df.index)\n",
    "\n",
    "        transformer = Transformer.from_crs(\"EPSG:3857\", \"EPSG:4326\", always_xy=True)\n",
    "        if 'ha' in df:\n",
    "            df['ha'] = 10 * df['ha']  # 10 because thats the sidelength of a block\n",
    "        df['longitude'], df['latitude'] = transformer.transform(df['x'].values, df['y'].values)\n",
    "        df['date'] = df['datetime'].dt.date\n",
    "        return df\n",
    "\n",
    "    def finalize(df):\n",
    "        front = ['identifier', 'timestamp', 'longitude', 'latitude', 'x', 'y', 'date']\n",
    "        cols = [col for col in front if col in df] + [col for col in df.columns if col not in front]\n",
    "        return df[cols].rename(columns={'identifier': 'uid', 'timestamp': 'timestamp'}).reset_index(drop=True)\n",
    "\n",
    "    if sparse_traj:\n",
    "        agent.sparse_traj = finalize(apply_projection_to_df(agent.sparse_traj))\n",
    "    if full_traj:\n",
    "        agent.trajectory = finalize(apply_projection_to_df(agent.trajectory))\n",
    "\n",
    "    if diaries:\n",
    "        diary = agent.diary.copy()\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for loc in diary[\"location\"]:\n",
    "            if loc is None:\n",
    "                xs.append(None)\n",
    "                ys.append(None)\n",
    "            else:\n",
    "                pt = agent.city.buildings[loc].geometry.centroid\n",
    "                xs.append(pt.x)\n",
    "                ys.append(pt.y)\n",
    "        diary[\"x\"] = xs\n",
    "        diary[\"y\"] = ys\n",
    "        agent.diary = finalize(apply_projection_to_df(diary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27199b",
   "metadata": {},
   "source": [
    "## Simple trajectory generation\n",
    "\n",
    "For simple trajectory generation tasks that don't require too much computation power and can be done on a personal laptop, the following code generates a trajectory for each agent and saves it to a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd2c0ede",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating trajectories: 100%|██████████| 1/1 [00:28<00:00, 28.22s/it]\n"
     ]
    }
   ],
   "source": [
    "def generate_trajectory_data(agent, seed_trajectory=None, seed_sparsity=None):\n",
    "    beta_params = gen_params_target_q(q_range=(0.3, 0.9), seed=seed_sparsity)\n",
    "    rng = npr.default_rng(seed_sparsity)\n",
    "    ha_sample = rng.uniform(11.5/15, 1)\n",
    "\n",
    "    agent.reset_trajectory()\n",
    "    agent.generate_trajectory(\n",
    "        datetime = \"2024-01-01T07:00 -04:00\",\n",
    "        end_time = pd.Timestamp('2024-01-31T09:00 -04:00'),\n",
    "        seed=seed_trajectory,\n",
    "        dt=1)\n",
    "\n",
    "    agent.sample_trajectory(\n",
    "        **beta_params,\n",
    "        seed=seed_sparsity,\n",
    "        ha=ha_sample,\n",
    "        replace_sparse_traj=True)\n",
    "\n",
    "    philly_to_lat_long(agent, sparse_traj=True, full_traj=False)\n",
    "    agent.reset_trajectory(trajectory = False, sparse = False, diary = False)\n",
    "    return None\n",
    "\n",
    "# Generate trajectories with progress bar\n",
    "N = 1\n",
    "population = Population(city)\n",
    "population.generate_agents(N=N, seed=250, name_count=2)\n",
    "\n",
    "for i, agent in enumerate(tqdm(population.roster.values(), desc=\"Generating trajectories\")):\n",
    "    generate_trajectory_data(agent, seed_trajectory=i, seed_sparsity=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70f2de13-10d7-4736-9a9f-527d1d1692ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.sparse_traj.to_csv(\"philly_sparse_traj.csv\", index=False)\n",
    "agent.trajectory.to_csv(\"philly_full_traj.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4286bf1-b6f9-4fd0-a284-b39fcbfb613f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "traj_cols = {\n",
    "    \"user_id\": \"uid\",\n",
    "    \"timestamp\": \"timestamp\",\n",
    "    \"latitude\": \"latitude\",\n",
    "    \"longitude\": \"longitude\",\n",
    "    \"x\": \"x\",\n",
    "    \"y\": \"y\",\n",
    "    \"duration\": \"duration\",\n",
    "    \"datetime\": \"datetime\"}\n",
    "# Save only sparse trajectories and diaries\n",
    "population.save_pop(\n",
    "    sparse_path=\"output/sparse_traj/\",\n",
    "    diaries_path=\"output/diaries/\",\n",
    "    partition_cols={\"sparse_traj\": [\"date\"]},\n",
    "    traj_cols=traj_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e81d81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sparse_df = loader.from_file(\"output/sparse_traj/\", format=\"parquet\", traj_cols=traj_cols,\n",
    "                      parse_dates=True)\n",
    "diaries_df = loader.from_file(\"output/diaries/\", format=\"parquet\", traj_cols=traj_cols,\n",
    "                       parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e29eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "diaries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0be1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f762faed",
   "metadata": {},
   "source": [
    "For larger trajectory generation tasks that require a lot of compute power, we can parallelize the trajectory generation using the following code. We generate ground-truth trajectories in agent-month \"chunks\", sparsify each chunk, then reset the ground-truth trajectory field to lessen the memory usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e81ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using parallel processing (e.g., using a cluster)\n",
    "%%time\n",
    "\n",
    "traj_cols = {\n",
    "    \"user_id\": \"uid\",\n",
    "    \"timestamp\": \"timestamp\",\n",
    "    \"latitude\": \"latitude\",\n",
    "    \"longitude\": \"longitude\",\n",
    "    \"x\": \"x\",\n",
    "    \"y\": \"y\",\n",
    "    \"duration\": \"duration\",\n",
    "    \"datetime\": \"datetime\"}\n",
    "\n",
    "def generate_trajectory_data(agent_id, agent, seed_trajectory=None, seed_sparsity=None):\n",
    "    \n",
    "    beta_params = gen_params_target_q(q_range=(0.3, 0.9), seed=seed_sparsity)\n",
    "    rng = npr.default_rng(seed_sparsity)\n",
    "    ha_sample = rng.uniform(11.5/15, 1)\n",
    "    \n",
    "    for month in range(1,13):\n",
    "        days = calendar.monthrange(2024, month)[1]\n",
    "        agent.generate_trajectory(\n",
    "            datetime=f'2024-{month}-01T00:00 -04:00',\n",
    "            end_time=pd.Timestamp(f'2024-{month}-{days}T23:59 -04:00'),\n",
    "            seed=seed_trajectory,\n",
    "            dt=1)\n",
    "    \n",
    "        agent.sample_trajectory(\n",
    "            **beta_params,\n",
    "            seed=seed_sparsity,\n",
    "            ha=ha_sample,\n",
    "            replace_sparse_traj=True)  ## Parallelized Trajectory Generation\n",
    "    \n",
    "    garden_city_to_lat_long(agent,\n",
    "                            sparse_traj=True,\n",
    "                            full_traj=False)\n",
    "    \n",
    "    return agent_id, copy.deepcopy(agent)\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "shared_roster = manager.dict(population_n.roster)\n",
    "\n",
    "start = 6001  # 12001  # can modify\n",
    "end = 12001   # 18001  # can modify\n",
    "roster = dict(population_n.roster)\n",
    "batch = islice(roster.items(), start, end)\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    with tqdm(total=(end-start), desc=\"Processing agents\") as pbar:\n",
    "        futures = [\n",
    "            executor.submit(generate_trajectory_data, agent_id, agent, i+15000)\n",
    "            for i, (agent_id, agent) in enumerate(batch, start=start)\n",
    "        ]\n",
    "        results = []\n",
    "        for future in futures:\n",
    "            results.append(future.result())\n",
    "            pbar.update(1)\n",
    "\n",
    "for agent_id, agent in results:\n",
    "    population_n.roster[agent_id] = agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee64f31",
   "metadata": {},
   "source": [
    "This code saves the generated trajectories in a parquet file, using the date as the partition column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdffd98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "partition_cols = {\n",
    "    'sparse_traj': ['date'],\n",
    "    'diaries': ['identifier']\n",
    "}\n",
    "\n",
    "roster = dict(islice(population_n.roster.items(), start, end))\n",
    "\n",
    "population.save_pop(bucket=\"synthetic-raw-data\",\n",
    "                    prefix=f\"agents-{start+15000}-{end+15000-1}/\",\n",
    "                    save_full_traj=False,\n",
    "                    save_sparse_traj=True,\n",
    "                    save_homes=True,\n",
    "                    save_diaries=True,\n",
    "                    partition_cols=partition_cols,\n",
    "                    roster=roster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478a261b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the Parquet files\n",
    "s3_path = \"s3://synthetic-raw-data/agents-1-1001/sparse_trajectories.parquet/\"\n",
    "df1 = pd.read_parquet(s3_path)\n",
    "s3_path = \"s3://synthetic-raw-data/agents-1001-2000/sparse_trajectories.parquet/\"\n",
    "df2 = pd.read_parquet(s3_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3 (ipykernel)"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    },
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3.10 (daphme)"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    },
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 3,
           "op": "addrange",
           "valuelist": "0"
          },
          {
           "key": 3,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
