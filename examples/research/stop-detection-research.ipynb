{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1364ba7a-c457-424d-b68d-0eabde1d9ac4",
   "metadata": {},
   "source": [
    "# ST-HDBSCAN: Spatiotemporal Hierarchical DBSCAN for Trajectory Data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf66e7f6-4f0c-4308-a2e9-5165e423dc63",
   "metadata": {},
   "source": [
    "Francisco Barreras\n",
    "\n",
    "Duncan Watts\n",
    "\n",
    "Bethanny Hsiao\n",
    "\n",
    "Andres Mondragon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff17d8-0050-4885-89f8-0b3aabd43ec7",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a16e0da-f862-43b5-a4fb-2d4f1e8c32f6",
   "metadata": {},
   "source": [
    "The study of human mobility has advanced greatly in recent years due to the availability of\n",
    "commercial large-scale GPS trajectory datasets [3]. However, the validity of findings that use\n",
    "these datasets depends heavily on the robustness of their pre-processing methods. An important\n",
    "step in the processing of mobility data is the detection of stops within GPS trajectories, for\n",
    "which many clustering algorithms have been proposed [4, 8, 6, 1]. Yet, the high sparsity of\n",
    "commercial GPS data can affect the performance of these stop-detection algorithms.\n",
    "In the case of DBSCAN, while it initially identifies dense regions, it can often over-cluster\n",
    "or under-cluster due to noise and weakly connected points given the chosen ε. ST-DBSCAN [4]\n",
    "uses two distance thresholds, Eps1 for spatial and Eps2 for non-spatial values. The algorithm\n",
    "compares the average non-spatial value, such as temperature, of a cluster with a new com-\n",
    "ing value, to prevent merging adjacent clusters. Nevertheless, datasets that include this kind\n",
    "of information are not comparable to realistic GPS-based trajectories. A promising algorithm\n",
    "is T-DBSCAN [6], which searches forward in time for a continuous density-based neighbor-\n",
    "hood of core points. Points spatially close, within Eps, and within a roaming threshold, CEps, are included in a cluster. Additionally, we used a time-augmented DBSCAN algorithm, TA-DBSCAN, which recursively processes the clusters obtained from DBSCAN to address the issue of initial clusters overlapping in time. However, methods [9] that validate stop-detection algorithms based on synthetic data show that these can omit, merge, or split stops based on the selection of epsilon and sparsity of the data.\n",
    "\n",
    "If we define parameters that may be considered fine (low ε), it might completely miss a stop at a larger location. In contrast, coarse parameters (large ε) may struggle to differentiate stops within small neighboring locations [3]. Since different venues vary in stop durations and areas, this could influence the parameter choices [9]. To address this parameter selection limitation, we propose a spatiotemporal variation of Hierarchical DBSCAN [5], ST-HDBSCAN. Unlike DBSCAN, which relies on one threshold of density to cluster points, our variation constructs separate structures for space and time distances that preserve density-based connections in these two dimensions. This approach ensures that when pruning the hierarchical tree structure needed for cluster formation, we account for varying spatiotemporal densities. As a result, clusters emerge naturally without requiring specific time and space thresholds, working effectively across different data sparsity levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8502ca-e6d1-4df1-9f2f-7bfceafd598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import pygeohash as gh\n",
    "import geopandas as gpd\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from pyproj import Transformer\n",
    "import heapq\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1af0f24-ab30-43a4-bfa6-5ed0dcb24dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nomad.io.base as loader\n",
    "import nomad.constants as constants\n",
    "import nomad.stop_detection.ta_dbscan as DBSCAN\n",
    "import nomad.stop_detection.lachesis as Lachesis\n",
    "import nomad.filters as filters\n",
    "import nomad.city_gen as cg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8eda3ba-6ee8-402e-bc4d-82a10ee4c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bursts(times, time_thresh, burst_col = False):\n",
    "    # Pairwise time differences\n",
    "    time_diffs = np.abs(times[:, np.newaxis] - times)\n",
    "    time_diffs = time_diffs.astype(int)\n",
    "    \n",
    "    # Filter by time threshold\n",
    "    within_time_thresh = np.triu(time_diffs <= (time_thresh * 60), k=1)\n",
    "    i_idx, j_idx = np.where(within_time_thresh)\n",
    "    \n",
    "    # Return a list of (timestamp1, timestamp2) tuples\n",
    "    time_pairs = [(times[i], times[j]) for i, j in zip(i_idx, j_idx)]\n",
    "    \n",
    "    return time_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52485801-f3e8-4580-a4d1-bbee9e51cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_core_distance_with_time(traj, bursts, min_samples = 2):\n",
    "    \"\"\"\n",
    "    Calculate the core distance for each ping in traj.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    traj : dataframe\n",
    "    \n",
    "    min_samples : int\n",
    "        used to calculate the core distance of a point p, where core distance of a point p \n",
    "        is defined as the distance from p to its min_samples-th smallest nearest neighbor\n",
    "        (including itself).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    core_distances : dictionary of timestamps\n",
    "        {timestamp_1: core_distance_1, ..., timestamp_n: core_distance_n}\n",
    "    \"\"\"\n",
    "    # it gives local density estimate: small core distance → high local density.\n",
    "    coords = traj[['x', 'y']].to_numpy()\n",
    "    timestamps = traj['timestamp'].to_numpy()\n",
    "    n = len(coords)\n",
    "    timestamps_dict = {ts: idx for idx, ts in enumerate(timestamps)}\n",
    "\n",
    "    # Build neighbor map from bursts\n",
    "    neighbor_map = {ts: set() for ts in timestamps}\n",
    "    \n",
    "    for t1, t2 in bursts:\n",
    "        neighbor_map[t1].add(t2)\n",
    "        neighbor_map[t2].add(t1)\n",
    "\n",
    "    core_distances = {}\n",
    "    \n",
    "    for i in range(n):\n",
    "        ts_i = timestamps[i]\n",
    "        allowed_neighbors = neighbor_map[ts_i]\n",
    "        dists = np.full(n, np.inf)\n",
    "        dists[i] = 0  # distance to itself\n",
    "        \n",
    "        for ts_j in allowed_neighbors:\n",
    "            j = timestamps_dict.get(ts_j)\n",
    "            if j is not None:\n",
    "                dists[j] = np.sqrt(np.sum((coords[i] - coords[j]) ** 2))\n",
    "        \n",
    "        sorted_dists = np.sort(dists)\n",
    "        core_distances[ts_i] = sorted_dists[min_samples - 1]\n",
    "\n",
    "    return core_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "021b5168-9257-4477-aeda-dd6c65e35a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_core_distance(traj, min_samples = 2):\n",
    "    \"\"\"\n",
    "    Calculate the core distance for each ping in traj.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    traj : dataframe\n",
    "    \n",
    "    min_samples : int\n",
    "        used to calculate the core distance of a point p, where core distance of a point p \n",
    "        is defined as the distance from p to its min_samples-th smallest nearest neighbor\n",
    "        (including itself).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    core_distances : dictionary of timestamps\n",
    "        {timestamp_1: core_distance_1, ..., timestamp_n: core_distance_n}\n",
    "    \"\"\"\n",
    "    # it gives local density estimate: small core distance → high local density.\n",
    "    coords = traj[['x', 'y']].to_numpy()\n",
    "    timestamps = traj['timestamp'].to_numpy()\n",
    "    n = len(coords)\n",
    "    core_distances = {}\n",
    "\n",
    "    # TC: 0(n^2)\n",
    "    for i in range(n):\n",
    "        # pairwise euclidean distances\n",
    "        dists = np.sqrt(np.sum((coords - coords[i]) ** 2, axis=1))\n",
    "        # sort distances and get min_samples-th smallest\n",
    "        sorted_dists = np.sort(dists)\n",
    "        core_distance = sorted_dists[min_samples - 1]\n",
    "        core_distances[timestamps[i]] = core_distance\n",
    "\n",
    "    return core_distances\n",
    "\n",
    "def compute_mrd(traj, core_distances):\n",
    "    \"\"\"\n",
    "    Mutual Reachability Distance (MRD) of point p and point q is the maximum of: \n",
    "    cordistance of p, core distance of q, and the distance between p and q.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    traj : dataframe\n",
    "    \n",
    "    core_distances : dict\n",
    "        Keys are (timestamp1, timestamp2), values are mutual reachability distances.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mrd : dictionary of timestamp pairs\n",
    "        {(timestamp1, timestamp2): mrd_value}.\n",
    "    \"\"\"\n",
    "    # edges between dense regions are favored, and sparse regions have larger edge weights\n",
    "    # MRD will inflate distances so that sparse areas are less likely to form clusters\n",
    "    # Even if two points are physically close, we shouldn’t treat them as equally “reachable”\n",
    "    # because they live in very different local densities\n",
    "    coords = traj[['x', 'y']].to_numpy()\n",
    "    timestamps = traj['timestamp'].to_numpy()\n",
    "    n = len(coords)\n",
    "    mrd = {}\n",
    "\n",
    "    # TC: 0(n^2)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            # euclidean distance between point i and j\n",
    "            dist = np.sqrt(np.sum((coords[j] - coords[i]) ** 2))\n",
    "            core_i = core_distances[timestamps[i]]\n",
    "            core_j = core_distances[timestamps[j]]\n",
    "\n",
    "            mrd[(timestamps[i], timestamps[j])] = max(core_i, core_j, dist)\n",
    "\n",
    "    return mrd\n",
    "\n",
    "def mst(mrd):\n",
    "    \"\"\"\n",
    "    Compute the MST using Prim's algorithm from mutual reachability distances.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mrd : dict\n",
    "        Keys are (timestamp1, timestamp2), values are mutual reachability distances.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mst_edges : list of tuples\n",
    "        (timestamp1, timestamp2, mrd_value) for the MST.\n",
    "    \"\"\"\n",
    "    graph = defaultdict(list)\n",
    "    \n",
    "    for (u, v), weight in mrd.items():\n",
    "        graph[u].append((weight, v))\n",
    "        graph[v].append((weight, u))\n",
    "\n",
    "    visited = set()\n",
    "    mst_edges = []\n",
    "    start_node = next(iter(graph))\n",
    "    min_heap = [(0, start_node, start_node)]  # (weight, from, to)\n",
    "\n",
    "    while min_heap and len(visited) < len(graph):\n",
    "        weight, frm, to = heapq.heappop(min_heap)\n",
    "        \n",
    "        if to in visited:\n",
    "            continue\n",
    "        \n",
    "        visited.add(to)\n",
    "        \n",
    "        if frm != to:\n",
    "            mst_edges.append((frm, to, weight))\n",
    "        \n",
    "        for next_weight, neighbor in graph[to]:\n",
    "            if neighbor not in visited:\n",
    "                heapq.heappush(min_heap, (next_weight, to, neighbor))\n",
    "\n",
    "    return mst_edges\n",
    "\n",
    "def mst_ext(mst_edges, core_distances):\n",
    "    \"\"\"\n",
    "    Add self-edges to MST with weights equal to each point's core distance.\n",
    "    \"\"\"\n",
    "    self_edges = [(ts, ts, core_distances[ts]) for ts in core_distances]\n",
    "    return mst_edges + self_edges\n",
    "\n",
    "\n",
    "def hdbscan(mst_ext, min_cluster_size):\n",
    "    hierarchy = []\n",
    "    \n",
    "    # 4.1 For the root of the tree assign all objects the same label (single “cluster”).\n",
    "    all_pings = set()\n",
    "    \n",
    "    for u, v, _ in mst_ext:\n",
    "        all_pings.add(u)\n",
    "        all_pings.add(v)\n",
    "        \n",
    "    label_map = {ts: 0 for ts in all_pings} # {'t1':0, 't2':0, 't3':0}\n",
    "    active_clusters = {0: set(all_pings)} # e.g. { 0: {'t1', 't2', 't3'} }\n",
    "    \n",
    "    # sort edges in decraesing order of weight\n",
    "    mst_ext_sorted = sorted(mst_ext, key=lambda x: -x[2]) \n",
    "    \n",
    "    # group edges by weight\n",
    "    dendrogram_scales = defaultdict(list)\n",
    "    for u, v, w in mst_ext_sorted:\n",
    "        dendrogram_scales[w].append((u, v))\n",
    "\n",
    "    current_label_id = max(label_map.values()) + 1\n",
    "\n",
    "    # 4.2.1 Before each removal, set the dendrogram scale value of the current hierarchical level as the weight of the edge(s) to be removed.\n",
    "    for scale, edges in dendrogram_scales.items():\n",
    "        affected_clusters = set()\n",
    "        edges_to_remove = []\n",
    "        \n",
    "        for u, v in edges:\n",
    "            if label_map.get(u) != label_map.get(v):\n",
    "                continue\n",
    "            cluster_id = label_map[u]\n",
    "            affected_clusters.add(cluster_id)\n",
    "            edges_to_remove.append((u, v))\n",
    "\n",
    "        # 4.2.2: For each affected cluster, reassign components\n",
    "        for cluster_id in affected_clusters:\n",
    "            if cluster_id == -1 or cluster_id not in active_clusters:\n",
    "                continue  # skip noise or already removed clusters\n",
    "            \n",
    "            members = active_clusters[cluster_id]\n",
    "    \n",
    "            # build connectivity graph (excluding removed edges)\n",
    "            G = build_connectivity_graph(members, mst_ext, edges_to_remove)\n",
    "            components = connected_components(G)\n",
    "            non_spurious = [c for c in components if len(c) >= min_cluster_size]\n",
    "\n",
    "            # cluster has disappeared\n",
    "            if not non_spurious:\n",
    "                for ts in members:\n",
    "                    label_map[ts] = -1  # noise\n",
    "                del active_clusters[cluster_id]\n",
    "            # cluster has just shrunk\n",
    "            elif len(non_spurious) == 1:\n",
    "                remaining = non_spurious[0]\n",
    "                for ts in members:\n",
    "                    label_map[ts] = cluster_id if ts in remaining else -1\n",
    "                active_clusters[cluster_id] = remaining\n",
    "            # true cluster split: multiple valid subclusters\n",
    "            elif len(non_spurious) > 1:\n",
    "                new_ids = []\n",
    "                del active_clusters[cluster_id]\n",
    "                for comp in non_spurious:\n",
    "                    for ts in comp:\n",
    "                        label_map[ts] = current_label_id\n",
    "                    active_clusters[current_label_id] = set(comp)\n",
    "                    new_ids.append(current_label_id)\n",
    "                    current_label_id += 1\n",
    "                \n",
    "                hierarchy.append((scale, cluster_id, new_ids))\n",
    "\n",
    "    return {\"label_map\": label_map, \"hierarchy\": hierarchy}\n",
    "\n",
    "def build_connectivity_graph(nodes, edges, removed_edges):\n",
    "    # nodes: set of timestamps {t1,t2,t3} \n",
    "    # edges: list of (u, v, w) tuples\n",
    "    # removed_edges: list of (u,v) tuples\n",
    "    graph = defaultdict(set)\n",
    "    removed_set = set(frozenset(e) for e in removed_edges)\n",
    "    for u, v, _ in edges:\n",
    "        if frozenset((u, v)) in removed_set:\n",
    "            continue\n",
    "        if u in nodes and v in nodes and u != v:\n",
    "            graph[u].add(v)\n",
    "            graph[v].add(u)\n",
    "    return graph\n",
    "\n",
    "def connected_components(graph):\n",
    "    seen = set()\n",
    "    components = []\n",
    "\n",
    "    for node in graph:\n",
    "        if node in seen:\n",
    "            continue\n",
    "        stack = [node]\n",
    "        comp = set()\n",
    "        while stack:\n",
    "            n = stack.pop()\n",
    "            if n not in seen:\n",
    "                seen.add(n)\n",
    "                comp.add(n)\n",
    "                stack.extend(graph[n] - seen)\n",
    "        components.append(comp)\n",
    "\n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07cd9276-30ed-435a-b8fe-4687a87d2f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_cols = {'user_id':'uid',\n",
    "             'datetime':'local_datetime',\n",
    "             'latitude':'latitude',\n",
    "             'longitude':'longitude'}\n",
    "\n",
    "data = loader.from_file(\"../../nomad/data/gc_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f0da7-8c0a-4228-b57b-819f56875105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a time offset column with different UTC offsets (in seconds)\n",
    "data['tz_offset'] = 0\n",
    "data.loc[data.index[:5000],'tz_offset'] = -7200\n",
    "data.loc[data.index[-5000:], 'tz_offset'] = 3600\n",
    "\n",
    "# create datetime column as a string\n",
    "data['local_datetime'] = loader._unix_offset_to_str(data.timestamp, data.tz_offset)\n",
    "data['local_datetime'] = pd.to_datetime(data['local_datetime'], utc=True)\n",
    "\n",
    "# create x, y columns in web mercator\n",
    "gdf = gpd.GeoSeries(gpd.points_from_xy(data.longitude, data.latitude),\n",
    "                        crs=\"EPSG:4326\")\n",
    "projected = gdf.to_crs(\"EPSG:3857\")\n",
    "data['x'] = projected.x\n",
    "data['y'] = projected.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083d50c-8f65-4c7d-ba7e-963b6ddc2a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2e83ba-85ee-4816-9545-102362e9aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sample = data.loc[data.uid == \"angry_spence\"]\n",
    "user_sample = user_sample[['timestamp', 'x', 'y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e9fab6-2bcb-4a61-838a-c1ebf2658300",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77af6f9d-e6b8-4244-8aad-a5ce3bee4f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_distances = compute_core_distance(user_sample, 4)\n",
    "mrd = compute_mrd(user_sample, core_distances)\n",
    "mst_edges = mst(mrd)\n",
    "mstext_edges = mst_ext(mst_edges, core_distances)\n",
    "hdbscan(mstext_edges, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nomad_env)",
   "language": "python",
   "name": "nomad_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
