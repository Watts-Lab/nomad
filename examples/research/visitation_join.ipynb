{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2803ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2fb0ef-73cc-4592-8ce5-9ad5f3027e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "\n",
    "import nomad.io.base as loader\n",
    "import nomad.stop_detection.utils as utils\n",
    "import nomad.stop_detection.hdbscan as HDBSCAN\n",
    "import nomad.stop_detection.lachesis as LACHESIS\n",
    "import nomad.stop_detection.ta_dbscan as TADBSCAN\n",
    "import nomad.stop_detection.grid_based as GRID_BASED # for oracle visits\n",
    "import nomad.stop_detection.postprocessing as pp\n",
    "\n",
    "import nomad.visit_attribution as visits\n",
    "import nomad.filters as filters\n",
    "import nomad.city_gen as cg\n",
    "\n",
    "from nomad.contact_estimation import overlapping_visits, compute_visitation_errors, compute_precision_recall_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892d2422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nomad.city_gen as cg\n",
    "city = cg.load('../garden-city.pkl')\n",
    "\n",
    "def classify_building_size_from_id(building_id):\n",
    "    building = city.buildings.get(building_id)\n",
    "    n_blocks = len(building.blocks)\n",
    "    if n_blocks == 1:\n",
    "        return 'small'\n",
    "    elif 2 <= n_blocks <= 3:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'big'\n",
    "\n",
    "def classify_building_type_from_id(building_id):\n",
    "    building = city.buildings.get(building_id)\n",
    "    return building.building_type\n",
    "\n",
    "def classify_dwell(duration):\n",
    "    if duration <= 5:\n",
    "        return 'low'\n",
    "    elif 6 <= duration <= 120:\n",
    "        return 'mid'\n",
    "    else:\n",
    "        return 'high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c3d06-7bc1-4669-a2bf-f04d8b3bd6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_cols = {'user_id':'uid',\n",
    "             'x':'x',\n",
    "             'y':'y',\n",
    "             'timestamp':'timestamp'}\n",
    "poi_table = gpd.read_file('../garden_city.gpkg')\n",
    "# building type\n",
    "poi_table = poi_table.rename({'type':'building_type'}, axis=1)\n",
    "# building size\n",
    "poi_table['building_size'] = poi_table['building_id'].apply(classify_building_size_from_id)\n",
    "\n",
    "diaries_df = loader.from_file(\"../../nomad/data/diaries\", format=\"parquet\", traj_cols=traj_cols)\n",
    "diaries_df = diaries_df.rename({'location':'building_id'}, axis=1)\n",
    "diaries_df = diaries_df.merge(poi_table[['building_id', 'building_size']], on='building_id', how='left')\n",
    "diaries_df = diaries_df.merge(poi_table[['building_id', 'building_type']], on='building_id', how='left')\n",
    "diaries_df['dwell_length'] = pd.NA\n",
    "diaries_df.loc[~diaries_df.building_id.isna(),'dwell_length'] = diaries_df.loc[~diaries_df.building_id.isna(),'duration'].apply(classify_dwell)\n",
    "\n",
    "sparse_df = loader.from_file(\"../../nomad/data/sparse_traj/\", format=\"parquet\", traj_cols=traj_cols,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac0bbd9-4ed4-4d61-8b38-9c5bb59d443b",
   "metadata": {},
   "source": [
    "## Analyze completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f51807",
   "metadata": {},
   "outputs": [],
   "source": [
    "completeness_df = filters.q_stats(sparse_df, traj_cols=traj_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea573424",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "ax = sns.kdeplot(\n",
    "    data=completeness_df,\n",
    "    x=\"q_stat\",\n",
    "    fill=True,\n",
    "    linewidth=1.5,\n",
    "    clip=(0, 1)\n",
    ")\n",
    "\n",
    "# cosmetics\n",
    "ax.set_xlabel(\"q (Trajectory Completeness)\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "sns.despine(top=True, right=True)\n",
    "\n",
    "# annotation (top-right corner in axes coords)\n",
    "ax.text(\n",
    "    0.99, 0.95,\n",
    "    f\"N = {len(completeness_df)}\",\n",
    "    transform=ax.transAxes,\n",
    "    ha=\"right\",\n",
    "    va=\"top\",\n",
    "    fontsize=9\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"q_stat_density.svg\", format=\"svg\", bbox_inches=\"tight\")\n",
    "plt.savefig(\"q_stat_density.png\", format=\"svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae7252-a50e-430f-aaee-f47425e1a74a",
   "metadata": {},
   "source": [
    "## Execution for all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a58f567-17c2-4773-b8ba-32d7777a32ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_cols = {'user_id':'uid',\n",
    "             'x':'x',\n",
    "             'y':'y',\n",
    "             'timestamp':'timestamp'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e76c98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "user1 = sparse_df.uid.unique()[9]\n",
    "sparse1 = sparse_df.loc[sparse_df.uid==user1]\n",
    "\n",
    "DUR_MIN=5\n",
    "DT_MAX=90\n",
    "DELTA_ROAM=30\n",
    "# to summarize stops after groupby passing through individual locations\n",
    "summarize_stops_with_loc = partial(utils.summarize_stop, x='x', y='y', keep_col_names=False, passthrough_cols = ['building_id'], complete_output=True) # < function(grouped_data)\n",
    "\n",
    "# If we want to use 'majority' to attribute visits, then we need stop labels on each ping\n",
    "labels_hdbscan = HDBSCAN.hdbscan_labels(traj=sparse1,\n",
    "                                            time_thresh=100,\n",
    "                                            min_pts=2,\n",
    "                                            min_cluster_size=3,\n",
    "                                            traj_cols=traj_cols)\n",
    "labels_hdbscan.name = 'cluster'\n",
    "# join with the original data\n",
    "sparse_with_cluster = sparse1.join(labels_hdbscan)\n",
    "\n",
    "# has same index as data\n",
    "pred_hdbscan = visits.point_in_polygon(data=sparse_with_cluster,\n",
    "                                        poi_table=poi_table,\n",
    "                                        method='majority',\n",
    "                                        data_crs='EPSG:3857',\n",
    "                                        max_distance=10,\n",
    "                                        user_id='uid',\n",
    "                                        cluster_label='cluster',\n",
    "                                        location_id='building_id',\n",
    "                                        x='x',\n",
    "                                        y='y')\n",
    "# join the building id\n",
    "pred = sparse_with_cluster.join(pred_hdbscan)\n",
    "\n",
    "stop_table_hdbscan = pred[pred.cluster!=-1].groupby('cluster', as_index=False).apply(summarize_stops_with_loc, include_groups=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37410cd6-9b14-43ec-838c-a4366059f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_detection_algos = ['oracle', 'lachesis', 'ta-dbscan', 'hdbscan']\n",
    "\n",
    "all_metrics_df = pd.DataFrame()\n",
    "metrics_size_df = pd.DataFrame()\n",
    "metrics_btype_df = pd.DataFrame()\n",
    "metrics_dwell_df = pd.DataFrame()\n",
    "\n",
    "TIME_THRESH=240\n",
    "DIST_THRESH=25\n",
    "MIN_PTS=2\n",
    "\n",
    "DUR_MIN=5\n",
    "DT_MAX=240\n",
    "DELTA_ROAM=35\n",
    "\n",
    "for user in tqdm(diaries_df.uid.unique()[:10], desc='Processing users'):\n",
    "    for algo in stop_detection_algos:\n",
    "        sparse = sparse_df[sparse_df['uid'] == user].copy()\n",
    "        truth = diaries_df.loc[diaries_df['uid'] == user].copy()\n",
    "        \n",
    "        if algo == 'oracle':\n",
    "            # oracle says correct poi for each ping\n",
    "            location = visits.oracle_map(\n",
    "                sparse,\n",
    "                truth,\n",
    "                timestamp='timestamp',\n",
    "                location_id='building_id')\n",
    "            # find cluster labels with naive grid-based continuity\n",
    "            labels = GRID_BASED.grid_based_labels(\n",
    "                data=sparse.join(location),\n",
    "                time_thresh=TIME_THRESH,\n",
    "                min_pts=0, #we allow stops of duration 0, patched later\n",
    "                location_id='building_id',\n",
    "                traj_cols=traj_cols)\n",
    "        \n",
    "        elif algo == 'lachesis':\n",
    "            labels = LACHESIS._lachesis_labels(\n",
    "                traj=sparse,\n",
    "                dt_max=DT_MAX,\n",
    "                dur_min=DUR_MIN,\n",
    "                delta_roam=DELTA_ROAM,\n",
    "                traj_cols=traj_cols)\n",
    "            \n",
    "            labels.name = 'cluster'           \n",
    "            sparse_with_cluster = sparse.join(labels)\n",
    "            \n",
    "        elif algo == 'ta-dbscan':\n",
    "            labels = TADBSCAN._temporal_dbscan_labels(\n",
    "                data=sparse,\n",
    "                time_thresh=TIME_THRESH,\n",
    "                dist_thresh=DIST_THRESH,\n",
    "                min_pts=MIN_PTS,\n",
    "                traj_cols=traj_cols)\n",
    "            labels.name = 'cluster'\n",
    "            sparse_with_cluster = sparse.join(labels)\n",
    "            \n",
    "        elif algo == 'hdbscan':\n",
    "            labels = HDBSCAN.hdbscan_labels(\n",
    "                traj=sparse,\n",
    "                time_thresh=TIME_THRESH,\n",
    "                min_pts=2,\n",
    "                min_cluster_size=1,\n",
    "                traj_cols=traj_cols)\n",
    "            labels.name = 'cluster'\n",
    "            sparse_with_cluster = sparse.join(labels)\n",
    "        else:\n",
    "            print(f\"Algorithm {algo} not in the list!\")\n",
    "\n",
    "        # ----------- COMPUTE STOPS FROM LABELS -----------------------\n",
    "        if algo != 'oracle': \n",
    "            pred = visits.point_in_polygon(data=sparse_with_cluster,\n",
    "                                            poi_table=poi_table,\n",
    "                                            method='majority',\n",
    "                                            data_crs='EPSG:3857',\n",
    "                                            max_distance=15,\n",
    "                                            cluster_label='cluster',\n",
    "                                            location_id='building_id',\n",
    "                                            x='x',\n",
    "                                            y='y')\n",
    "        \n",
    "            pred = sparse_with_cluster.join(pred)\n",
    "            stops = pred[pred.cluster!=-1].groupby('cluster', as_index=False).apply(summarize_stops_with_loc, include_groups=False)\n",
    "        # ------------------------------ POST PROCESSING SHOULD BE EMBEDDED IN ALGORITHMS -------------------------------------\n",
    "            try:\n",
    "                utils.invalid_stops(stops)\n",
    "            except:\n",
    "                print(f\"Algorithm {algo} has overlapping stops for user {user}. Postprocessing.\")\n",
    "                labels = GRID_BASED.grid_based_labels(\n",
    "                                data=pred.drop('cluster', axis=1),\n",
    "                                time_thresh=TIME_THRESH,\n",
    "                                min_pts=0, #we allow stops of duration 0, patched later\n",
    "                                location_id='building_id',\n",
    "                                traj_cols=traj_cols)\n",
    "                \n",
    "                pred['cluster'] = labels\n",
    "                stops = pred[pred.cluster!=-1].groupby('cluster', as_index=False).apply(summarize_stops_with_loc, include_groups=False)\n",
    "         # -------------------------------------------------------------------            \n",
    "        else: #location exists by oracle, we join everything\n",
    "            pred = sparse.join(location).join(labels)\n",
    "            stops = pred[pred.cluster!=-1].groupby('cluster', as_index=False).apply(summarize_stops_with_loc, include_groups=False)\n",
    "            # we add a 5min duration to oracle stops with just one ping\n",
    "            stops = utils.pad_short_stops(stops, pad=5, dur_min=0, start_timestamp = 'start_timestamp')\n",
    "\n",
    "        # ------------------------------ COMPUTE METRICS OF INTEREST -------------------------------------\n",
    "        # general metrics\n",
    "        overlaps = overlapping_visits(left=stops,\n",
    "                                      right=truth,\n",
    "                                      location_id='building_id',\n",
    "                                      match_location=False)\n",
    "\n",
    "        errors = compute_visitation_errors(overlaps=overlaps,\n",
    "                                           true_visits=truth,\n",
    "                                           location_id='building_id')\n",
    "\n",
    "        prf1 = compute_precision_recall_f1(overlaps=overlaps,\n",
    "                                           pred_visits=stops,\n",
    "                                           true_visits=truth,\n",
    "                                           location_id='building_id')\n",
    "        \n",
    "        all_metrics = {**errors, **prf1, 'user': user, 'algorithm': algo}\n",
    "        \n",
    "        \n",
    "        all_metrics_df = pd.concat([all_metrics_df, pd.DataFrame([all_metrics])], ignore_index=True)\n",
    "\n",
    "        # WARNING THIS IS WRONG...\n",
    "        # TO DO: The true way to filter stops, is not with attributes based on PREDICTED location\n",
    "        # instead, we need clip the stop table to the intervals given by the truth table. >>> PACO\n",
    "        \n",
    "        # size metrics\n",
    "        stops_with_attr = stops.merge(poi_table[['building_id', 'building_size', 'building_type']], on='building_id')\n",
    "\n",
    "        for build_size in ['small', 'medium', 'big']:    \n",
    "            if (truth.building_size==build_size).sum() == 0:\n",
    "                continue\n",
    "                \n",
    "            truth_subset = truth.loc[truth.building_size == build_size]\n",
    "\n",
    "            overlaps = overlapping_visits(\n",
    "                            left=stops_with_attr,\n",
    "                            right=truth_subset,\n",
    "                            location_id='building_id',\n",
    "                            match_location=False)\n",
    "            \n",
    "            errors = compute_visitation_errors(overlaps=overlaps,\n",
    "                                               true_visits=truth_subset,\n",
    "                                               location_id='building_id')\n",
    "            \n",
    "            prf1 = compute_precision_recall_f1(overlaps=overlaps,\n",
    "                                               pred_visits=stops_with_attr,\n",
    "                                               true_visits=truth_subset,\n",
    "                                               location_id='building_id')\n",
    "            \n",
    "            \n",
    "            metrics_size = {**errors, **prf1, 'user': user, 'algorithm': algo, 'building_size':build_size}\n",
    "            metrics_size_df = pd.concat([metrics_size_df, pd.DataFrame([metrics_size])], ignore_index=True)\n",
    "\n",
    "        # btype metrics\n",
    "        for building_type in ['home', 'retail', 'work', 'park']:    \n",
    "            if (truth.building_type==building_type).sum() == 0:\n",
    "                continue\n",
    "\n",
    "            truth_subset = truth.loc[truth.building_type==building_type]\n",
    "                \n",
    "            overlaps = overlapping_visits(\n",
    "                            left=stops_with_attr,\n",
    "                            right=truth_subset,\n",
    "                            location_id='building_id',\n",
    "                            match_location=False)\n",
    "            \n",
    "            errors = compute_visitation_errors(overlaps=overlaps,\n",
    "                                               true_visits=truth_subset,\n",
    "                                               location_id='building_id')\n",
    "            \n",
    "            prf1 = compute_precision_recall_f1(overlaps=overlaps,\n",
    "                                               pred_visits=stops_with_attr,\n",
    "                                               true_visits=truth_subset,\n",
    "                                               location_id='building_id')\n",
    "            \n",
    "            \n",
    "            metrics_btype = {**errors, **prf1, 'user': user, 'algorithm': algo, 'building_type':building_type}\n",
    "            metrics_btype_df = pd.concat([metrics_btype_df, pd.DataFrame([metrics_btype])], ignore_index=True)\n",
    "\n",
    "        # btype metrics\n",
    "        for dwell_length in ['low', 'mid', 'high']:    \n",
    "            if (truth.dwell_length==dwell_length).sum() == 0:\n",
    "                continue\n",
    "\n",
    "            truth_subset = truth.loc[truth.dwell_length==dwell_length]\n",
    "                \n",
    "            overlaps = overlapping_visits(\n",
    "                            left=stops_with_attr,\n",
    "                            right=truth_subset,\n",
    "                            location_id='building_id',\n",
    "                            match_location=False)\n",
    "            \n",
    "            errors = compute_visitation_errors(overlaps=overlaps,\n",
    "                                               true_visits=truth_subset,\n",
    "                                               location_id='building_id')\n",
    "            \n",
    "            prf1 = compute_precision_recall_f1(overlaps=overlaps,\n",
    "                                               pred_visits=stops_with_attr,\n",
    "                                               true_visits=truth_subset,\n",
    "                                               location_id='building_id')\n",
    "            \n",
    "            \n",
    "            metrics_dwell = {**errors, **prf1, 'user': user, 'algorithm': algo, 'dwell_length':dwell_length}\n",
    "            metrics_dwell_df = pd.concat([metrics_dwell_df, pd.DataFrame([metrics_dwell])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa584f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse1 = sparse_df[sparse_df['uid'] == 'modest_borg'].copy()\n",
    "sparse1_labels = HDBSCAN.hdbscan_labels(\n",
    "                traj=sparse1,\n",
    "                time_thresh=240,\n",
    "                min_pts=2,\n",
    "                min_cluster_size=2,\n",
    "                traj_cols=traj_cols)\n",
    "\n",
    "sparse1_labels.name = 'cluster'\n",
    "sparse1_with_cluster = sparse1.join(sparse1_labels)\n",
    "pred1 = visits.point_in_polygon(data=sparse1_with_cluster,\n",
    "                                            poi_table=poi_table,\n",
    "                                            method='majority',\n",
    "                                            data_crs='EPSG:3857',\n",
    "                                            max_distance=15,\n",
    "                                            cluster_label='cluster',\n",
    "                                            location_id='building_id',\n",
    "                                            x='x',\n",
    "                                            y='y')\n",
    "pred1 = sparse1_with_cluster.join(pred1)\n",
    "pred1['building_id'] = pred1['building_id'].fillna('No Stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8652e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.remove_overlaps(pred=pred1,\n",
    "                   time_thresh=240,\n",
    "                   traj_cols=traj_cols,\n",
    "                   post_processing='polygon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a3c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapping_visits(left=stops,\n",
    "                   right=truth,\n",
    "                   location_id='building_id',\n",
    "                   match_location=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c581b3b7-e479-4b87-9de4-6d9da743ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_df.groupby(['algorithm'])[['precision','recall','f1']].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4657c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_size_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_size_df.groupby(['building_size', 'algorithm'])[['precision', 'recall', 'f1']].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52af0753-1d9b-4d2d-a659-2e65086665d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first group: the three error‐fractions, share y‐axis [0,1]\n",
    "frac_metrics = [\"missed_fraction\", \"merged_fraction\", \"split_fraction\"]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4), sharey=True)\n",
    "for ax, metric in zip(axes, frac_metrics):\n",
    "    all_metrics_df.boxplot(column=metric, by=\"algorithm\", ax=ax)\n",
    "    ax.set_title(metric.replace(\"_\", \" \").title())\n",
    "    ax.set_xlabel(\"\")           # drop the “by …” label\n",
    "    ax.set_ylabel(metric if ax is axes[0] else \"\")  # only leftmost gets ylabel\n",
    "    ax.grid(True)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_df.groupby(['algorithm'])[['missed_fraction','merged_fraction','split_fraction','precision','recall','f1']].mean()\n",
    "\n",
    "bootstrapping=True\n",
    "if bootstrapping:\n",
    "    output = []\n",
    "    agg_keys = ['missed_fraction','merged_fraction','split_fraction','precision','recall','f1']\n",
    "    agg_dict = {key:'median' for key in agg_keys}\n",
    "    for _ in range(100):\n",
    "        output += [all_metrics_df.sample(len(all_metrics_df), replace=True).groupby('algorithm', as_index=False).agg(agg_dict)]\n",
    "    metrics_bootstrap_df = pd.concat(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d2aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first group: the three error‐fractions, share y‐axis [0,1]\n",
    "frac_metrics = [\"missed_fraction\", \"merged_fraction\", \"split_fraction\"]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4), sharey=True)\n",
    "for ax, metric in zip(axes, frac_metrics):\n",
    "    metrics_bootstrap_df.boxplot(column=metric, by=\"algorithm\", ax=ax)\n",
    "    ax.set_title(metric.replace(\"_\", \" \").title())\n",
    "    ax.set_xlabel(\"\")           # drop the “by …” label\n",
    "    ax.set_ylabel(metric if ax is axes[0] else \"\")  # only leftmost gets ylabel\n",
    "    ax.grid(True)\n",
    "fig.suptitle(\"Errors\")\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d28bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prf_metrics = [\"precision\", \"recall\", \"f1\"]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "for ax, metric in zip(axes, prf_metrics):\n",
    "    metrics_bootstrap_df.boxplot(column=metric, by=\"algorithm\", ax=ax)\n",
    "    ax.set_title(metric.upper() if metric==\"f1\" else metric.title())\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(metric if ax is axes[0] else \"\")\n",
    "    ax.grid(True)\n",
    "fig.suptitle(\"Metrics\")\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef94453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n",
    "\n",
    "algo_pairs = [('oracle', 'hdbscan'), ('oracle', 'ta-dbscan'), ('oracle', 'lachesis')]\n",
    "colors = {'oracle': 'royalblue', 'hdbscan': 'darkgreen', 'ta-dbscan': 'limegreen', 'lachesis': 'palevioletred'}\n",
    "\n",
    "for ax, (algo1, algo2) in zip(axes, algo_pairs):\n",
    "    filtered = all_metrics_df[all_metrics_df['algorithm'].isin([algo1, algo2])].copy()\n",
    "    filtered = filtered.merge(completeness_df, left_on='user', right_on='uid', how='left')\n",
    "\n",
    "    for algo in [algo1, algo2]:\n",
    "        subset = filtered[filtered['algorithm'] == algo].sort_values('q_stat')\n",
    "        smoothed = lowess(subset['f1'], subset['q_stat'], frac=0.4)\n",
    "        ax.plot(smoothed[:, 0], smoothed[:, 1], label=algo, color=colors[algo], linewidth=3)\n",
    "        ax.scatter(subset['q_stat'], subset['f1'], color=colors[algo], alpha=0.5, s=5)\n",
    "\n",
    "    ax.set_xlabel(\"q (Trajectory Completeness)\", fontsize=12)\n",
    "    ax.tick_params(axis='y', labelsize=12)\n",
    "    ax.tick_params(axis='x', labelsize=12)\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True)\n",
    "\n",
    "axes[0].set_ylabel(\"F1 Score\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"f1_score_vs_completeness.png\", dpi=300)\n",
    "plt.savefig(\"f1_score_vs_completeness.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2460fd-d2fc-48f1-98a1-274ab89b0a91",
   "metadata": {},
   "source": [
    "## Exploration of building size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50712748",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_plotting = [metrics_size_df, metrics_btype_df, metrics_dwell_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14141c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_keys = ['missed_fraction','merged_fraction','split_fraction','precision','recall','f1']\n",
    "agg_dict = {key:'median' for key in agg_keys}\n",
    "\n",
    "output = []\n",
    "for _ in range(100):\n",
    "    output += [metrics_size_df.sample(len(metrics_size_df), replace=True).groupby(['algorithm', 'building_size'], as_index=False).agg(agg_dict)]\n",
    "\n",
    "size_bootstrap_df = pd.concat(output)\n",
    "\n",
    "output = []\n",
    "for _ in range(100):\n",
    "    output += [metrics_btype_df.sample(len(metrics_btype_df), replace=True).groupby(['algorithm', 'building_type'], as_index=False).agg(agg_dict)]\n",
    "\n",
    "btype_bootstrap_df = pd.concat(output)\n",
    "\n",
    "output = []\n",
    "for _ in range(100):\n",
    "    output += [metrics_dwell_df.sample(len(metrics_dwell_df), replace=True).groupby(['algorithm', 'dwell_length'], as_index=False).agg(agg_dict)]\n",
    "\n",
    "dwell_bootstrap_df = pd.concat(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549036fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prf_metrics = [\"precision\", \"recall\", \"f1\"]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=False)\n",
    "\n",
    "for ax, metric in zip(axes, prf_metrics):\n",
    "    sns.boxplot(\n",
    "        data=size_bootstrap_df,\n",
    "        x=\"building_size\",        # group on building size\n",
    "        y=metric,\n",
    "        hue=\"algorithm\",          # different colors for each algorithm\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(metric.upper() if metric == \"f1\" else metric.title())\n",
    "    ax.set_ylabel(metric if ax is axes[0] else \"\")\n",
    "    ax.set_xlabel(\"Building Size\")\n",
    "    ax.grid(True)\n",
    "\n",
    "# fig.suptitle(\"Metrics by Building Size and Algorithm\", fontsize=16)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e27db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prf_metrics = [\"precision\", \"recall\", \"f1\"]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=False)\n",
    "\n",
    "for ax, metric in zip(axes, prf_metrics):\n",
    "    sns.boxplot(\n",
    "        data=btype_bootstrap_df,\n",
    "        x=\"building_type\",        # group on building size\n",
    "        y=metric,\n",
    "        hue=\"algorithm\",          # different colors for each algorithm\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(metric.upper() if metric == \"f1\" else metric.title())\n",
    "    ax.set_ylabel(metric if ax is axes[0] else \"\")\n",
    "    ax.set_xlabel(\"Building Type\")\n",
    "    ax.grid(True)\n",
    "\n",
    "# fig.suptitle(\"Metrics by Building Type and Algorithm\", fontsize=16)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a194c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prf_metrics = [\"precision\", \"recall\", \"f1\"]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=False)\n",
    "\n",
    "for ax, metric in zip(axes, prf_metrics):\n",
    "    sns.boxplot(\n",
    "        data=dwell_bootstrap_df,\n",
    "        x=\"dwell_length\",        # group on building size\n",
    "        y=metric,\n",
    "        hue=\"algorithm\",          # different colors for each algorithm\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(metric.upper() if metric == \"f1\" else metric.title())\n",
    "    ax.set_ylabel(metric if ax is axes[0] else \"\")\n",
    "    ax.set_xlabel(\"Dwell Length\")\n",
    "    ax.grid(True)\n",
    "\n",
    "# fig.suptitle(\"Metrics by Dwell Length and Algorithm\", fontsize=16)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba4ffcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (daphme)",
   "language": "python",
   "name": "daphme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
