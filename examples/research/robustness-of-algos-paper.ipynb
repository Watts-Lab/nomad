{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20c2bbc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-muted')\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "\n",
    "from pyproj import Transformer\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import numpy.random as npr\n",
    "import matplotlib.dates as mdates\n",
    "from itertools import product\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nomad.city_gen import City, Building, Street\n",
    "import nomad.traj_gen as tg\n",
    "from nomad.traj_gen import Agent, Population\n",
    "import nomad.stop_detection.ta_dbscan as DBSCAN\n",
    "import nomad.stop_detection.lachesis as Lachesis\n",
    "from nomad.constants import DEFAULT_SPEEDS, FAST_SPEEDS, SLOW_SPEEDS, DEFAULT_STILL_PROBS\n",
    "from nomad.constants import FAST_STILL_PROBS, SLOW_STILL_PROBS, ALLOWED_BUILDINGS\n",
    "\n",
    "from metrics import nearest, mode, prepare_diary, prepare_stop_table, cluster_metrics\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132f841-e679-4024-9b25-6a9df9764b92",
   "metadata": {},
   "source": [
    "# Robustness to sparsity in pre-processing of human mobility data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572296dc-9bdc-40ab-b030-fd58cb374426",
   "metadata": {},
   "source": [
    "**Francisco Jose Barreras**, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA, USA\n",
    "\n",
    "**Thomas Li**, Department of Economics, University of Pennsylvania, Philadelphia, PA, USA\n",
    "\n",
    "**Duncan Watts**, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA, USA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fb9c37-4523-4482-b92a-dcb83d524c6f",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf4d69d-db7c-4c11-896e-aa6ccd1eb83f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68c01d62-ee41-4ec1-af6b-bcc78204e9f3",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d550feba-2bc3-40df-8fe1-8b8ca8e9f878",
   "metadata": {},
   "source": [
    "- We explain garden city and how we produced ground truth, heavily referencing the Arxiv version\n",
    "- We divide in three sections after that\n",
    "- Possible problems in stop detection\n",
    "- Are these problems present in real (global) trajectories? + covariates\n",
    "- Can the parameterizations address these issues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea05e21c-520a-41eb-8675-09268e6d9fc1",
   "metadata": {},
   "source": [
    "There are regimes in which certain problems disappear.\n",
    "- E.g. going from extremely complete signals to very complete signals, might not make a difference in terms of quality---they are both good enough. This analysis could be visualized in a plot with q in the x-axis, showing that for high q and very low q things break down. \n",
    "- E.g. a very bad parameterization of DBScan for retail, might obscure a small effect of beta ping on retail. \n",
    "- Some problems might mostly affect users that explore a lot (this is application-relevant). \n",
    "- Maybe the conclusion is that you need a hierarchical DBScan\n",
    "\n",
    "We show problems. Can we THEN show solutions? Then maybe we can hint at solutions while describing problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a32733f-26f7-4d37-91dd-79657cab953a",
   "metadata": {},
   "source": [
    "## Possible errors in stop detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2304c4-dfb2-4f11-a9bc-b28fd5cac63b",
   "metadata": {},
   "source": [
    "Related to the intensity within a burst (beta_ping) and include merging, splitting, merging pings from the street (are there bursts of several pings per minute in the real data?). Related to building area as well. \n",
    "- Merging by ground-truth dwell time and area. Using \"default\" DBScan and pick a signal sparsity in which there's plenty a chance to observe merging and splitting (perhaps remarking that ppl with short bursts don't even have this problem -> appendix).\n",
    "- Splitting actually is mediated by two parameters, beta_ping and beta_start. Two resolutions for the same type of problem. However, maybe beta_ping explains splitting when there is a large area, but beta_start explains splitting when there's a long dwell.\n",
    "\n",
    "Experiment: neighboring buildings (e.g. there is merging -- effect could be modulated by amount of noise, building area)\n",
    "\n",
    "Experiment setup\n",
    "- we could combine into one diary and we visit neighboring buildings that are small and neighboring buildings that are larger\n",
    "- a big long burst so we're not worried about beta_start, beta_dur. vary beta_ping.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459fbcc6-cb76-4e25-a607-8b6584faa570",
   "metadata": {},
   "source": [
    "## Global problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a619daca-fa35-418c-9cfb-78bb33d76734",
   "metadata": {},
   "source": [
    "Related to the intensity and clustering of the bursts. It can affect if stops are missed (recall), it can also mess with accuracy by building type (typically long/short dwells).\n",
    "\n",
    "- Missingness is more severe for home and work compared to retail and park. Maybe a ratio statistic alongside the magnitude decreases. Pick a \"reasonable\" DBScan (based on recall). Parameters of a typical user in a typical dataset (avoid a regime where nothing/everything works). \n",
    "- Obvious plot is you have less signal (gray rectangles) then you miss more stops/time-at-right-stop overall. Show regimes? Specially for exploration-prone users? + nuance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fdbab3-915b-4463-b1c2-528a850ef497",
   "metadata": {},
   "source": [
    "## Parameterization problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8204a114-9b40-46b6-a761-62834642b437",
   "metadata": {},
   "source": [
    "You can truly choose the wrong parameters, and you might want to incorporate user signal parameters and building areas, and whatnot. Quality of algorithms and how to choose. x-axis is parameter of DBScan and y-axis is quality of clusters.\n",
    "- What would you do with a complete and regular signal? Maybe a \"reasonable\" DBScan would fail miserably and Lachesis would succeed? min_pts would save the day? \n",
    "- Time parameter ranges from 1 hour to 16 hours. Long-dwell \"bridging\" of huge gaps would increase (obviously) but, the nuance is overestimating time at work or at home. Absolute vs Relative. \n",
    "- Epsilon affects splitting and merging. Pick a \"default and reasonable\" beta_ping and change epsilon. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fedb635",
   "metadata": {},
   "source": [
    "Initialization of city and population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "698353df-7777-445e-bff1-204a39af27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nomad.city_gen as cg\n",
    "city = cg.load('../garden-city.pkl')\n",
    "\n",
    "N = 10\n",
    "population_1000 = Population(city)\n",
    "population_1000.generate_agents(N = N, \n",
    "                                start_time = pd.Timestamp('2024-01-01 07:00', tz='America/New_York'),\n",
    "                                seed = 100)\n",
    "\n",
    "for i, agent_id in enumerate(population_1000.roster):\n",
    "    agent = population_1000.roster[agent_id]\n",
    "    population_1000.generate_trajectory(agent, \n",
    "                                        T = pd.Timestamp('2024-01-08 07:00', tz='America/New_York'),\n",
    "                                        seed=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e5e08-ec3a-4aa1-be87-d0b4f9a4985d",
   "metadata": {},
   "source": [
    "## Run DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d80a661d-bb26-4463-ba06-e0347684a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_param = (120, 2, 4)\n",
    "sparsity_param = (120, 30, 10)\n",
    "traj_cols = {'user_id':'identifier',\n",
    "             'latitude':'x',\n",
    "             'longitude':'y',\n",
    "             'datetime':'local_timestamp'}\n",
    "seed = 100\n",
    "\n",
    "agent = population_1000.roster['quirky_chebyshev']\n",
    "agent.sample_traj_hier_nhpp(*sparsity_param, seed=seed, ha=3/4, replace_sparse_traj=True)\n",
    "dbscan_out = DBSCAN._temporal_dbscan_labels(agent.sparse_traj, *dbscan_param, traj_cols)\n",
    "stop_table = DBSCAN.temporal_dbscan(agent.sparse_traj, *dbscan_param, traj_cols, complete_output=False)\n",
    "\n",
    "stop_table = mode(dbscan_out, agent.sparse_traj, stop_table, city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f055448c-c73b-4f02-99c1-93797b2a9e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare diary: create stop IDs\n",
    "dt = agent.dt\n",
    "diary = prepare_diary(agent, dt, city)\n",
    "\n",
    "# Prepare stop table: map detected stops to diary stops\n",
    "stop_table = prepare_stop_table(stop_table, diary, dt)\n",
    "\n",
    "# Count number of rows in stop_table for each stop_id in diary\n",
    "stop_table_exploded = stop_table.explode(\"mapped_stop_ids\").rename(columns={\"mapped_stop_ids\": \"stop_id\"})\n",
    "stop_counts = stop_table_exploded[\"stop_id\"].value_counts().reset_index()\n",
    "stop_counts.columns = [\"stop_id\", \"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "df2f605b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>unix_timestamp</th>\n",
       "      <th>tz_offset</th>\n",
       "      <th>duration</th>\n",
       "      <th>location</th>\n",
       "      <th>identifier</th>\n",
       "      <th>end_time</th>\n",
       "      <th>stop_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 07:00:00-05:00</td>\n",
       "      <td>1704110400</td>\n",
       "      <td>-18000</td>\n",
       "      <td>61</td>\n",
       "      <td>h-x11-y13</td>\n",
       "      <td>quirky_chebyshev</td>\n",
       "      <td>2024-01-01 08:01:00-05:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01 08:01:00-05:00</td>\n",
       "      <td>1704114060</td>\n",
       "      <td>-18000</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>quirky_chebyshev</td>\n",
       "      <td>2024-01-01 08:07:00-05:00</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 08:07:00-05:00</td>\n",
       "      <td>1704114420</td>\n",
       "      <td>-18000</td>\n",
       "      <td>234</td>\n",
       "      <td>w-x18-y8</td>\n",
       "      <td>quirky_chebyshev</td>\n",
       "      <td>2024-01-01 12:01:00-05:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01 12:01:00-05:00</td>\n",
       "      <td>1704128460</td>\n",
       "      <td>-18000</td>\n",
       "      <td>9</td>\n",
       "      <td>None</td>\n",
       "      <td>quirky_chebyshev</td>\n",
       "      <td>2024-01-01 12:10:00-05:00</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01 12:10:00-05:00</td>\n",
       "      <td>1704129000</td>\n",
       "      <td>-18000</td>\n",
       "      <td>6</td>\n",
       "      <td>r-x4-y21</td>\n",
       "      <td>quirky_chebyshev</td>\n",
       "      <td>2024-01-01 12:16:00-05:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2024-01-07 18:10:00-05:00</td>\n",
       "      <td>1704669000</td>\n",
       "      <td>-18000</td>\n",
       "      <td>6</td>\n",
       "      <td>r-x3-y5</td>\n",
       "      <td>quirky_chebyshev</td>\n",
       "      <td>2024-01-07 18:16:00-05:00</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2024-01-07 18:16:00-05:00</td>\n",
       "      <td>1704669360</td>\n",
       "      <td>-18000</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>quirky_chebyshev</td>\n",
       "      <td>2024-01-07 18:22:00-05:00</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2024-01-07 18:22:00-05:00</td>\n",
       "      <td>1704669720</td>\n",
       "      <td>-18000</td>\n",
       "      <td>234</td>\n",
       "      <td>h-x11-y13</td>\n",
       "      <td>quirky_chebyshev</td>\n",
       "      <td>2024-01-07 22:16:00-05:00</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2024-01-07 22:16:00-05:00</td>\n",
       "      <td>1704683760</td>\n",
       "      <td>-18000</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>quirky_chebyshev</td>\n",
       "      <td>2024-01-07 22:20:00-05:00</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2024-01-07 22:20:00-05:00</td>\n",
       "      <td>1704684000</td>\n",
       "      <td>-18000</td>\n",
       "      <td>521</td>\n",
       "      <td>h-x6-y13</td>\n",
       "      <td>quirky_chebyshev</td>\n",
       "      <td>2024-01-08 07:01:00-05:00</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   start_time  unix_timestamp  tz_offset  duration   location  \\\n",
       "0   2024-01-01 07:00:00-05:00      1704110400     -18000        61  h-x11-y13   \n",
       "1   2024-01-01 08:01:00-05:00      1704114060     -18000         6       None   \n",
       "2   2024-01-01 08:07:00-05:00      1704114420     -18000       234   w-x18-y8   \n",
       "3   2024-01-01 12:01:00-05:00      1704128460     -18000         9       None   \n",
       "4   2024-01-01 12:10:00-05:00      1704129000     -18000         6   r-x4-y21   \n",
       "..                        ...             ...        ...       ...        ...   \n",
       "126 2024-01-07 18:10:00-05:00      1704669000     -18000         6    r-x3-y5   \n",
       "127 2024-01-07 18:16:00-05:00      1704669360     -18000         6       None   \n",
       "128 2024-01-07 18:22:00-05:00      1704669720     -18000       234  h-x11-y13   \n",
       "129 2024-01-07 22:16:00-05:00      1704683760     -18000         4       None   \n",
       "130 2024-01-07 22:20:00-05:00      1704684000     -18000       521   h-x6-y13   \n",
       "\n",
       "           identifier                  end_time  stop_id  \n",
       "0    quirky_chebyshev 2024-01-01 08:01:00-05:00        0  \n",
       "1    quirky_chebyshev 2024-01-01 08:07:00-05:00       -1  \n",
       "2    quirky_chebyshev 2024-01-01 12:01:00-05:00        1  \n",
       "3    quirky_chebyshev 2024-01-01 12:10:00-05:00       -1  \n",
       "4    quirky_chebyshev 2024-01-01 12:16:00-05:00        2  \n",
       "..                ...                       ...      ...  \n",
       "126  quirky_chebyshev 2024-01-07 18:16:00-05:00       63  \n",
       "127  quirky_chebyshev 2024-01-07 18:22:00-05:00       -1  \n",
       "128  quirky_chebyshev 2024-01-07 22:16:00-05:00       64  \n",
       "129  quirky_chebyshev 2024-01-07 22:20:00-05:00       -1  \n",
       "130  quirky_chebyshev 2024-01-08 07:01:00-05:00       65  \n",
       "\n",
       "[131 rows x 8 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "23b19d18-80b5-46ca-b804-3232a74e62bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>duration</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>location</th>\n",
       "      <th>end_time</th>\n",
       "      <th>stop_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-07 03:10:00-05:00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>11.588966</td>\n",
       "      <td>14.903447</td>\n",
       "      <td>h-x11-y13</td>\n",
       "      <td>2024-01-07 03:55:00-05:00</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-06 14:40:00-05:00</td>\n",
       "      <td>194.0</td>\n",
       "      <td>4.887563</td>\n",
       "      <td>7.782111</td>\n",
       "      <td>w-x3-y6</td>\n",
       "      <td>2024-01-06 17:54:00-05:00</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-05 20:00:00-05:00</td>\n",
       "      <td>460.0</td>\n",
       "      <td>7.744317</td>\n",
       "      <td>13.373035</td>\n",
       "      <td>h-x6-y13</td>\n",
       "      <td>2024-01-06 03:40:00-05:00</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-04 22:38:00-05:00</td>\n",
       "      <td>104.0</td>\n",
       "      <td>11.990007</td>\n",
       "      <td>14.556264</td>\n",
       "      <td>h-x11-y13</td>\n",
       "      <td>2024-01-05 00:22:00-05:00</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-04 14:10:00-05:00</td>\n",
       "      <td>135.0</td>\n",
       "      <td>17.001706</td>\n",
       "      <td>9.808945</td>\n",
       "      <td>w-x15-y9</td>\n",
       "      <td>2024-01-04 16:25:00-05:00</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-01-03 14:14:00-05:00</td>\n",
       "      <td>99.0</td>\n",
       "      <td>12.243575</td>\n",
       "      <td>17.321162</td>\n",
       "      <td>w-x13-y18</td>\n",
       "      <td>2024-01-03 15:53:00-05:00</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-01-03 12:23:00-05:00</td>\n",
       "      <td>69.0</td>\n",
       "      <td>10.920442</td>\n",
       "      <td>10.687370</td>\n",
       "      <td>p-x13-y11</td>\n",
       "      <td>2024-01-03 13:32:00-05:00</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-01-02 18:54:00-05:00</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11.505380</td>\n",
       "      <td>14.343611</td>\n",
       "      <td>h-x11-y13</td>\n",
       "      <td>2024-01-02 20:02:00-05:00</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-01-01 08:35:00-05:00</td>\n",
       "      <td>58.0</td>\n",
       "      <td>17.565845</td>\n",
       "      <td>9.696820</td>\n",
       "      <td>w-x18-y8</td>\n",
       "      <td>2024-01-01 09:33:00-05:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 start_time  duration          x          y   location  \\\n",
       "0 2024-01-07 03:10:00-05:00      45.0  11.588966  14.903447  h-x11-y13   \n",
       "1 2024-01-06 14:40:00-05:00     194.0   4.887563   7.782111    w-x3-y6   \n",
       "2 2024-01-05 20:00:00-05:00     460.0   7.744317  13.373035   h-x6-y13   \n",
       "3 2024-01-04 22:38:00-05:00     104.0  11.990007  14.556264  h-x11-y13   \n",
       "4 2024-01-04 14:10:00-05:00     135.0  17.001706   9.808945   w-x15-y9   \n",
       "5 2024-01-03 14:14:00-05:00      99.0  12.243575  17.321162  w-x13-y18   \n",
       "6 2024-01-03 12:23:00-05:00      69.0  10.920442  10.687370  p-x13-y11   \n",
       "7 2024-01-02 18:54:00-05:00      68.0  11.505380  14.343611  h-x11-y13   \n",
       "8 2024-01-01 08:35:00-05:00      58.0  17.565845   9.696820   w-x18-y8   \n",
       "\n",
       "                   end_time stop_id  \n",
       "0 2024-01-07 03:55:00-05:00      53  \n",
       "1 2024-01-06 17:54:00-05:00      52  \n",
       "2 2024-01-06 03:40:00-05:00      44  \n",
       "3 2024-01-05 00:22:00-05:00      32  \n",
       "4 2024-01-04 16:25:00-05:00      30  \n",
       "5 2024-01-03 15:53:00-05:00      21  \n",
       "6 2024-01-03 13:32:00-05:00      18  \n",
       "7 2024-01-02 20:02:00-05:00      14  \n",
       "8 2024-01-01 09:33:00-05:00       1  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_table_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a7136914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stop_id  count\n",
       "0       53      1\n",
       "1       52      1\n",
       "2       44      1\n",
       "3       32      1\n",
       "4       30      1\n",
       "5       21      1\n",
       "6       18      1\n",
       "7       14      1\n",
       "8        1      1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1c6073aa-805d-4835-8e0b-f4ec8e9b13f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [stop_id, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops_split = stop_counts[stop_counts[\"count\"] > 1]\n",
    "stops_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fb07cabc-316c-4435-ac98-99ddcf01cc33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>unix_timestamp</th>\n",
       "      <th>tz_offset</th>\n",
       "      <th>duration</th>\n",
       "      <th>location</th>\n",
       "      <th>identifier</th>\n",
       "      <th>end_time</th>\n",
       "      <th>stop_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 07:00:00-05:00</td>\n",
       "      <td>1704110400</td>\n",
       "      <td>-18000</td>\n",
       "      <td>61</td>\n",
       "      <td>h-x11-y13</td>\n",
       "      <td>quirky_chebyshev</td>\n",
       "      <td>2024-01-01 08:01:00-05:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01 08:01:00-05:00</td>\n",
       "      <td>1704114060</td>\n",
       "      <td>-18000</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>quirky_chebyshev</td>\n",
       "      <td>2024-01-01 08:07:00-05:00</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 08:07:00-05:00</td>\n",
       "      <td>1704114420</td>\n",
       "      <td>-18000</td>\n",
       "      <td>234</td>\n",
       "      <td>w-x18-y8</td>\n",
       "      <td>quirky_chebyshev</td>\n",
       "      <td>2024-01-01 12:01:00-05:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01 12:01:00-05:00</td>\n",
       "      <td>1704128460</td>\n",
       "      <td>-18000</td>\n",
       "      <td>9</td>\n",
       "      <td>None</td>\n",
       "      <td>quirky_chebyshev</td>\n",
       "      <td>2024-01-01 12:10:00-05:00</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01 12:10:00-05:00</td>\n",
       "      <td>1704129000</td>\n",
       "      <td>-18000</td>\n",
       "      <td>6</td>\n",
       "      <td>r-x4-y21</td>\n",
       "      <td>quirky_chebyshev</td>\n",
       "      <td>2024-01-01 12:16:00-05:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2024-01-07 18:10:00-05:00</td>\n",
       "      <td>1704669000</td>\n",
       "      <td>-18000</td>\n",
       "      <td>6</td>\n",
       "      <td>r-x3-y5</td>\n",
       "      <td>quirky_chebyshev</td>\n",
       "      <td>2024-01-07 18:16:00-05:00</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2024-01-07 18:16:00-05:00</td>\n",
       "      <td>1704669360</td>\n",
       "      <td>-18000</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>quirky_chebyshev</td>\n",
       "      <td>2024-01-07 18:22:00-05:00</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2024-01-07 18:22:00-05:00</td>\n",
       "      <td>1704669720</td>\n",
       "      <td>-18000</td>\n",
       "      <td>234</td>\n",
       "      <td>h-x11-y13</td>\n",
       "      <td>quirky_chebyshev</td>\n",
       "      <td>2024-01-07 22:16:00-05:00</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2024-01-07 22:16:00-05:00</td>\n",
       "      <td>1704683760</td>\n",
       "      <td>-18000</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>quirky_chebyshev</td>\n",
       "      <td>2024-01-07 22:20:00-05:00</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2024-01-07 22:20:00-05:00</td>\n",
       "      <td>1704684000</td>\n",
       "      <td>-18000</td>\n",
       "      <td>521</td>\n",
       "      <td>h-x6-y13</td>\n",
       "      <td>quirky_chebyshev</td>\n",
       "      <td>2024-01-08 07:01:00-05:00</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   start_time  unix_timestamp  tz_offset  duration   location  \\\n",
       "0   2024-01-01 07:00:00-05:00      1704110400     -18000        61  h-x11-y13   \n",
       "1   2024-01-01 08:01:00-05:00      1704114060     -18000         6       None   \n",
       "2   2024-01-01 08:07:00-05:00      1704114420     -18000       234   w-x18-y8   \n",
       "3   2024-01-01 12:01:00-05:00      1704128460     -18000         9       None   \n",
       "4   2024-01-01 12:10:00-05:00      1704129000     -18000         6   r-x4-y21   \n",
       "..                        ...             ...        ...       ...        ...   \n",
       "126 2024-01-07 18:10:00-05:00      1704669000     -18000         6    r-x3-y5   \n",
       "127 2024-01-07 18:16:00-05:00      1704669360     -18000         6       None   \n",
       "128 2024-01-07 18:22:00-05:00      1704669720     -18000       234  h-x11-y13   \n",
       "129 2024-01-07 22:16:00-05:00      1704683760     -18000         4       None   \n",
       "130 2024-01-07 22:20:00-05:00      1704684000     -18000       521   h-x6-y13   \n",
       "\n",
       "           identifier                  end_time  stop_id  \n",
       "0    quirky_chebyshev 2024-01-01 08:01:00-05:00        0  \n",
       "1    quirky_chebyshev 2024-01-01 08:07:00-05:00       -1  \n",
       "2    quirky_chebyshev 2024-01-01 12:01:00-05:00        1  \n",
       "3    quirky_chebyshev 2024-01-01 12:10:00-05:00       -1  \n",
       "4    quirky_chebyshev 2024-01-01 12:16:00-05:00        2  \n",
       "..                ...                       ...      ...  \n",
       "126  quirky_chebyshev 2024-01-07 18:16:00-05:00       63  \n",
       "127  quirky_chebyshev 2024-01-07 18:22:00-05:00       -1  \n",
       "128  quirky_chebyshev 2024-01-07 22:16:00-05:00       64  \n",
       "129  quirky_chebyshev 2024-01-07 22:20:00-05:00       -1  \n",
       "130  quirky_chebyshev 2024-01-08 07:01:00-05:00       65  \n",
       "\n",
       "[131 rows x 8 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4107652a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_id</th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>pings_merged</th>\n",
       "      <th>stops_merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.247863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>521.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    stop_id    tp     fp     fn  precision    recall  pings_merged  \\\n",
       "0         0   0.0   61.0    0.0        0.0  0.000000           0.0   \n",
       "1         1  58.0    0.0  176.0        1.0  0.247863           0.0   \n",
       "2         2   0.0    6.0    0.0        0.0  0.000000           0.0   \n",
       "3         3   0.0   53.0    0.0        0.0  0.000000           0.0   \n",
       "4         4   0.0   24.0    0.0        0.0  0.000000           0.0   \n",
       "..      ...   ...    ...    ...        ...       ...           ...   \n",
       "61       61   0.0   28.0    0.0        0.0  0.000000           0.0   \n",
       "62       62   0.0  233.0    0.0        0.0  0.000000           0.0   \n",
       "63       63   0.0    6.0    0.0        0.0  0.000000           0.0   \n",
       "64       64   0.0  234.0    0.0        0.0  0.000000           0.0   \n",
       "65       65   0.0  521.0    0.0        0.0  0.000000           0.0   \n",
       "\n",
       "    stops_merged  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "..           ...  \n",
       "61             0  \n",
       "62             0  \n",
       "63             0  \n",
       "64             0  \n",
       "65             0  \n",
       "\n",
       "[66 rows x 8 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_overlap(a_start, a_end, b_start, b_end):\n",
    "    latest_start = max(a_start, b_start)\n",
    "    earliest_end = min(a_end, b_end)\n",
    "    overlap = (earliest_end - latest_start).total_seconds() / 60\n",
    "    return max(0, overlap)\n",
    "\n",
    "tp_fp_fn = {}\n",
    "merging_dict = {}\n",
    "merged_stop_count_dict = {}\n",
    "\n",
    "# Efficient lookup\n",
    "diary_indexed = diary.set_index(\"stop_id\")\n",
    "\n",
    "# First: true positive / false positive / false negative\n",
    "for stop_id in diary[\"stop_id\"].unique():\n",
    "    if stop_id == -1:\n",
    "        continue\n",
    "\n",
    "    diary_start = diary_indexed.loc[stop_id][\"start_time\"]\n",
    "    diary_end = diary_indexed.loc[stop_id][\"end_time\"]\n",
    "    \n",
    "    diary_duration = (diary_end - diary_start).total_seconds() / 60\n",
    "    \n",
    "    if stop_id in stop_table_exploded[\"stop_id\"].values:\n",
    "        stop_row = stop_table_exploded[stop_table_exploded[\"stop_id\"] == stop_id].iloc[0]\n",
    "        stop_start = stop_row[\"start_time\"]\n",
    "        stop_end = stop_row[\"end_time\"]\n",
    "        stop_duration = (stop_end - stop_start).total_seconds() / 60\n",
    "\n",
    "        tp = compute_overlap(stop_start, stop_end, diary_start, diary_end)\n",
    "        fp = stop_duration - tp\n",
    "        fn = diary_duration - tp\n",
    "    else:\n",
    "        tp = 0\n",
    "        fp = diary_duration\n",
    "        fn = 0\n",
    "\n",
    "    if stop_id not in tp_fp_fn:\n",
    "        tp_fp_fn[stop_id] = [0, 0, 0]\n",
    "    tp_fp_fn[stop_id][0] += tp\n",
    "    tp_fp_fn[stop_id][1] += fp\n",
    "    tp_fp_fn[stop_id][2] += fn\n",
    "\n",
    "# Second: merging minutes per diary stop\n",
    "for _, d in diary[diary[\"stop_id\"] != -1].iterrows():\n",
    "    d_sid = d[\"stop_id\"]\n",
    "    d_start = d[\"start_time\"]\n",
    "    d_end = d[\"end_time\"]\n",
    "\n",
    "    overlaps = stop_table_exploded[\n",
    "        (stop_table_exploded[\"end_time\"] > d_start) &\n",
    "        (stop_table_exploded[\"start_time\"] < d_end)\n",
    "    ]\n",
    "\n",
    "    merging_minutes = 0\n",
    "    merged_stop_ids_seen = set()\n",
    "\n",
    "    for _, s in overlaps.iterrows():\n",
    "        s_sid = s[\"stop_id\"]\n",
    "        if s_sid != d_sid:\n",
    "            overlap_start = max(d_start, s[\"start_time\"])\n",
    "            overlap_end = min(d_end, s[\"end_time\"])\n",
    "            delta = (overlap_end - overlap_start).total_seconds() / 60\n",
    "            minutes = max(0, delta)\n",
    "        \n",
    "            if minutes > 0:\n",
    "                merging_minutes += minutes\n",
    "                merged_stop_ids_seen.add(s_sid)\n",
    "\n",
    "    if d_sid not in merging_dict:\n",
    "        merging_dict[d_sid] = 0\n",
    "    if d_sid not in merged_stop_count_dict:\n",
    "        merged_stop_count_dict[d_sid] = 0\n",
    "\n",
    "    merging_dict[d_sid] += merging_minutes\n",
    "    merged_stop_count_dict[d_sid] += len(merged_stop_ids_seen)\n",
    "\n",
    "stops_split = stop_counts[stop_counts[\"count\"] > 1]\n",
    "\n",
    "# Collect\n",
    "metrics_data = []\n",
    "for stop_id in diary[\"stop_id\"].unique():\n",
    "    if stop_id == -1:\n",
    "        continue\n",
    "\n",
    "    tp, fp, fn = tp_fp_fn.get(stop_id)\n",
    "    mm = merging_dict.get(stop_id)\n",
    "    sm = merged_stop_count_dict.get(stop_id)\n",
    "\n",
    "    metrics_data.append({\n",
    "        \"stop_id\": stop_id,\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn,\n",
    "        \"precision\": tp / (tp + fp) if (tp + fp) != 0 else 0,\n",
    "        \"recall\": tp / (tp + fn) if (tp + fn) != 0 else 0,\n",
    "        \"pings_merged\": mm,\n",
    "        \"stops_merged\": sm,\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fcaf01-13ec-4980-b474-823871d026f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Simulate 1000 Agents*\n",
    "\n",
    "Initialize 1000 agents and generate one week of ground-truth trajectory data for each.\n",
    "\n",
    "USE SYNTHETIC DATA!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846e2874",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "population_1000 = Population(city)\n",
    "population_1000.generate_agents(N = N, \n",
    "                                start_time = datetime(2024, 1, 1, hour=7, minute=0),\n",
    "                                seed = 100)\n",
    "\n",
    "for i, agent_id in enumerate(population_1000.roster):\n",
    "    agent = population_1000.roster[agent_id]\n",
    "    population_1000.generate_trajectory(agent, \n",
    "                                        T = datetime(2024, 1, 8, hour=7, minute=0),\n",
    "                                        seed=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a7903-4257-4932-9bee-42dd1ebaf2c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "N = 10\n",
    "\n",
    "population_1000 = Population(city)\n",
    "population_1000.generate_agents(N = N, \n",
    "                                start_time = datetime(2024, 1, 1, hour=7, minute=0),\n",
    "                                seed = 100)\n",
    "\n",
    "def generate_agent_trajectory(agent_id, shared_roster, seed):\n",
    "    agent = shared_roster[agent_id]\n",
    "    population_1000.generate_trajectory(agent, \n",
    "                                   T = datetime(2024, 1, 8, hour=7, minute=0),\n",
    "                                   seed=seed)\n",
    "    shared_roster[agent_id] = agent\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "shared_roster = manager.dict(population_1000.roster)\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(generate_agent_trajectory, agent_id, shared_roster, i)\n",
    "        for i, agent_id in enumerate(shared_roster)\n",
    "    ]\n",
    "\n",
    "    with tqdm(total=len(futures), desc=\"Generating agent trajectories\") as pbar:\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "                pbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Task failed with exception: {e}\")\n",
    "                pbar.update(1)\n",
    "\n",
    "population_1000.roster = dict(shared_roster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e17381b-d21a-41e0-b5b2-af0beed7663b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Global Problems\n",
    "\n",
    "Related to the intensity and clustering of the bursts. It can affect if stops are missed (recall), it can also mess with accuracy by building type (typically long/short dwells).\n",
    "\n",
    "- Missingness is more severe for home and work compared to retail and park. Maybe a ratio statistic alongside the magnitude decreases. Pick a \"reasonable\" DBScan (based on recall). Parameters of a typical user in a typical dataset (avoid a regime where nothing/everything works). \n",
    "- Obvious plot is you have less signal (gray rectangles) then you miss more stops/time-at-right-stop overall. Show regimes? Specially for exploration-prone users? + nuance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5900509-f6fd-4820-8a56-36698cde1d21",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EPR, recall and precision*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cadd3e-b521-49b6-9a75-e83e21bdf378",
   "metadata": {},
   "source": [
    "fix the agent and their ground-truth trajectory. vary sampling parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b13de7-7423-4334-b21d-4d5b56383177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beta_starts = range(120, 1600, 40)\n",
    "beta_durs = range(30, 400, 10)\n",
    "beta_ping = 10\n",
    "sparsity_params = [(start, dur, beta_ping) for start, dur in zip(beta_starts, beta_durs)]\n",
    "\n",
    "dbscan_params = [(120, 0.5, 4), (180, 2, 2)] \n",
    "lachesis_params = [(5, 120, 3)] # [(5, 120, 2), (5, 120, 1.5)] #dur_min, dt_max, and delta_roam\n",
    "\n",
    "N = 1  # how many times to run each agent-sparsity pair\n",
    "\n",
    "traj_cols = {'user_id':'identifier',\n",
    "             'latitude':'x',\n",
    "             'longitude':'y',\n",
    "             'datetime':'local_timestamp'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ee7091-f41b-4735-bdfe-c60e67c5bb08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# run without parallelization\n",
    "\n",
    "agent = population_1000.roster['quirky_chebyshev']\n",
    "\n",
    "def process_sample(sparsity_param, seed, agent, city, shared_metrics):\n",
    "    agent.sample_traj_hier_nhpp(*sparsity_param, seed=seed, ha=3/4, replace_sparse_traj=True)\n",
    "    qstat = q_stat(agent)\n",
    "\n",
    "    # Run DBScan\n",
    "    for j, dbscan_param in enumerate(dbscan_params):\n",
    "        dbscan_out = sd._temporal_dbscan_labels(agent.sparse_traj, *dbscan_param, traj_cols)\n",
    "        stop_table = sd.temporal_dbscan(agent.sparse_traj, *dbscan_param, traj_cols, complete_output=False)\n",
    "        stop_table = identify_stop(dbscan_out, agent.sparse_traj, stop_table, city, method='mode')\n",
    "        _, metrics = cluster_metrics(stop_table, agent)\n",
    "        merged = metrics['Stops Merged']\n",
    "        split = metrics['Split']\n",
    "        recall = metrics['Recall']\n",
    "        precision = metrics['Precision']\n",
    "        # Save metrics\n",
    "        shared_metrics.append([qstat, seed, agent.identifier, ['dbscan', 'dbscan2'][j], \n",
    "                               merged, split, recall, precision])\n",
    "\n",
    "    # Run Lachesis\n",
    "    for j, lachesis_param in enumerate(lachesis_params):\n",
    "        lachesis_out = sd._lachesis_labels(agent.sparse_traj, *lachesis_param, traj_cols)\n",
    "        stop_table = sd.lachesis(agent.sparse_traj, *lachesis_param, traj_cols, complete_output=False)\n",
    "        stop_table = identify_stop(lachesis_out, agent.sparse_traj, stop_table, city, method='mode')\n",
    "        _, metrics = cluster_metrics(stop_table, agent)\n",
    "        merged = metrics['Stops Merged']\n",
    "        split = metrics['Split']\n",
    "        recall = metrics['Recall']\n",
    "        precision = metrics['Precision']\n",
    "        # Save metrics\n",
    "        shared_metrics.append([qstat, seed, agent.identifier, ['lachesis', 'lachesis2'][j], \n",
    "                               merged, split, recall, precision])\n",
    "\n",
    "def sequential_work(sparsity_params, N, city):\n",
    "    shared_metrics = []\n",
    "\n",
    "    total_tasks = N*len(sparsity_params)\n",
    "    task_counter = 0\n",
    "\n",
    "    with tqdm(total=total_tasks, desc=\"Processing tasks\") as pbar:\n",
    "        for sparsity_param in sparsity_params:\n",
    "            for seed in range(N):\n",
    "                process_sample(sparsity_param, seed, Chebyshev, city, shared_metrics)\n",
    "                task_counter += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "    return shared_metrics\n",
    "\n",
    "epr_metrics = sequential_work(sparsity_params, N, city)\n",
    "epr_metrics = pd.DataFrame(\n",
    "    epr_metrics,\n",
    "    columns=['q', 'seed', 'agent', 'algorithm', 'merged', 'split', 'recall', 'precision']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f042e1-7bff-4cf6-bdb7-3afa4e7c2ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Run with parallelization\n",
    "\n",
    "def process_sample(sparsity_param, seed, agent_id, city, shared_metrics):\n",
    "    agent = population_1000.roster[agent_id]\n",
    "    agent.sample_traj_hier_nhpp(*sparsity_param, seed=seed, ha=3/4, replace_sparse_traj=True)\n",
    "    qstat = q_stat(agent)\n",
    "\n",
    "    # Run DBScan\n",
    "    for j, dbscan_param in enumerate(dbscan_params):\n",
    "        dbscan_out = sd._temporal_dbscan_labels(agent.sparse_traj, *dbscan_param, traj_cols)\n",
    "        stop_table = sd.temporal_dbscan(agent.sparse_traj, *dbscan_param, traj_cols, complete_output=False)\n",
    "        stop_table = identify_stop(dbscan_out, agent.sparse_traj, stop_table, city, method='mode')\n",
    "        _, metrics = cluster_metrics(stop_table, agent)\n",
    "        merged = metrics['Stops Merged']\n",
    "        prop_merged = metrics['Prop Stops Merged']\n",
    "        split = metrics['Split']\n",
    "        recall = metrics['Recall']\n",
    "        precision = metrics['Precision']\n",
    "        # Save metrics\n",
    "        shared_metrics.append([qstat, seed, agent_id, ['dbscan-fine', 'dbscan-coarse'][j], \n",
    "                               merged, prop_merged, split, recall, precision])\n",
    "\n",
    "    # Run Lachesis\n",
    "    for j, lachesis_param in enumerate(lachesis_params):\n",
    "        lachesis_out = sd._lachesis_labels(agent.sparse_traj, *lachesis_param, traj_cols)\n",
    "        stop_table = sd.lachesis(agent.sparse_traj, *lachesis_param, traj_cols, complete_output=False)\n",
    "        stop_table = identify_stop(lachesis_out, agent.sparse_traj, stop_table, city, method='mode')\n",
    "        _, metrics = cluster_metrics(stop_table, agent)\n",
    "        merged = metrics['Stops Merged']\n",
    "        prop_merged = metrics['Prop Stops Merged']\n",
    "        split = metrics['Split']\n",
    "        recall = metrics['Recall']\n",
    "        precision = metrics['Precision']\n",
    "        # Save metrics\n",
    "        shared_metrics.append([qstat, seed, agent_id, ['lachesis', 'lachesis2'][j],\n",
    "                               merged, prop_merged, split, recall, precision])\n",
    "    \n",
    "def parallelize_work(sparsity_params, N, city):\n",
    "    with multiprocessing.Manager() as manager:\n",
    "        shared_metrics = manager.list()\n",
    "\n",
    "        # Using ProcessPoolExecutor for parallelism\n",
    "        with ProcessPoolExecutor(max_workers=36) as executor:\n",
    "            tasks = [\n",
    "                executor.submit(\n",
    "                    process_sample,\n",
    "                    sparsity_param,\n",
    "                    seed,\n",
    "                    agent_id,\n",
    "                    city,\n",
    "                    shared_metrics\n",
    "                )\n",
    "                for agent_id in population_1000.roster\n",
    "                for sparsity_param in sparsity_params\n",
    "                for seed in range(N)\n",
    "            ]\n",
    "\n",
    "            total_tasks = N * len(population_1000.roster) * len(sparsity_params)\n",
    "\n",
    "            with tqdm(total=total_tasks, desc=\"Processing tasks\") as pbar:\n",
    "                for future in concurrent.futures.as_completed(tasks):\n",
    "                    try:\n",
    "                        future.result()\n",
    "                        pbar.update(1)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Task failed with exception: {e}\")\n",
    "                        pbar.update(1)\n",
    "\n",
    "        return list(shared_metrics)\n",
    "\n",
    "epr_metrics = parallelize_work(sparsity_params, N, city)\n",
    "epr_metrics = pd.DataFrame(list(epr_metrics),\n",
    "                           columns=['q', 'seed', 'agent', 'algorithm', 'merged', 'prop_merged', 'split', 'recall', 'precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f31cf42-f9b7-42ec-92a9-86d5a7258da9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"epr_metrics.pkl\", \"rb\") as f:\n",
    "    epr_metrics = pickle.load(f)\n",
    "\n",
    "# epr_metrics['f1_score'] = 2 * (epr_metrics['precision'] * epr_metrics['recall']) / (epr_metrics['precision'] + epr_metrics['recall'])\n",
    "# epr_metrics.to_pickle(\"epr_metrics.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978942b5-a81c-4aaa-81b9-bd2a1f88a80a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epr_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9fe0dc-855f-4161-95b2-5de88133fb3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epr_metrics_avg = (\n",
    "    epr_metrics.groupby(['q', 'algorithm'])['merged']\n",
    "    .agg(lambda x: x.mean() if len(x) > 10 else None)  # Only take mean if more than 10\n",
    "    .reset_index()\n",
    ")\n",
    "pivoted = epr_metrics_avg.pivot(index='q', columns='algorithm', values='merged')\n",
    "\n",
    "ax = pivoted.plot(\n",
    "    kind='line',\n",
    "    figsize=(5, 4)\n",
    ")\n",
    "\n",
    "ax.set_title('Effect of Sparsity on Stop Merging', fontsize=16)\n",
    "ax.set_xlabel('Sparsity (q)', fontsize=12)\n",
    "ax.set_ylabel('Number of Stops Merged', fontsize=12)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"epr-merging.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dbab71-a87b-46ae-aca1-020660e86ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epr_metrics_avg = (\n",
    "    epr_metrics.groupby(['q', 'algorithm'])['precision']\n",
    "    .agg(lambda x: x.mean() if len(x) > 10 else None)  # Only take mean if more than 10\n",
    "    .reset_index()\n",
    ")\n",
    "pivoted = epr_metrics_avg.pivot(index='q', columns='algorithm', values='precision')\n",
    "\n",
    "ax = pivoted.plot(\n",
    "    kind='line',\n",
    "    figsize=(5, 4)\n",
    ")\n",
    "\n",
    "ax.set_title('Effect of Sparsity on Precision', fontsize=16)\n",
    "ax.set_xlabel('Sparsity (q)', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"epr-precision.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948cfc2a-057e-479b-9007-487d0faa800c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epr_metrics_avg = (\n",
    "    epr_metrics.groupby(['q', 'algorithm'])['recall']\n",
    "    .agg(lambda x: x.mean() if len(x) > 10 else None)  # Only take mean if more than 10\n",
    "    .reset_index()\n",
    ")\n",
    "pivoted = epr_metrics_avg.pivot(index='q', columns='algorithm', values='recall')\n",
    "\n",
    "ax = pivoted.plot(\n",
    "    kind='line',\n",
    "    figsize=(5, 4)\n",
    ")\n",
    "\n",
    "ax.set_title('Effect of Sparsity on Recall', fontsize=16)\n",
    "ax.set_xlabel('Sparsity (q)', fontsize=12)\n",
    "ax.set_ylabel('Recall', fontsize=12)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"epr-recall.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd6598f-f654-4931-84c1-306a3a7695fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epr_metrics_avg = (\n",
    "    epr_metrics.groupby(['q', 'algorithm'])['f1_score']\n",
    "    .agg(lambda x: x.mean() if len(x) > 10 else None)  # Only take mean if more than 10\n",
    "    .reset_index()\n",
    ")\n",
    "pivoted = epr_metrics_avg.pivot(index='q', columns='algorithm', values='f1_score')\n",
    "\n",
    "ax = pivoted.plot(\n",
    "    kind='line',\n",
    "    figsize=(5, 4)\n",
    ")\n",
    "\n",
    "ax.set_title('Effect of Sparsity on F1', fontsize=16)\n",
    "ax.set_xlabel('Sparsity (q)', fontsize=12)\n",
    "ax.set_ylabel('F1 Score', fontsize=12)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"epr-f1.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ffb88-b182-47ff-8881-f7334a7b3e61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Proportion of stops missed v.s. amount of signal\n",
    "\n",
    "For each agent, sample their trajectory with different parameterizations. Compute some measure of completeness for their sampled trajectory (proportion of pings sampled, q, etc.). Compute the proportion of stops missed (recall, or we could go hour by hour?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb72b63c-c7a1-485e-98ec-583bf9886be4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def sample_agent_trajectory(agent_id, shared_roster, seed):\n",
    "    npr.seed(seed)\n",
    "    agent = shared_roster[agent_id]\n",
    "\n",
    "    # We could try different ranges and different distributions\n",
    "    beta_start = npr.uniform(60, 1200)\n",
    "    beta_duration = npr.uniform(15, 180)\n",
    "    beta_ping = npr.uniform(5, 30)\n",
    "\n",
    "    param = (beta_start, beta_duration, beta_ping)\n",
    "    agent.sample_traj_hier_nhpp(*param, seed=seed)\n",
    "    \n",
    "    # compute completeness\n",
    "    prop_sampled = len(agent.sparse_traj) / len(agent.trajectory)\n",
    "    q = q_stat(agent)\n",
    "    \n",
    "    sampling_stats.append((agent_id, beta_start, beta_duration, beta_ping, prop_sampled, q))\n",
    "\n",
    "    shared_roster[agent_id] = agent\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "shared_roster = manager.dict(population.roster)\n",
    "sampling_stats = manager.list()\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(sample_agent_trajectory, agent_id, shared_roster, i)\n",
    "        for i, agent_id in enumerate(shared_roster)\n",
    "    ]\n",
    "    concurrent.futures.wait(futures)\n",
    "\n",
    "population.roster = dict(shared_roster)\n",
    "sampling_stats = pd.DataFrame(list(sampling_stats), columns=['agent', 'beta_start', 'beta_duration', 'beta_ping', 'prop_sampled', 'q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc688057-01f5-4e11-b56e-f48dddedb7a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampling_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be926c5-2066-4b0b-865a-7049130ec887",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dbscan_params = [(180, 7, 2), (45, 2, 3)]\n",
    "\n",
    "def compute_stop_detection_metrics(agent_id, shared_roster, dbscan_params, seed):\n",
    "    agent = population.roster[agent_id]\n",
    "    combined = []\n",
    "    \n",
    "    for j, dbscan_param in enumerate(dbscan_params):\n",
    "        dbscan_out = sd.temporal_dbscan(agent.sparse_traj, *dbscan_param)\n",
    "        stop_table = sd.generate_stop_table(agent.sparse_traj, dbscan_out)\n",
    "        \n",
    "        if stop_table.empty:\n",
    "            continue\n",
    "        \n",
    "        stop_table = identify_stop(dbscan_out, agent.sparse_traj, stop_table)\n",
    "        _, metrics, _ = cluster_metrics(stop_table, agent)\n",
    "        \n",
    "        sd_metrics.append((agent_id, metrics['Recall'], metrics['Missed']/metrics['Stop Count'], ['coarse', 'fine'][j]))\n",
    "    \n",
    "    return pd.concat(combined, ignore_index=True) if combined else pd.DataFrame()\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "shared_roster = manager.dict(population.roster)\n",
    "sd_metrics = manager.list()\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(compute_stop_detection_metrics, agent_id, shared_roster, dbscan_params, i)\n",
    "        for i, agent_id in enumerate(shared_roster)\n",
    "    ]\n",
    "    concurrent.futures.wait(futures)\n",
    "\n",
    "sd_metrics = pd.DataFrame(list(sd_metrics), columns=['agent', 'recall', 'missed', 'dbscan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725f4c37-bd6c-4479-9962-e44fff61cd4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_df = pd.merge(sampling_stats, sd_metrics, on='agent', how='outer')\n",
    "combined_df\n",
    "\n",
    "with open('global-results.pkl', 'wb') as file:\n",
    "    pickle.dump(combined_df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47356457-8157-4a54-8047-9ebf04497c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('global-results.pkl', 'rb') as file:\n",
    "    combined_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ba53d6-284c-4c0b-aa2d-a2798a52d1b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bin_width = 0.025\n",
    "q_bins = np.arange(0, 1, bin_width)\n",
    "combined_df['q_bin'] = pd.cut(combined_df['q'], bins=q_bins, right=False)\n",
    "count_df = combined_df.groupby('q_bin').size().reset_index(name='count')\n",
    "averaged_df = combined_df.groupby(['dbscan', 'q_bin'])[['recall', 'missed']].mean().reset_index()\n",
    "averaged_df = averaged_df.merge(count_df, on='q_bin')\n",
    "averaged_df = averaged_df[averaged_df['count'] > 10]\n",
    "averaged_df['q_bin_med'] = averaged_df['q_bin'].apply(lambda x: (x.left + x.right) / 2)\n",
    "averaged_df\n",
    "\n",
    "# ADD CONFIDENCE INTERVALS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b668f-dc75-44c8-a44c-0da1cc12c075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dbscan_type, group_data in averaged_df.groupby('dbscan'):\n",
    "    plt.scatter(group_data['q_bin_med'], group_data['recall'], label=dbscan_type)\n",
    "    \n",
    "    # Trend line\n",
    "    sns.regplot(\n",
    "        x='q_bin_med', \n",
    "        y='recall', \n",
    "        data=group_data, \n",
    "        scatter=False, \n",
    "        ci=None\n",
    "    )\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel(\"q\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.title(\"Recall vs. q with Trend Lines\")\n",
    "plt.legend(title=\"DBSCAN Type\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"global.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eda8f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f50db20d",
   "metadata": {},
   "source": [
    "# Possible stop detection errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be571ac",
   "metadata": {},
   "source": [
    "Related to the intensity within a burst (beta_ping) and include merging, splitting, merging pings from the street (are there bursts of several pings per minute in the real data?). Related to building area as well. \n",
    "- Merging by ground-truth dwell time and area. Using \"default\" DBScan and pick a signal sparsity in which there's plenty a chance to observe merging and splitting (perhaps remarking that ppl with short bursts don't even have this problem -> appendix).\n",
    "- Splitting actually is mediated by two parameters, beta_ping and beta_start. Two resolutions for the same type of problem. However, maybe beta_ping explains splitting when there is a large area, but beta_start explains splitting when there's a long dwell.\n",
    "\n",
    "Experiment: neighboring buildings (e.g. there is merging -- effect could be modulated by amount of noise, building area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8721423",
   "metadata": {},
   "source": [
    "### Experiment Setup\n",
    "\n",
    "Two neighboring small stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1dc60b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = [datetime(2024, 6, 1, hour=0, minute=0) + timedelta(minutes=60*t) for t in range(3)]\n",
    "unix_timestamp = [int(t.timestamp()) for t in start_time]\n",
    "duration = [60]*3  # in minutes\n",
    "\n",
    "location = ['h-x13-y11'] * 1 + ['h-x13-y9'] * 1 + ['w-x15-y9'] * 1\n",
    "\n",
    "destination = pd.DataFrame(\n",
    "    {\"unix_timestamp\": unix_timestamp, \"local_timestamp\": start_time,\n",
    "     \"duration\": duration, \"location\": location}\n",
    ")\n",
    "destination = tg.condense_destinations(destination)\n",
    "\n",
    "traj_cols = {'user_id':'identifier',\n",
    "             'latitude':'x',\n",
    "             'longitude':'y',\n",
    "             'datetime':'local_timestamp'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81731c9-dbec-4c56-9ad9-672fa51f3c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beta_start = 20\n",
    "beta_dur = 500\n",
    "beta_ping = np.arange(2, 12, 0.2)\n",
    "\n",
    "dbscan_params = [(120, 0.5, 2)] # [(120, 0.5, 2), (120, 1, 3)]   # first param (time eps) does not matter for this experiment\n",
    "lachesis_params = [(5, 120, 3)] # [(5, 120, 2), (5, 120, 1.5)] #dur_min, dt_max, and delta_roam\n",
    "\n",
    "N = 100\n",
    "total_tasks = N*len(beta_ping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d0ca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Run without parallelization\n",
    "\n",
    "local_metrics = []\n",
    "\n",
    "# loop thru beta_ping \n",
    "for bp in beta_ping:\n",
    "    # run each beta_ping n times so we can average\n",
    "    for seed in range(N):\n",
    "        # Check if first sampled ping is early enough; else redraw\n",
    "        first_timestamp = datetime(2024, 6, 1, hour=1, minute=0)\n",
    "        while first_timestamp > datetime(2024, 6, 1, hour=0, minute=10):\n",
    "            seed += N\n",
    "            # reset agent\n",
    "            Charlie = Agent(\n",
    "                identifier=\"Charlie\",\n",
    "                home='h-x13-y12',\n",
    "                workplace='w-x15-y9',\n",
    "                city=city,\n",
    "                destination_diary=destination,\n",
    "                start_time=datetime(2024, 6, 1, hour=0, minute=0),\n",
    "                dt=0.5)\n",
    "            # simulate trajectory and sparsify\n",
    "            population.generate_trajectory(Charlie, seed=seed)\n",
    "            Charlie.sample_traj_hier_nhpp(beta_start, beta_dur, bp, seed=seed, ha=3/4)\n",
    "            first_timestamp = Charlie.sparse_traj['local_timestamp'].iloc[0]\n",
    "        # Run DBScan\n",
    "        for j, dbscan_param in enumerate(dbscan_params):\n",
    "            dbscan_out = sd._temporal_dbscan_labels(Charlie.sparse_traj, *dbscan_param, traj_cols)\n",
    "            stop_table = sd.temporal_dbscan(Charlie.sparse_traj, *dbscan_param, traj_cols, complete_output=False)\n",
    "            stop_table = identify_stop(dbscan_out, Charlie.sparse_traj, stop_table, city, method='mode')\n",
    "            _, metrics = cluster_metrics(stop_table, Charlie)\n",
    "            merged = metrics['Stops Merged']\n",
    "            prop_merged = metrics['Prop Stops Merged']\n",
    "            split = metrics['Split']\n",
    "            recall = metrics['Recall']\n",
    "            precision = metrics['Precision']\n",
    "            # save metrics\n",
    "            local_metrics.append([float(bp), seed, ['dbscan1', 'dbscan2'][j], merged, prop_merged, split, recall, precision])\n",
    "        # Run Lachesis\n",
    "        for j, lachesis_param in enumerate(lachesis_params):\n",
    "            lachesis_out = sd._lachesis_labels(Charlie.sparse_traj, *lachesis_param, traj_cols)\n",
    "            stop_table = sd.lachesis(Charlie.sparse_traj, *lachesis_param, traj_cols, complete_output=True)\n",
    "            stop_table = identify_stop(lachesis_out, Charlie.sparse_traj, stop_table, city, method='mode')\n",
    "            _, metrics = cluster_metrics(stop_table, Charlie) # This becomes co-location!!\n",
    "            merged = metrics['Stops Merged']\n",
    "            prop_merged = metrics['Prop Stops Merged']\n",
    "            split = metrics['Split']\n",
    "            recall = metrics['Recall']\n",
    "            precision = metrics['Precision']\n",
    "            # save metrics\n",
    "            local_metrics.append([float(bp), seed, ['lachesis1', 'lachesis2'][j], merged, prop_merged, split, recall, precision])\n",
    "\n",
    "# do the same stuff above but for a greater horizontal accuracy \n",
    "# (\"fig 1b replicates the same experiment but with more noise\")\n",
    "\n",
    "# ditto for building size\n",
    "\n",
    "# something that summarizes the extent of the error -\n",
    "\n",
    "local_metrics = pd.DataFrame(list(local_metrics), columns=['beta_ping', 'seed', 'algorithm', 'merged', 'split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6415b5-a3cb-4545-baa1-eed102b37a37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Run with parallelization\n",
    "\n",
    "def process_bp(bp, seed, dbscan_params, beta_start, beta_dur, city, destination, shared_metrics):\n",
    "    first_timestamp = datetime(2024, 6, 1, hour=1, minute=0)\n",
    "    while first_timestamp > datetime(2024, 6, 1, hour=0, minute=10):\n",
    "        seed += N\n",
    "        # Reset agent\n",
    "        Charlie = Agent(\n",
    "            identifier=\"Charlie\",\n",
    "            home='h-x13-y12',\n",
    "            workplace='w-x15-y9',\n",
    "            city=city,\n",
    "            destination_diary=destination,\n",
    "            start_time=datetime(2024, 6, 1, hour=0, minute=0),\n",
    "            dt=0.5\n",
    "        )\n",
    "        # Simulate trajectory and sparsify\n",
    "        population.generate_trajectory(Charlie, seed=seed)\n",
    "        Charlie.sample_traj_hier_nhpp(beta_start, beta_dur, bp, seed=seed, ha=3/4)\n",
    "        first_timestamp = Charlie.sparse_traj['local_timestamp'].iloc[0]\n",
    "\n",
    "    # Run DBScan\n",
    "    for j, dbscan_param in enumerate(dbscan_params):\n",
    "        dbscan_out = sd._temporal_dbscan_labels(Charlie.sparse_traj, *dbscan_param, traj_cols)\n",
    "        stop_table = sd.temporal_dbscan(Charlie.sparse_traj, *dbscan_param, traj_cols, complete_output=False)\n",
    "        stop_table = identify_stop(dbscan_out, Charlie.sparse_traj, stop_table, city, method='mode')\n",
    "        _, metrics = cluster_metrics(stop_table, Charlie)\n",
    "        merged = metrics['Stops Merged']\n",
    "        prop_merged = metrics['Prop Stops Merged']\n",
    "        split = metrics['Split']\n",
    "        recall = metrics['Recall']\n",
    "        precision = metrics['Precision']\n",
    "        # save metrics\n",
    "        shared_metrics.append([float(bp), seed, ['dbscan', 'dbscan2'][j], merged, prop_merged, split, recall, precision])\n",
    "    # Run Lachesis\n",
    "    for j, lachesis_param in enumerate(lachesis_params):\n",
    "        lachesis_out = sd._lachesis_labels(Charlie.sparse_traj, *lachesis_param, traj_cols)\n",
    "        stop_table = sd.lachesis(Charlie.sparse_traj, *lachesis_param, traj_cols, complete_output=False)\n",
    "        stop_table = identify_stop(lachesis_out, Charlie.sparse_traj, stop_table, city, method='mode')\n",
    "        _, metrics = cluster_metrics(stop_table, Charlie)\n",
    "        merged = metrics['Stops Merged']\n",
    "        prop_merged = metrics['Prop Stops Merged']\n",
    "        split = metrics['Split']\n",
    "        recall = metrics['Recall']\n",
    "        precision = metrics['Precision']\n",
    "        # save metrics\n",
    "        shared_metrics.append([float(bp), seed, ['lachesis', 'lachesis2'][j], merged, prop_merged, split, recall, precision])\n",
    "\n",
    "def parallelize_work(beta_ping, N, dbscan_params, beta_start, beta_dur, city, destination):\n",
    "    with multiprocessing.Manager() as manager:\n",
    "        shared_metrics = manager.list()\n",
    "\n",
    "        # Using ProcessPoolExecutor for parallelism\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            tasks = [\n",
    "                executor.submit(\n",
    "                    process_bp,\n",
    "                    bp,\n",
    "                    seed,\n",
    "                    dbscan_params,\n",
    "                    beta_start,\n",
    "                    beta_dur,\n",
    "                    city,\n",
    "                    destination,\n",
    "                    shared_metrics\n",
    "                )\n",
    "                for bp in beta_ping\n",
    "                for seed in range(N)\n",
    "            ]\n",
    "            \n",
    "            with tqdm(total=total_tasks, desc=\"Processing tasks\") as pbar:\n",
    "                for future in concurrent.futures.as_completed(tasks):\n",
    "                    try:\n",
    "                        future.result()\n",
    "                        pbar.update(1)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Task failed with exception: {e}\")\n",
    "                        pbar.update(1)\n",
    "\n",
    "        return list(shared_metrics)\n",
    "\n",
    "local_metrics = parallelize_work(beta_ping, N, dbscan_params, beta_start, beta_dur, city, destination)\n",
    "local_metrics = pd.DataFrame(list(local_metrics), \n",
    "                             columns=['beta_ping', 'seed', 'algorithm', 'merged', 'prop_merged', 'split', 'recall', 'precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9554fe2-17bb-4b33-9641-6da76ba8f45a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# local_metrics['f1_score'] = 2 * (local_metrics['precision'] * local_metrics['recall']) / (local_metrics['precision'] + local_metrics['recall'])\n",
    "# local_metrics.to_pickle(\"local_metrics.pkl\")\n",
    "with open(\"local_metrics.pkl\", \"rb\") as f:\n",
    "    local_metrics = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de1db3a-2eb1-4c32-9fda-7f1bf9bcad5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd600c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_metrics_avg = local_metrics.groupby(['beta_ping', 'algorithm'])['prop_merged'].mean().reset_index()\n",
    "pivoted = local_metrics_avg.pivot(index='beta_ping', columns='algorithm', values='prop_merged')\n",
    "\n",
    "ax = pivoted.plot(\n",
    "    kind='line',\n",
    "    figsize=(5, 4)\n",
    ")\n",
    "\n",
    "ax.set_title('Stops Exhibiting Merging / (True Stops - 1)', fontsize=16)\n",
    "ax.set_xlabel('beta_p : Average Time Between Pings (in Burst)', fontsize=12)\n",
    "ax.set_ylabel('Percentage Stops', fontsize=12)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"merging.png\")\n",
    "plt.savefig(\"merging.svg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a6d3ae-cf9f-43cf-ac93-0a18acbfd202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_metrics_avg = local_metrics.groupby(['beta_ping', 'algorithm'])['split'].mean().reset_index()\n",
    "pivoted = local_metrics_avg.pivot(index='beta_ping', columns='algorithm', values='split')\n",
    "\n",
    "ax = pivoted.plot(\n",
    "    kind='line',\n",
    "    figsize=(5, 4)\n",
    ")\n",
    "\n",
    "ax.set_title('Effect of Ping Latency on Stop Splitting', fontsize=16)\n",
    "ax.set_xlabel('Within-burst Ping Latency', fontsize=12)\n",
    "ax.set_ylabel('Average Number of Stops Split', fontsize=12)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"splitting.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d62ef7-b869-48b4-bd3e-29c8008c25b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_metrics_avg = local_metrics.groupby(['beta_ping', 'algorithm'])['precision'].mean().reset_index()\n",
    "pivoted = local_metrics_avg.pivot(index='beta_ping', columns='algorithm', values='precision')\n",
    "\n",
    "ax = pivoted.plot(\n",
    "    kind='line',\n",
    "    figsize=(5, 4)\n",
    ")\n",
    "\n",
    "ax.set_title('Effect of Ping Latency on Precision', fontsize=16)\n",
    "ax.set_xlabel('Within-burst Ping Latency', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"precision.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b2b058-d1ee-429c-9041-bfc0eb8e0500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_metrics_avg = local_metrics.groupby(['beta_ping', 'algorithm'])['recall'].mean().reset_index()\n",
    "pivoted = local_metrics_avg.pivot(index='beta_ping', columns='algorithm', values='recall')\n",
    "\n",
    "ax = pivoted.plot(\n",
    "    kind='line',\n",
    "    figsize=(5, 4)\n",
    ")\n",
    "\n",
    "ax.set_title('Effect of Ping Latency on Recall', fontsize=16)\n",
    "ax.set_xlabel('Within-burst Ping Latency', fontsize=12)\n",
    "ax.set_ylabel('Recall', fontsize=12)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"recall.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e459b8c7-ccd5-45f4-bfe8-da346778bf0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_metrics_avg = local_metrics.groupby(['beta_ping', 'algorithm'])['f1_score'].mean().reset_index()\n",
    "pivoted = local_metrics_avg.pivot(index='beta_ping', columns='algorithm', values='f1_score')\n",
    "\n",
    "ax = pivoted.plot(\n",
    "    kind='line',\n",
    "    figsize=(5, 4)\n",
    ")\n",
    "\n",
    "ax.set_title('Effect of Ping Latency on F1 Score', fontsize=16)\n",
    "ax.set_xlabel('Within-burst Ping Latency', fontsize=12)\n",
    "ax.set_ylabel('F1 Score', fontsize=12)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"f1.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d3fdbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed=20000\n",
    "beta_ping=2.0\n",
    "sd_param=(5, 120, 1.2)\n",
    "\n",
    "Charlie = Agent(identifier=\"Charlie\",\n",
    "        home='h-x13-y12',\n",
    "        workplace='w-x15-y9',\n",
    "        city=city,\n",
    "        destination_diary=destination,\n",
    "        start_time=datetime(2024, 6, 1, hour=0, minute=0),\n",
    "        dt=0.5)\n",
    "# simulate trajectory and sparsify\n",
    "population.generate_trajectory(Charlie, seed=seed)\n",
    "Charlie.sample_traj_hier_nhpp(beta_start, beta_dur, beta_ping, seed=seed)\n",
    "\n",
    "sd_out = sd._lachesis_labels(Charlie.sparse_traj, *sd_param, traj_cols)\n",
    "sd_out.index = sd_out.index.map(lambda x: int(x.timestamp()))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "num_clusters = sum(sd_out.cluster.unique() > -1)\n",
    "for cid in range(num_clusters):\n",
    "    cpings = sd_out[sd_out.cluster == cid]\n",
    "    cdata = Charlie.sparse_traj.loc[cpings.index]\n",
    "    col = cm.tab20c(cid/(num_clusters+1))\n",
    "    ax.scatter(cdata.x, cdata.y, s=80, color=col, alpha=1, zorder=2)\n",
    "\n",
    "ax.scatter(x=Charlie.sparse_traj.x, \n",
    "           y=Charlie.sparse_traj.y, \n",
    "           s=6, color='black', alpha=1, zorder=2)\n",
    "city.plot_city(ax, doors=True, address=False, zorder=1)\n",
    "\n",
    "ax.plot(Charlie.sparse_traj.x,\n",
    "        Charlie.sparse_traj.y,\n",
    "        linewidth=1, color='blue', alpha=0.2)\n",
    "\n",
    "ax.set_xlim(12, 17)\n",
    "ax.set_ylim(8, 13)\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"clusters.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a0d8c3-6ebd-4f8f-a0cb-2e79b93db5b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## need to check everything below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa64ad-2f7f-40d5-a8bb-c2f5cba35480",
   "metadata": {},
   "source": [
    "## Bar charts from diaries, type vs fraction_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655624ee-04ec-4c7b-a97c-09c671dccf22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_type = []\n",
    "output_size = []\n",
    "output_truehome = []\n",
    "for agent_id, agent in population_n.roster.items():\n",
    "    d = agent.diary.dropna()\n",
    "    d['id'] = agent_id\n",
    "    d['type'] = d['location'].apply(lambda b: city.buildings[b].building_type)\n",
    "    d['size'] = d['location'].apply(lambda b: len(city.buildings[b].blocks) if b in city.buildings else None)\n",
    "    d['true_home'] = d.apply(lambda x: agent.home == x.location, axis=1)\n",
    "    bin_edges = [0, 1, 4, 100]\n",
    "    bin_labels = ['small', 'medium', 'large']\n",
    "    d['size'] = pd.cut(d['size'], bins=bin_edges, labels=bin_labels, include_lowest=True)\n",
    "\n",
    "    d_type_agg = d.groupby(['id', 'type'], observed=False).duration.sum().reset_index()\n",
    "    d_size_agg = d.groupby(['id', 'size'], observed=False).duration.sum().reset_index()\n",
    "    d_truehome_agg = d.groupby(['id', 'true_home'], observed=False).duration.sum().reset_index()\n",
    "    output_type += [d_type_agg]\n",
    "    output_size += [d_size_agg]\n",
    "    output_truehome += [d_truehome_agg]\n",
    "\n",
    "bars_type_gc = pd.concat(output_type)\n",
    "temp = bars_type_gc.groupby('id')['duration'].transform('sum')\n",
    "bars_type_gc['duration'] = bars_type_gc['duration'] / temp\n",
    "\n",
    "bars_size_gc = pd.concat(output_size)\n",
    "temp = bars_size_gc.groupby('id')['duration'].transform('sum')\n",
    "bars_size_gc['duration'] = bars_size_gc['duration'] / temp\n",
    "\n",
    "bars_trhome_gc = pd.concat(output_truehome)\n",
    "temp = bars_trhome_gc.groupby('id')['duration'].transform('sum')\n",
    "bars_trhome_gc['duration'] = bars_trhome_gc['duration'] / temp\n",
    "\n",
    "all_stop_tables['type'] = all_stop_tables['location'].apply(\n",
    "    lambda b: city.buildings[b].building_type if b in city.buildings else None)\n",
    "\n",
    "all_stop_tables['true_home'] = all_stop_tables.apply(\n",
    "    lambda x: population_n.roster[x.id].home == x.location, axis=1)\n",
    "\n",
    "all_stop_tables['size'] = all_stop_tables['location'].apply(\n",
    "    lambda b: len(city.buildings[b].blocks) if b in city.buildings else None)\n",
    "bin_edges = [0, 1, 4, 100]\n",
    "bin_labels = ['small', 'medium', 'large']\n",
    "all_stop_tables['size'] = pd.cut(all_stop_tables['size'], bins=bin_edges, labels=bin_labels, include_lowest=True)\n",
    "\n",
    "bars_type = all_stop_tables.groupby(['id', 'dbscan', 'sparsity', 'type'], observed=False).duration.sum().reset_index()\n",
    "temp = bars_type.groupby(['id', 'dbscan', 'sparsity'])['duration'].transform('sum')\n",
    "bars_type['duration'] = bars_type['duration'] / temp\n",
    "\n",
    "bars_size = all_stop_tables.groupby(['id', 'dbscan', 'sparsity', 'size'], observed=False).duration.sum().reset_index()\n",
    "temp = bars_size.groupby(['id', 'dbscan', 'sparsity'])['duration'].transform('sum')\n",
    "bars_size['duration'] = bars_size['duration'] / temp\n",
    "\n",
    "bars_trhome = all_stop_tables.groupby(['id', 'dbscan', 'sparsity', 'true_home'], observed=False).duration.sum().reset_index()\n",
    "temp = bars_trhome.groupby(['id', 'dbscan', 'sparsity'])['duration'].transform('sum')\n",
    "bars_trhome['duration'] = bars_trhome['duration'] / temp\n",
    "\n",
    "# Ensure all combinations of 'id', 'dbscan', 'sparsity', and 'type' are present\n",
    "unique_ids = bars_type['id'].unique()\n",
    "unique_dbscan = bars_type['dbscan'].unique()\n",
    "unique_sparsity = bars_type['sparsity'].unique()\n",
    "unique_types = bars_type['type'].unique()\n",
    "all_combinations = pd.DataFrame(list(product(unique_ids, unique_dbscan, unique_sparsity, unique_types)),\n",
    "                                columns=['id', 'dbscan', 'sparsity', 'type'])\n",
    "bars_type = all_combinations.merge(bars_type, on=['id', 'dbscan', 'sparsity', 'type'], how='left')\n",
    "bars_type['duration'] = bars_type['duration'].fillna(0)\n",
    "\n",
    "bars_rog = all_stop_tables.groupby(['id', 'dbscan', 'sparsity']).agg(\n",
    "    rog_d=('rog_d', 'mean'),\n",
    "    rog_sd=('rog_sd', 'mean')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8105e09-306a-479c-9d31-c64beefc54e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sparsity_levels = ['low', 'high']\n",
    "dbscan_options = ['coarse', 'fine']\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(sparsity_levels))\n",
    "\n",
    "grouped_gc = bars_trhome_gc.groupby('true_home')['duration']\n",
    "mean_duration_gc = grouped_gc.mean()[True]\n",
    "std_duration_gc = grouped_gc.std()[True]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "\n",
    "colors = [None, 'lightcoral', 'lightgreen']\n",
    "labels = ['Ground Truth', 'Coarse DBSCAN', 'Fine DBSCAN']\n",
    "labelsx = ['Low Sparsity', 'High Sparsity']\n",
    "\n",
    "ax.bar(index, mean_duration_gc, bar_width, yerr=std_duration_gc, capsize=5, label='Ground Truth', color=colors[0])\n",
    "\n",
    "for j, dbscan_param in enumerate(dbscan_options):\n",
    "    df_sparsity = bars_trhome[bars_trhome['dbscan'] == dbscan_param]\n",
    "\n",
    "    mean_duration = df_sparsity.groupby('sparsity')['duration'].mean()\n",
    "    std_duration = df_sparsity.groupby('sparsity')['duration'].std()\n",
    "\n",
    "    mean_duration = mean_duration.sort_index(ascending=False)\n",
    "    std_duration = std_duration.sort_index(ascending=False)\n",
    "\n",
    "    bar_positions = index + (j + 1) * bar_width\n",
    "    ax.bar(bar_positions, mean_duration.values, bar_width, yerr=std_duration.values, \n",
    "           capsize=5, label=labels[j + 1], color=colors[j + 1])\n",
    "    ax.grid(True)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "ax.set_title('True Home')\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(labelsx, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e98b7-59b7-448a-a249-fd1d508f9eb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sparsity_levels = ['low', 'high']\n",
    "dbscan_options = ['coarse', 'fine']\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(sparsity_levels))\n",
    "\n",
    "mean_gc = bars_rog['rog_d'].mean()\n",
    "std_gc = bars_rog['rog_d'].std()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "\n",
    "colors = [None, 'lightcoral', 'lightgreen']\n",
    "labels = ['Ground Truth', 'Coarse DBSCAN', 'Fine DBSCAN']\n",
    "labelsx = ['Low Sparsity', 'High Sparsity']\n",
    "\n",
    "ax.bar(index, mean_gc, bar_width, yerr=std_gc, capsize=5, label='Ground Truth', color=colors[0])\n",
    "\n",
    "for j, dbscan_param in enumerate(dbscan_options):\n",
    "    df_sparsity = bars_rog[bars_rog['dbscan'] == dbscan_param]\n",
    "\n",
    "    mean_duration = df_sparsity.groupby('sparsity')['rog_sd'].mean()\n",
    "    std_duration = df_sparsity.groupby('sparsity')['rog_sd'].std()\n",
    "\n",
    "    mean_duration = mean_duration.sort_index(ascending=False)\n",
    "    std_duration = std_duration.sort_index(ascending=False)\n",
    "\n",
    "    bar_positions = index + (j + 1) * bar_width\n",
    "    ax.bar(bar_positions, mean_duration.values, bar_width, yerr=std_duration.values, \n",
    "           capsize=5, label=labels[j + 1], color=colors[j + 1])\n",
    "    ax.grid(True)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "ax.set_title('Radius of Gyration')\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(labelsx, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0637793-2fc3-4cc3-86b5-e319c4f7da5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sparsity_levels = ['low', 'high']\n",
    "dbscan_options = ['coarse', 'fine']\n",
    "building_types = bars_type_gc['type'].unique()\n",
    "\n",
    "grouped_gc = bars_type_gc.groupby('type')['duration']\n",
    "mean_duration_gc = grouped_gc.mean()\n",
    "std_duration_gc = grouped_gc.std()\n",
    "\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(sparsity_levels))\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(9, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = [None, 'lightcoral', 'lightgreen']\n",
    "labels = ['Ground Truth', 'Coarse DBSCAN', 'Fine DBSCAN']\n",
    "labelsx = ['Low Sparsity', 'High Sparsity']\n",
    "\n",
    "for k, building_type in enumerate(building_types):\n",
    "    ax = axes[k]\n",
    "\n",
    "    mean_gc = mean_duration_gc[building_type]\n",
    "    std_gc = std_duration_gc[building_type]\n",
    "    ax.bar(index, mean_gc, bar_width, yerr=std_gc, capsize=5, label='Ground Truth', color=colors[0])\n",
    "\n",
    "    for j, dbscan_param in enumerate(dbscan_options):\n",
    "        df_sparsity = bars_type[bars_type['dbscan'] == dbscan_param]\n",
    "        df_sparsity = df_sparsity[df_sparsity['type'] == building_type]\n",
    "\n",
    "        mean_duration = df_sparsity.groupby('sparsity')['duration'].mean()\n",
    "        std_duration = df_sparsity.groupby('sparsity')['duration'].std()\n",
    "\n",
    "        mean_duration = mean_duration.sort_index(ascending=False)\n",
    "        std_duration = std_duration.sort_index(ascending=False)\n",
    "\n",
    "        bar_positions = index + (j + 1) * bar_width\n",
    "        ax.bar(bar_positions, mean_duration.values, bar_width, yerr=std_duration.values, \n",
    "               capsize=5, label=labels[j + 1], color=colors[j + 1])\n",
    "\n",
    "    ax.set_title(f'{building_type.capitalize()}', fontsize=16)\n",
    "    ax.set_ylabel('Duration (%)', fontsize=14)\n",
    "    ax.set_xticks(index + bar_width)\n",
    "    ax.set_xticklabels(labelsx, rotation=0, fontsize=14)\n",
    "    ax.set_ylim(0,)\n",
    "    #ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n",
    "    ax.grid(True)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.1), ncol=3, fontsize=14)\n",
    "\n",
    "plt.savefig('bar_charts.svg')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b22105e-17b9-470b-be1e-105bd3522102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sparsity_levels = ['low', 'high']\n",
    "dbscan_options = ['coarse', 'fine']\n",
    "building_sizes = bars_size_gc['size'].unique()\n",
    "\n",
    "grouped_gc = bars_size_gc.groupby('size', observed=False)['duration']\n",
    "mean_duration_gc = grouped_gc.mean()\n",
    "std_duration_gc = grouped_gc.std()\n",
    "\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(sparsity_levels))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = [None, 'lightcoral', 'lightgreen']\n",
    "labels = ['Ground Truth', 'Coarse DBSCAN', 'Fine DBSCAN']\n",
    "labelsx = ['Low Sparsity', 'High Sparsity']\n",
    "\n",
    "for k, building_size in enumerate(building_sizes):\n",
    "    ax = axes[k]\n",
    "\n",
    "    mean_gc = mean_duration_gc[building_size]\n",
    "    std_gc = std_duration_gc[building_size]\n",
    "    ax.bar(index, mean_gc, bar_width, yerr=std_gc, capsize=5, label='Ground Truth', color=colors[0])\n",
    "\n",
    "    for j, dbscan_param in enumerate(dbscan_options):\n",
    "        df_sparsity = bars_size[bars_size['dbscan'] == dbscan_param]\n",
    "        df_sparsity = df_sparsity[df_sparsity['size'] == building_size]\n",
    "\n",
    "        mean_duration = df_sparsity.groupby('sparsity')['duration'].mean()\n",
    "        std_duration = df_sparsity.groupby('sparsity')['duration'].std()\n",
    "\n",
    "        mean_duration = mean_duration.sort_index(ascending=False)\n",
    "        std_duration = std_duration.sort_index(ascending=False)\n",
    "\n",
    "        bar_positions = index + (j + 1) * bar_width\n",
    "        ax.bar(bar_positions, mean_duration.values, bar_width, yerr=std_duration.values, \n",
    "               capsize=5, label=labels[j + 1], color=colors[j + 1])\n",
    "\n",
    "    ax.set_title(f'{building_size.capitalize()} Buildings', fontsize=16)\n",
    "    ax.set_ylabel('Duration (%)', fontsize=14)\n",
    "    ax.set_xticks(index + bar_width)\n",
    "    ax.set_xticklabels(labelsx, rotation=0, fontsize=14)\n",
    "    ax.set_ylim(0,)\n",
    "    #ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n",
    "    ax.grid(True)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.05), ncol=3)\n",
    "\n",
    "plt.savefig('bar_size.svg')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e5a12-9e79-422f-85b1-8b13fb89b610",
   "metadata": {},
   "source": [
    "# Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc4ed8-eed3-4ace-ab00-9a03d35e20e5",
   "metadata": {},
   "source": [
    "### Fix frequency (q). Vary sparsity pattern by changing first two params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a43150b-5a16-48be-97b1-676acbb8a61e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def process_seed(seed, nhpp_params, dbscan_params, lachesis_params, city):\n",
    "    results_dbscan = []\n",
    "    results_lachesis = []\n",
    "    \n",
    "    Bethany = Agent('Bethany',\n",
    "                    'h-x13-y9',\n",
    "                    'w-x3-y13',\n",
    "                    city,\n",
    "                    start_time=datetime(2024, 1, 1, hour=0, minute=0))\n",
    "\n",
    "    population.add_agent(Bethany, verbose=False)\n",
    "    population.generate_trajectory(Bethany, T=datetime(2024, 1, 15, hour=0, minute=0), seed=seed)\n",
    "\n",
    "    for nhpp_param in nhpp_params: \n",
    "        Bethany.sample_traj_hier_nhpp(*nhpp_param, seed=seed)\n",
    "        qstat = q_stat(Bethany)\n",
    "\n",
    "        for k in range(len(dbscan_params)):\n",
    "            dbscan_param = dbscan_params[k]\n",
    "            lachesis_param = lachesis_params[k]\n",
    "\n",
    "            dbscan_out = sd.temporal_dbscan(Bethany.sparse_traj, *dbscan_param)\n",
    "            lachesis_out = sd.lachesis_patches(Bethany.sparse_traj, *lachesis_param)\n",
    "\n",
    "            stop_table_dbscan = sd.generate_stop_table(Bethany.sparse_traj, dbscan_out)\n",
    "            stop_table_lachesis = sd.generate_stop_table(Bethany.sparse_traj, lachesis_out)\n",
    "\n",
    "            result_dbscan = {\n",
    "                'seed': seed,\n",
    "                'alg_param': ['coarse', 'fine'][k],\n",
    "                'beta_dur': nhpp_param[1],\n",
    "                'q': qstat,\n",
    "                'recall': 0,\n",
    "                'precision': 0,\n",
    "                'stop_merging': None,\n",
    "                'trip_merging': None,\n",
    "                'missed': 0,\n",
    "                'split': None,\n",
    "                'rog_diary': 0,\n",
    "                'rog_alg': 0\n",
    "            }\n",
    "\n",
    "            result_lachesis = result_dbscan.copy()\n",
    "\n",
    "            if stop_table_dbscan.empty:\n",
    "                prepared_diary, _ = prepare_diary(Bethany)\n",
    "                n_stops = np.sum(prepared_diary['stop_id'].unique() > 0)\n",
    "                result_dbscan.update({\n",
    "                    'recall': 0,\n",
    "                    'precision': None,\n",
    "                    'stop_merging': None,\n",
    "                    'trip_merging': None,\n",
    "                    'missed': n_stops,\n",
    "                    'split': None,\n",
    "                    'rog_diary': radius_of_gyration(prepared_diary[['x', 'y']]),\n",
    "                    'rog_alg': 0\n",
    "                })\n",
    "            else:\n",
    "                stop_table_dbscan = identify_stop(dbscan_out, Bethany.sparse_traj, stop_table_dbscan)\n",
    "                # stop_table_dbscan['location'] = stop_table_dbscan.apply(\n",
    "                #     lambda row: city.get_block((row.centroid_x, row.centroid_y)).id, axis=1)\n",
    "                _, metrics, rog = cluster_metrics(stop_table_dbscan, Bethany)\n",
    "                result_dbscan.update({\n",
    "                    'recall': metrics['Recall'],\n",
    "                    'precision': metrics['Precision'],\n",
    "                    'stop_merging': metrics['Stops Merged'],#/metrics['Stop Count'],\n",
    "                    'trip_merging': metrics['Trip Merging'],\n",
    "                    'missed': metrics['Missed'],#/metrics['Stop Count'],\n",
    "                    'split': metrics['Split'],#/metrics['Stop Count'],\n",
    "                    'rog_diary': rog['diary'],\n",
    "                    'rog_alg': rog['stop_table']\n",
    "                })\n",
    "\n",
    "            if stop_table_lachesis.empty:\n",
    "                prepared_diary, _ = prepare_diary(Bethany)\n",
    "                n_stops = np.sum(prepared_diary['stop_id'].unique() > 0)\n",
    "                result_lachesis.update({\n",
    "                    'recall': 0,\n",
    "                    'precision': None,\n",
    "                    'stop_merging': None,\n",
    "                    'trip_merging': None,\n",
    "                    'missed': n_stops,\n",
    "                    'split': None,\n",
    "                    'rog_diary': radius_of_gyration(prepared_diary[['x', 'y']]),\n",
    "                    'rog_alg': 0\n",
    "                })\n",
    "            else:\n",
    "                stop_table_lachesis = identify_stop(lachesis_out, Bethany.sparse_traj, stop_table_lachesis)\n",
    "                # stop_table_lachesis['location'] = stop_table_lachesis.apply(\n",
    "                #     lambda row: city.get_block((row.centroid_x, row.centroid_y)).id, axis=1)\n",
    "                _, metrics, rog = cluster_metrics(stop_table_lachesis, Bethany)\n",
    "                result_lachesis.update({\n",
    "                    'recall': metrics['Recall'],\n",
    "                    'precision': metrics['Precision'],\n",
    "                    'stop_merging': metrics['Stops Merged'],#/metrics['Stop Count'],\n",
    "                    'trip_merging': metrics['Trip Merging'],\n",
    "                    'missed': metrics['Missed'],#/metrics['Stop Count'],\n",
    "                    'split': metrics['Split'],#/metrics['Stop Count'],\n",
    "                    'rog_diary': rog['diary'],\n",
    "                    'rog_alg': rog['stop_table']\n",
    "                })\n",
    "\n",
    "                results_dbscan.append(result_dbscan)\n",
    "                results_lachesis.append(result_lachesis)\n",
    "\n",
    "    return pd.DataFrame(results_dbscan), pd.DataFrame(results_lachesis)\n",
    "\n",
    "\n",
    "seeds = range(1, 175)\n",
    "dbscan_params = [(360, 5, 2), (60, 1.5, 3)]\n",
    "lachesis_params = [(10, 480, 6), (10, 60, 1.5)]\n",
    "beta_starts = range(120, 1600, 40)\n",
    "beta_durs = range(30, 400, 10)\n",
    "beta_ping = 10\n",
    "\n",
    "nhpp_params = [(start, dur, beta_ping) for start, dur in zip(beta_starts, beta_durs)]\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    work = partial(process_seed, nhpp_params=nhpp_params,\n",
    "                   dbscan_params=dbscan_params, lachesis_params=lachesis_params,\n",
    "                   city=city)\n",
    "    all_results = list(executor.map(work, seeds))\n",
    "\n",
    "results_dbscan = [result[0] for result in all_results]\n",
    "results_lachesis = [result[1] for result in all_results]\n",
    "\n",
    "results_dbscan = pd.concat(results_dbscan, ignore_index=True)\n",
    "results_lachesis = pd.concat(results_lachesis, ignore_index=True)\n",
    "\n",
    "results_dbscan.to_pickle('results-sparsity-dbscan-nofilter-1.pkl')\n",
    "results_lachesis.to_pickle('results-sparsity-lachesis-nofilter-1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1102e9e4-38ee-4578-9c0b-a16f2da577cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('results-sparsity-dbscan-nofilter.pkl', 'rb') as file:\n",
    "    results_dbscan = pickle.load(file)\n",
    "\n",
    "with open('results-sparsity-lachesis-nofilter.pkl', 'rb') as file:\n",
    "    results_lachesis = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8566733-392e-426d-b456-82bcea9b9d1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create bins for q, size 0.05 from 0 to 1\n",
    "# bins = np.arange(0, 1.05, 0.05)\n",
    "# labels = [i/100+0.025 for i in range(0, 100, 5)]\n",
    "# results_dbscan['q_bin'] = pd.cut(results_dbscan['q'], bins=bins, labels=labels, include_lowest=True)\n",
    "# results_lachesis['q_bin'] = pd.cut(results_lachesis['q'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "results_dbscan_sparsity = results_dbscan.groupby(['beta_dur', 'alg_param'], observed=True).mean().reset_index()\n",
    "counts = results_dbscan.groupby(['beta_dur', 'alg_param'], observed=True).size().reset_index(name='count')\n",
    "results_dbscan_sparsity = results_dbscan_sparsity.merge(counts, on=['beta_dur', 'alg_param'])\n",
    "results_dbscan_sparsity = results_dbscan_sparsity[results_dbscan_sparsity['count'] >= 10]\n",
    "\n",
    "results_lachesis_sparsity = results_lachesis.groupby(['beta_dur', 'alg_param'], observed=True).mean().reset_index()\n",
    "counts = results_lachesis.groupby(['beta_dur', 'alg_param'], observed=True).size().reset_index(name='count')\n",
    "results_lachesis_sparsity = results_lachesis_sparsity.merge(counts, on=['beta_dur', 'alg_param'])\n",
    "results_lachesis_sparsity = results_lachesis_sparsity[results_lachesis_sparsity['count'] >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af8ed55-6773-40a7-acfc-f48492dd1e35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_dbscan_sparsity\n",
    "#results_lachesis_sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28025e93-4cf8-4b02-89af-b7434038524c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 1.5))\n",
    "\n",
    "for i, p in enumerate([2,4,5]):\n",
    "    ax = axes[i]\n",
    "    for j in range(2):\n",
    "        setting = ['coarse', 'fine'][j]\n",
    "        results_df = results_dbscan_sparsity[results_dbscan_sparsity['alg_param']==setting]\n",
    "        ax.plot(results_df['beta_dur'], results_df.iloc[:, p+4],\n",
    "                label=['Coarse', 'Fine'][j],\n",
    "                color=['red', 'darkgreen'][j],\n",
    "                linestyle=['-', ':'][j])\n",
    "        ax.set_title(['Stop Merging', 'Missed', 'Split'][i])\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_xlabel('Expected Burst Durations')\n",
    "\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', fontsize='large',\n",
    "           ncol=3, bbox_to_anchor=(0.51, -0.55))\n",
    "plt.subplots_adjust(hspace=0.8, wspace=0.2)\n",
    "\n",
    "plt.savefig(\"metrics_vs_sparsity-dbscan.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a933407-d294-44e8-883b-ef92b360f0bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(12, 4))\n",
    "\n",
    "for i in range(6):\n",
    "    ax = axes[i % 2, i//2]\n",
    "    for j in range(2):\n",
    "        setting = ['coarse', 'fine'][j]\n",
    "        results_df = results_lachesis_sparsity[results_lachesis_sparsity['alg_param']==setting]\n",
    "        ax.plot(results_df['beta_dur'], results_df.iloc[:, i+4],\n",
    "                label=['Coarse', 'Fine'][j],\n",
    "                color=['black', 'red'][j],\n",
    "                linestyle=['-', ':'][j])\n",
    "        ax.set_title(['Recall', 'Precision', 'Stop Merging', 'Trip Merging', 'Missed', 'Split'][i])\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_xlabel('Expected Burst Durations')\n",
    "\n",
    "handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', fontsize='large',\n",
    "           ncol=3, bbox_to_anchor=(0.51, -0.15))\n",
    "plt.subplots_adjust(hspace=0.8, wspace=0.2)\n",
    "\n",
    "plt.savefig(\"metrics_vs_sparsity-lachesis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a0ce99-9176-4591-91f6-2edd7f28a4ac",
   "metadata": {},
   "source": [
    "### Fix sparsity. Vary DBSCAN parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022f861-0b9d-42a7-8a1e-49a59c5fa3f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seeds = range(100, 150)\n",
    "hier_nhpp_params = [(90, 30, 4), (60, 60, 2)]  # 3pph, 12pph\n",
    "\n",
    "# dbscan_params = (time_thresh, dist_thresh, min_pts)\n",
    "dist_threshs = np.arange(5/15, 200/15, 0.25)\n",
    "\n",
    "results = np.zeros((len(seeds), len(hier_nhpp_params), len(dist_threshs), 8))\n",
    "\n",
    "for i, seed in enumerate(seeds):\n",
    "    Bethany = Agent('Bethany',\n",
    "                    'h-x13-y9',\n",
    "                    'w-x3-y13',\n",
    "                    city,\n",
    "                    destination_diary=destination)\n",
    "    population.add_agent(Bethany, verbose=False)\n",
    "    population.generate_trajectory(Bethany, seed=seed, dt=1)\n",
    "\n",
    "    for j, hier_nhpp_param in enumerate(hier_nhpp_params):\n",
    "        Bethany.sample_traj_hier_nhpp(*hier_nhpp_param, seed=seed)\n",
    "        truepph = round(len(Bethany.sparse_traj) * 2 / 9) / 2\n",
    "        print(truepph)\n",
    "\n",
    "        for k, dist_thresh in enumerate(dist_threshs):\n",
    "            dbscan_param = (240, dist_thresh, 2)\n",
    "            dbscan_out = sd.temporal_dbscan(Bethany.sparse_traj, *dbscan_param)\n",
    "            stop_table = sd.generate_stop_table(Bethany.sparse_traj, dbscan_out)\n",
    "\n",
    "            if stop_table.empty:\n",
    "                prepared_diary, _ = prepare_diary(Bethany)\n",
    "                n_stops = np.sum(prepared_diary['stop_id'].unique() > 0)\n",
    "\n",
    "                results[i, j, k, 0] = dist_thresh\n",
    "                results[i, j, k, 1] = 0  # Recall\n",
    "                results[i, j, k, 2] = 0  # Precision\n",
    "                results[i, j, k, 3] = None  # Weighted Stop Merging\n",
    "                results[i, j, k, 4] = None  # Trip Merging\n",
    "                results[i, j, k, 5] = n_stops  # Stops missed\n",
    "                results[i, j, k, 6] = None  # Stops split\n",
    "                results[i, j, k, 7] = q_stat(Bethany)  # q\n",
    "                continue\n",
    "\n",
    "            stop_table['location'] = stop_table.apply(\n",
    "                lambda row: city.get_block((row.centroid_x, row.centroid_y)).id, axis=1)\n",
    "            metrics_df, metrics = cluster_metrics(stop_table, Bethany)\n",
    "\n",
    "            results[i, j, k, 0] = dist_thresh\n",
    "            results[i, j, k, 1] = metrics['Recall']\n",
    "            results[i, j, k, 2] = metrics['Precision']\n",
    "            #results[i, j, k, 3] = metrics['Weighted Stop Merging']\n",
    "            results[i, j, k, 3] = metrics['Stops Merged']\n",
    "            results[i, j, k, 4] = metrics['Trip Merging']\n",
    "            results[i, j, k, 5] = metrics['Missed']\n",
    "            results[i, j, k, 6] = metrics['Split']\n",
    "            results[i, j, k, 7] = q_stat(Bethany)\n",
    "\n",
    "    print(seed)\n",
    "\n",
    "results_param = np.nanmean(results, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04dff38-ad57-4346-a0aa-2118548d1314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(10, 10))\n",
    "\n",
    "for i in range(6):\n",
    "    ax = axes[i//2, i % 2]\n",
    "    for j in range(2):\n",
    "        results_df = pd.DataFrame(\n",
    "            results_param[j, :, :],\n",
    "            columns=['dist_thresh', 'recall', 'precision', 'weighted_stop_merging', 'trip_merging', 'missed', 'split', 'q_stat'])\n",
    "        ax.plot(results_df['dist_thresh'], results_df.iloc[:, i+1],\n",
    "                label=['3pph', '12pph'][j], color=['black', 'red'][j], linestyle=['-', '--'][j])\n",
    "        ax.set_title(['Recall', 'Precision', 'Stop Merging', 'Trip Merging', 'Missed', 'Split'][i])\n",
    "        ax.set_ylabel('')\n",
    "\n",
    "#fig.suptitle('Metrics v.s. DBSCAN parameters', fontsize=20)\n",
    "handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', fontsize='large',\n",
    "           ncol=2, bbox_to_anchor=(0.5, 0.03))\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.2)\n",
    "\n",
    "plt.savefig(\"metrics_vs_params.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19945c7-4287-4412-abaf-9803724c3a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
