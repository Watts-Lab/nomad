{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5120a5-6e2f-4e3e-9808-bf24dacb14d2",
   "metadata": {},
   "source": [
    "# TODOS\n",
    "\n",
    "- disaggregate by building size, dwell time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041f4fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install -e ../..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c2bbc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-muted')\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "\n",
    "from pyproj import Transformer\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import numpy.random as npr\n",
    "import matplotlib.dates as mdates\n",
    "from itertools import product\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nomad.io.base as loader\n",
    "import nomad.city_gen as cg\n",
    "from nomad.city_gen import City, Building, Street\n",
    "import nomad.traj_gen as tg\n",
    "from nomad.traj_gen import Agent, Population\n",
    "import nomad.stop_detection.ta_dbscan as DBSCAN\n",
    "import nomad.stop_detection.lachesis as Lachesis\n",
    "from nomad.constants import DEFAULT_SPEEDS, FAST_SPEEDS, SLOW_SPEEDS, DEFAULT_STILL_PROBS\n",
    "from nomad.constants import FAST_STILL_PROBS, SLOW_STILL_PROBS, ALLOWED_BUILDINGS\n",
    "\n",
    "from metrics import poi_map, identify_stop, prepare_diary, prepare_stop_table, cluster_metrics\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4152cdfc-d11f-4b75-9099-2507d61fef59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_sparse_clusters(\n",
    "    sparse_traj,\n",
    "    labels,\n",
    "    ax,\n",
    "    full_traj=None,\n",
    "    buffer=None,              # 0‒1 → pad bbox by (1+buffer); None → no limits\n",
    "    cmap=cm.tab20c\n",
    "):\n",
    "    n_clusters = int(labels[labels >= 0].max() + 1) if (labels >= 0).any() else 0\n",
    "    for cid in range(n_clusters):\n",
    "        m = labels == cid\n",
    "        ax.scatter(sparse_traj.x[m], sparse_traj.y[m],\n",
    "                   s=80, color=cmap(cid / (n_clusters + 1)),\n",
    "                   zorder=2)\n",
    "    ax.scatter(sparse_traj.x, sparse_traj.y, s=6, color='black', zorder=2)\n",
    "    if full_traj is not None:\n",
    "        ax.plot(full_traj.x, full_traj.y, lw=1.2, color='blue', alpha=0.2, zorder=1)\n",
    "        if buffer is not None:\n",
    "            x0, x1 = full_traj.x.min(), full_traj.x.max()\n",
    "            y0, y1 = full_traj.y.min(), full_traj.y.max()\n",
    "            pad_x = (x1 - x0) * buffer / 2\n",
    "            pad_y = (y1 - y0) * buffer / 2\n",
    "            ax.set_xlim(x0 - pad_x, x1 + pad_x)\n",
    "            ax.set_ylim(y0 - pad_y, y1 + pad_y)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    plt.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132f841-e679-4024-9b25-6a9df9764b92",
   "metadata": {},
   "source": [
    "# Robustness to sparsity in pre-processing of human mobility data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572296dc-9bdc-40ab-b030-fd58cb374426",
   "metadata": {},
   "source": [
    "**Francisco Jose Barreras**, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA, USA\n",
    "\n",
    "**Thomas Li**, Department of Economics, University of Pennsylvania, Philadelphia, PA, USA\n",
    "\n",
    "**Duncan Watts**, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA, USA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fb9c37-4523-4482-b92a-dcb83d524c6f",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf4d69d-db7c-4c11-896e-aa6ccd1eb83f",
   "metadata": {},
   "source": [
    "Pre-processing algorithms for human mobility data, such as stop detection, are vulnerable to errors when applied to sparse and bursty GPS datasets, but evaluating their robustness is difficult in the absence of ground truth. We address this challenge with a synthetic benchmarking framework that combines an agent-based generator of realistic trajectories—based on the Exploration and Preferential Return model (EPR)—with a sparse sampler that replicates the temporal structure of real GPS data. This setup allows controlled comparison of algorithm outputs against known ground-truth stops. We evaluate three stop-detection methods—ST-DBScan, Lachesis, and a grid-based algorithm—across varying levels of sparsity, burstiness, and parameterizations. Our analysis identifies failure modes such as stop merging, splitting, and missed stops, quantifies how often they arise under realistic movement patterns, and assesses how parameter tuning can mitigate them. We find that commonly used algorithms are not robust to levels of sparsity commonly found in commercial datasets, and that parameter choices critically affect outcomes. These experiments offer practical guidance for improving the reliability of GPS data processing in applied research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4429317c-fb67-4a78-85e0-caffd176d901",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4342cfbe-afab-470d-acae-a1a7740e4f58",
   "metadata": {
    "tags": []
   },
   "source": [
    "Commercial GPS datasets have become central to human mobility research, powering applications in epidemiology, disaster response, transportation, urban planning, and behavioral science \\cite{chang2021mobility, pepe2020covid, couture2021jue, moro2021mobility, gauvin2020gender, song2014prediction}. These datasets provide granular, high-frequency location signals from smartphones, but they are also sparse, bursty, and noisy—particularly when pings are collected opportunistically via apps or under privacy-preserving constraints. To extract useful signals from these raw data, researchers apply pre-processing algorithms to identify stops, trips, and frequently visited locations. However, many of these algorithms, including variants of DBScan \\cite{birant2007st, ester1996density}, rule-based methods like Project Lachesis \\cite{hariharan2004project}, and grid-based heuristics, were not designed with sparse or bursty signals in mind. Their performance in such settings is rarely tested, and robustness to signal degradation remains poorly understood.\n",
    "\n",
    "Evaluating robustness in these settings is nontrivial. Real datasets typically lack ground truth: it is unknown whether a detected stop reflects a real event, or if a missed stop is due to noise, sparsity, or poor parameterization. Even synthetic benchmarks often assume uniformly sampled trajectories, which overlook the temporal irregularities that characterize real data. As a result, existing evaluations often prioritize computational efficiency over behavioral accuracy \\cite{aslak2020infostop, chen2014t}, leaving open questions about how well these algorithms perform under the kinds of signal degradation found in commercial datasets.\n",
    "\n",
    "We address this challenge by introducing a synthetic benchmarking framework designed to test the robustness of stop-detection algorithms under realistic conditions. Our framework builds on the Garden City mobility model \\cite{2412.00913v1}, which uses an Exploration and Preferential Return (EPR) model to simulate human mobility patterns at both macro and micro scales. Agents generate full, minute-level “ground-truth” trajectories as they move through a synthetic city composed of homes, workplaces, and public locations. These trajectories are then sparsified using a self-exciting point process that captures the bursty nature of GPS pings, and measurement noise is added to mimic GPS error.\n",
    "\n",
    "Using this framework, we evaluate the robustness of three stop-detection algorithms—ST-DBScan \\cite{birant2007st}, Lachesis \\cite{hariharan2004project}, and a simple grid-based method—by comparing their outputs to the ground-truth mobility diaries of the simulated agents. Our analysis proceeds in three parts. First, we characterize the failure modes of these algorithms using controlled synthetic trajectories, identifying phenomena such as stop merging (over-clustering), stop splitting (under-clustering), and omission of stops altogether. Second, we test whether these failure modes persist under more realistic mobility traces, and analyze their frequency and correlation with features of the signal such as ping density, burst structure, and location size. Finally, we show that careful parameter tuning can mitigate some—but not all—of these issues, and provide practical guidance on how to select parameters in the presence of sparse and noisy data.\n",
    "\n",
    "Our results indicate that most stop-detection algorithms are not robust to realistic forms of sparsity, and that failure rates depend strongly on the temporal structure of the data—not just its average sampling rate. These findings underscore the need for more principled benchmarking and parameter selection in GPS data processing pipelines, especially for applications where downstream metrics are sensitive to stop detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f17698-c695-445b-aaa5-cbc93e407516",
   "metadata": {},
   "source": [
    "## Possible errors in stop detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b75f9fa",
   "metadata": {},
   "source": [
    "We focus on errors of two types. Clustering pings that should not be clustered together, in particular, merging of two stops that occur in nearby buildings. This should occur more often when the algorithm parameters are coarser.\n",
    "The other problem is failure to cluster pings that correspond to the same stop, this is a problem when the movement is to broad and the parameters are too fine, but also when there are long gaps in the data. Thus, we anticipate the error to be related to area and dwell times. We call these \"missed stops\" and \"splitting\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185708d1",
   "metadata": {},
   "source": [
    "**Ad-hoc trajectory 1**: 4 one-hour-long visits to nearby buildings.\n",
    "\n",
    "$\\varepsilon$: Stop-detection distance thresholds ($\\varepsilon$) larger than the distance between two buildings guarantee merging. We experiment with $\\varepsilon = 12m$ for DBScan, and $\\varepsilon = 30m$ for Lachesis, to cover a non-trivial regime.\n",
    "\n",
    "![Ad-hoc Trajectory 1](./ad-hoc-traj-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba519dc5",
   "metadata": {},
   "source": [
    "**Ad-hoc trajectory 2**:  5 visits to separate buildings, with varying areas and dwell times.\n",
    "\n",
    "$\\Delta T$: Stop-detection max allowed gap of 90 minutes is common and allows for splitting when dwelling for a long time. We experiment with $\\varepsilon = 18m$ for DBSCan, and $\\varepsilon = 30m$ for Lachesis, to allow some spatial splitting.\n",
    "\n",
    "![Ad-hoc Trajectory 2](./ad-hoc-traj-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8721423",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Experiment 1\n",
    "\n",
    "Demonstrate merging: four neighboring small stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1dc60b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tz = ZoneInfo(\"America/New_York\")\n",
    "start_time = pd.date_range(start='2024-06-01 00:00', periods=4, freq='60min', tz=tz)\n",
    "tz_offset = loader._offset_seconds_from_ts(start_time[0])\n",
    "unix_timestamp = [int(t.timestamp()) for t in start_time]\n",
    "duration = [60]*4  # in minutes\n",
    "location = ['h-x13-y11'] * 1 + ['h-x13-y9'] * 1 + ['w-x18-y10'] * 1 + ['w-x18-y8'] * 1\n",
    "\n",
    "destination = pd.DataFrame(\n",
    "    {\"local_timestamp\":start_time,\n",
    "     \"unix_timestamp\":unix_timestamp,\n",
    "     \"duration\":duration,\n",
    "     \"location\":location}\n",
    "     )\n",
    "destination = tg.condense_destinations(destination)\n",
    "\n",
    "traj_cols = {'user_id':'identifier',\n",
    "             'latitude':'x',\n",
    "             'longitude':'y',\n",
    "             'datetime':'local_timestamp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c905cc7c",
   "metadata": {},
   "source": [
    "Example plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5ab12e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Charlie = Agent(identifier=\"Charlie\",\n",
    "                home='h-x13-y11',\n",
    "                workplace='w-x18-y8',\n",
    "                city=city)\n",
    "\n",
    "Charlie.generate_trajectory(destination_diary=destination, seed=109, dt=0.5)\n",
    "Charlie.sample_trajectory(*(float('inf'), float('inf'), 10), seed=1, replace_sparse_traj=True)\n",
    "\n",
    "dbscan_out = DBSCAN._temporal_dbscan_labels(Charlie.sparse_traj, *(600, 0.8, 3), traj_cols)\n",
    "dbscan_out.index = dbscan_out.index.map(lambda x: int(x.timestamp()))\n",
    "\n",
    "lachesis_out = Lachesis._lachesis_labels(Charlie.sparse_traj, *(5, 600, 2), traj_cols)\n",
    "lachesis_out.index = lachesis_out.index.map(lambda x: int(x.timestamp()))  \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "city.plot_city(ax, doors=True, address=False, zorder=0)\n",
    "\n",
    "plot_sparse_clusters(Charlie.sparse_traj, dbscan_out.cluster, ax,\n",
    "                     full_traj=Charlie.trajectory, buffer=0.5)\n",
    "\n",
    "plt.savefig(\"exp1-merging-dbscan.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81731c9-dbec-4c56-9ad9-672fa51f3c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beta_start = float('inf')\n",
    "beta_dur = float('inf')\n",
    "beta_ping = np.arange(2, 12.5, 0.5)\n",
    "\n",
    "dbscan_params = [(600, 0.8, 3)]\n",
    "lachesis_params = [(5, 600, 2)]  # dur_min, dt_max, and delta_roam\n",
    "\n",
    "N = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6415b5-a3cb-4545-baa1-eed102b37a37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Run with parallelization\n",
    "\n",
    "def process_seed(seed, beta_ping, dbscan_params, beta_start, beta_dur, city, destination):\n",
    "    metrics_out = []\n",
    "\n",
    "    agent = Agent(\n",
    "        identifier=\"Charlie\",\n",
    "        home='h-x13-y11',\n",
    "        workplace='w-x18-y8',\n",
    "        city=city)\n",
    "    \n",
    "    agent.reset_trajectory()\n",
    "    agent.generate_trajectory(destination_diary=destination, dt=1, seed=seed)\n",
    "\n",
    "    for bp in beta_ping:\n",
    "        agent.sample_trajectory(beta_start, beta_dur, bp, seed=seed, ha=3/4, replace_sparse_traj=True)\n",
    "\n",
    "        # Run DBScan\n",
    "        for j, dbscan_param in enumerate(dbscan_params):\n",
    "            dbscan_out = DBSCAN._temporal_dbscan_labels(agent.sparse_traj, *dbscan_param, traj_cols)\n",
    "            dbscan_out.index = dbscan_out.index.map(lambda x: int(x.timestamp()))\n",
    "            stop_table = DBSCAN.temporal_dbscan(agent.sparse_traj, *dbscan_param, traj_cols, complete_output=False)\n",
    "            stop_table = identify_stop(dbscan_out.cluster, agent.sparse_traj, stop_table, city_geojson, method='mode')\n",
    "            _, metrics = cluster_metrics(stop_table, agent, city)\n",
    "\n",
    "            metrics_out.append([\n",
    "                float(bp), seed, f'dbscan (eps=12)', \n",
    "                metrics['Stops Merged'], metrics['Prop Stops Merged'],\n",
    "                metrics['Split'], metrics['Recall'], metrics['Precision']\n",
    "            ])\n",
    "        \n",
    "        # Run Lachesis\n",
    "        for j, lachesis_param in enumerate(lachesis_params):\n",
    "            lachesis_out = Lachesis._lachesis_labels(agent.sparse_traj, *lachesis_param, traj_cols)\n",
    "            lachesis_out.index = lachesis_out.index.map(lambda x: int(x.timestamp()))\n",
    "            stop_table = Lachesis.lachesis(agent.sparse_traj, *lachesis_param, traj_cols, complete_output=False)\n",
    "            stop_table.rename(columns={'local_timestamp':'start_time'}, inplace=True)\n",
    "            stop_table = identify_stop(lachesis_out, agent.sparse_traj, stop_table, city_geojson, method='mode')\n",
    "            _, metrics = cluster_metrics(stop_table, agent, city)\n",
    "\n",
    "            metrics_out.append([\n",
    "                float(bp), seed, f'lachesis (eps=30)', \n",
    "                metrics['Stops Merged'], metrics['Prop Stops Merged'],\n",
    "                metrics['Split'], metrics['Recall'], metrics['Precision']\n",
    "            ])\n",
    "\n",
    "    return metrics_out\n",
    "    \n",
    "def parallelize_work(beta_ping, N, dbscan_params, beta_start, beta_dur, city, destination):\n",
    "    results = []\n",
    "\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        tasks = [\n",
    "            executor.submit(\n",
    "                process_seed,\n",
    "                seed,\n",
    "                beta_ping,\n",
    "                dbscan_params,\n",
    "                beta_start,\n",
    "                beta_dur,\n",
    "                city,\n",
    "                destination\n",
    "            )\n",
    "            for seed in range(N)\n",
    "        ]\n",
    "        total_tasks = len(tasks)\n",
    "        with tqdm(total=total_tasks, desc=\"Processing tasks\") as pbar:\n",
    "            for future in concurrent.futures.as_completed(tasks):\n",
    "                try:\n",
    "                    task_metrics = future.result()\n",
    "                    results.extend(task_metrics)\n",
    "                except Exception as e:\n",
    "                    print(f\"Task failed: {e}\")\n",
    "                finally:\n",
    "                    pbar.update(1)\n",
    "\n",
    "    return results\n",
    "\n",
    "local_metrics = parallelize_work(beta_ping, N, dbscan_params, beta_start, beta_dur, city, destination)\n",
    "local_metrics = pd.DataFrame(list(local_metrics), \n",
    "                             columns=['beta_ping', 'seed', 'algorithm', 'merged', 'prop_merged', 'split', 'recall', 'precision'])\n",
    "local_metrics['f1_score'] = 2 * (local_metrics['precision'] * local_metrics['recall']) / (local_metrics['precision'] + local_metrics['recall'])\n",
    "local_metrics.to_pickle(\"local_metrics_exp1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2382d8e-c50c-4bcc-bc4d-7c76d7b7c7df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"local_metrics_exp1.pkl\", \"rb\") as f:\n",
    "    local_metrics = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd600c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_metrics_avg = local_metrics.groupby(['beta_ping', 'algorithm'])['prop_merged'].mean().reset_index()\n",
    "pivoted = local_metrics_avg.pivot(index='beta_ping', columns='algorithm', values='prop_merged')\n",
    "\n",
    "ax = pivoted.plot(\n",
    "    kind='line',\n",
    "    figsize=(8, 6)\n",
    ")\n",
    "\n",
    "ax.set_title('Stops Exhibiting Merging / (True Stops - 1)', fontsize=16)\n",
    "ax.set_xlabel('beta_p : Average Time Between Pings (in Burst)', fontsize=12)\n",
    "ax.set_ylabel('Proportion', fontsize=12)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"merging.png\")\n",
    "plt.savefig(\"merging.svg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42576772-5d38-42b3-b2b7-a278f9181ca2",
   "metadata": {},
   "source": [
    "## Experiment 2\n",
    "\n",
    "Demonstrate splitting: 5 stops of varying areas and dwell times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16efe8-ed57-4244-bddd-4ce69dc9de2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tz = ZoneInfo(\"America/New_York\")\n",
    "start_time = pd.date_range(start='2024-06-01 03:00', periods=18, freq='60min', tz=tz)\n",
    "unix_timestamp = [int(t.timestamp()) for t in start_time]\n",
    "duration = [60]*18  # in minutes\n",
    "location = ['h-x13-y6'] * 7 + ['r-x15-y0'] * 1 + ['w-x18-y13'] * 6 + ['r-x18-y5'] * 1 + ['p-x13-y11'] * 3\n",
    "\n",
    "destination = pd.DataFrame(\n",
    "    {\"local_timestamp\":start_time,\n",
    "     \"unix_timestamp\":unix_timestamp,\n",
    "     \"duration\":duration,\n",
    "     \"location\":location}\n",
    "     )\n",
    "destination = tg.condense_destinations(destination)\n",
    "\n",
    "traj_cols = {'user_id':'identifier',\n",
    "             'latitude':'x',\n",
    "             'longitude':'y',\n",
    "             'datetime':'local_timestamp'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c29e91d-4aef-4d03-98cf-2406317adb04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "destination.location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e630cbb-10dc-437c-b8f1-372fc4085391",
   "metadata": {},
   "source": [
    "Example plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62ba2b8-b20f-489a-a551-4dd3819e2557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Charlie = Agent(identifier=\"Charlie\",\n",
    "                home='h-x13-y11',\n",
    "                workplace='w-x18-y8',\n",
    "                city=city)\n",
    "\n",
    "Charlie.generate_trajectory(destination_diary=destination, dt=1, seed=300)\n",
    "Charlie.sample_trajectory(*(20, 45, 10), seed=300, replace_sparse_traj=True)\n",
    "\n",
    "dbscan_param = (90, 2, 2)\n",
    "dbscan_out = DBSCAN._temporal_dbscan_labels(Charlie.sparse_traj, *dbscan_param, traj_cols)\n",
    "dbscan_out.index = dbscan_out.index.map(lambda x: int(x.timestamp()))\n",
    "stop_table = DBSCAN.temporal_dbscan(Charlie.sparse_traj, *dbscan_param, traj_cols, complete_output=False)\n",
    "stop_table = identify_stop(dbscan_out.cluster, Charlie.sparse_traj, stop_table, city_geojson, method='centroid')\n",
    "metrics_df, metrics = cluster_metrics(stop_table, Charlie, city)\n",
    "\n",
    "# lachesis_param = (20, 75, 2)\n",
    "# lachesis_out = Lachesis._lachesis_labels(Charlie.sparse_traj, *lachesis_param, traj_cols)\n",
    "# lachesis_out.index = lachesis_out.index.map(lambda x: int(x.timestamp()))  \n",
    "# stop_table = Lachesis.lachesis(Charlie.sparse_traj, *lachesis_param, traj_cols, complete_output=False)\n",
    "# stop_table.rename(columns={'local_timestamp':'start_time'}, inplace=True)\n",
    "# stop_table = identify_stop(lachesis_out, Charlie.sparse_traj, stop_table, city_geojson, method='mode')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "city.plot_city(ax, doors=True, address=False, zorder=0)\n",
    "\n",
    "plot_sparse_clusters(Charlie.sparse_traj, dbscan_out.cluster, ax,\n",
    "                     full_traj=Charlie.trajectory, buffer=0.25)\n",
    "\n",
    "# plt.savefig(\"exp2-splitting-dbscan.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45751bb7-8779-4d2c-a22c-935fff117757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a8e42d-f484-421c-b5bd-59e7988b4c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diary = Charlie.diary.dropna().copy()\n",
    "sparse_traj = Charlie.sparse_traj.copy()\n",
    "\n",
    "location_mapping = {loc: idx for idx, loc in enumerate(diary['location'].unique())}\n",
    "diary['location_num'] = diary['location'].map(location_mapping)\n",
    "stop_table['location_num'] = stop_table['location'].map(location_mapping)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(14, 2))\n",
    "\n",
    "# Plot stop_table\n",
    "for idx, row in stop_table.iterrows():\n",
    "    start_time = row['start_time']\n",
    "    end_time = start_time + pd.Timedelta(minutes=row['duration'])\n",
    "    plt.plot([start_time, end_time], [row['location_num'], row['location_num']], color='blue', label='Lachesis' if idx == 0 else \"\", zorder=3)\n",
    "\n",
    "# Plot diary data with shaded rectangles\n",
    "for idx, row in diary.iterrows():\n",
    "    start_time = row['local_timestamp']\n",
    "    end_time = start_time + pd.Timedelta(minutes=row['duration'])\n",
    "    plt.fill_between([start_time, end_time], [row['location_num'] - 0.2, row['location_num'] - 0.2], [row['location_num'] + 0.2, row['location_num'] + 0.2], color='lightgrey', alpha=0.5, label='Ground Truth' if idx == 0 else \"\", zorder=1)\n",
    "    pings = sparse_traj[(sparse_traj['local_timestamp'] >= start_time) & (sparse_traj['local_timestamp'] <= end_time)]\n",
    "    pings = pings['local_timestamp'].to_frame()\n",
    "    pings['location_num'] = row['location_num']\n",
    "    plt.plot(pings['local_timestamp'], pings['location_num'], '|', markersize=8, color='black', alpha=0.5, zorder=2)\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%H:%M'))\n",
    "plt.gca().xaxis.set_major_locator(plt.matplotlib.dates.HourLocator(interval=1))\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Building', fontsize=14)\n",
    "plt.yticks(ticks=list(location_mapping.values()), labels=list(location_mapping.keys()))\n",
    "plt.legend()\n",
    "\n",
    "# plt.savefig('stops_temporal_lachesis.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0429a0e-fc1f-4fd9-86c2-620a090bbd57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beta_start = np.arange(10, 200, 10)\n",
    "beta_dur = 45\n",
    "beta_ping = 10\n",
    "\n",
    "dbscan_params = [(90, 1.2, 2)]  # time_thresh, dist_thresh, min_pts\n",
    "lachesis_params = [(5, 90, 2)]  # dur_min, dt_max, and delta_roam\n",
    "\n",
    "N = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318e0568-1556-4417-a6ab-c864bb1ab8d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Run with parallelization\n",
    "metrics_by_stops_dbscan = {}\n",
    "metrics_by_stops_lachesis = {}\n",
    "\n",
    "def process_seed(seed, beta_ping, dbscan_params, beta_start, beta_dur, city, destination):\n",
    "    metrics_out = []\n",
    "\n",
    "    agent = Agent(\n",
    "        identifier=\"Charlie\",\n",
    "        home='h-x13-y11',\n",
    "        workplace='w-x18-y8',\n",
    "        city=city)\n",
    "    \n",
    "    agent.reset_trajectory()\n",
    "    agent.generate_trajectory(destination_diary=destination, dt=1, seed=seed)\n",
    "\n",
    "    for bs in beta_start:\n",
    "        agent.sample_trajectory(bs, beta_dur, beta_ping, seed=seed, ha=3/4, replace_sparse_traj=True)\n",
    "\n",
    "        # Run DBScan\n",
    "        for j, dbscan_param in enumerate(dbscan_params):\n",
    "            dbscan_out = DBSCAN._temporal_dbscan_labels(agent.sparse_traj, *dbscan_param, traj_cols)\n",
    "            dbscan_out.index = dbscan_out.index.map(lambda x: int(x.timestamp()))\n",
    "            stop_table = DBSCAN.temporal_dbscan(agent.sparse_traj, *dbscan_param, traj_cols, complete_output=False)\n",
    "            stop_table = identify_stop(dbscan_out.cluster, agent.sparse_traj, stop_table, city_geojson, method='mode')\n",
    "            metrics_df, metrics = cluster_metrics(stop_table, agent, city)\n",
    "\n",
    "            metrics_out.append([\n",
    "                float(bs), seed, f'dbscan (eps=12)', \n",
    "                metrics['Stops Merged'], metrics['Prop Stops Merged'],\n",
    "                metrics['Split'], metrics['Prop Split'], metrics['Missed']/5, \n",
    "                metrics['Recall'], metrics['Precision']\n",
    "            ])\n",
    "\n",
    "        # Run Lachesis\n",
    "        for j, lachesis_param in enumerate(lachesis_params):\n",
    "            lachesis_out = Lachesis._lachesis_labels(agent.sparse_traj, *lachesis_param, traj_cols)\n",
    "            lachesis_out.index = lachesis_out.index.map(lambda x: int(x.timestamp()))\n",
    "            stop_table = Lachesis.lachesis(agent.sparse_traj, *lachesis_param, traj_cols, complete_output=False)\n",
    "            stop_table.rename(columns={'local_timestamp':'start_time'}, inplace=True)\n",
    "            stop_table = identify_stop(lachesis_out, agent.sparse_traj, stop_table, city_geojson, method='mode')\n",
    "            metrics_df, metrics = cluster_metrics(stop_table, agent, city)\n",
    "\n",
    "            metrics_out.append([\n",
    "                float(bs), seed, f'lachesis (eps=30)', \n",
    "                metrics['Stops Merged'], metrics['Prop Stops Merged'],\n",
    "                metrics['Split'], metrics['Prop Split'], metrics['Missed']/5, \n",
    "                metrics['Recall'], metrics['Precision']\n",
    "            ])\n",
    "\n",
    "    return metrics_out\n",
    "    \n",
    "def parallelize_work(beta_ping, N, dbscan_params, beta_start, beta_dur, city, destination):\n",
    "    results = []\n",
    "\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        tasks = [\n",
    "            executor.submit(\n",
    "                process_seed,\n",
    "                seed,\n",
    "                beta_ping,\n",
    "                dbscan_params,\n",
    "                beta_start,\n",
    "                beta_dur,\n",
    "                city,\n",
    "                destination\n",
    "            )\n",
    "            for seed in range(N)\n",
    "        ]\n",
    "        total_tasks = len(tasks)\n",
    "        with tqdm(total=total_tasks, desc=\"Processing tasks\") as pbar:\n",
    "            for future in concurrent.futures.as_completed(tasks):\n",
    "                try:\n",
    "                    task_metrics = future.result()\n",
    "                    results.extend(task_metrics)\n",
    "                except Exception as e:\n",
    "                    print(f\"Task failed: {e}\")\n",
    "                finally:\n",
    "                    pbar.update(1)\n",
    "\n",
    "    return results\n",
    "\n",
    "local_metrics = parallelize_work(beta_ping, N, dbscan_params, beta_start, beta_dur, city, destination)\n",
    "local_metrics = pd.DataFrame(list(local_metrics), \n",
    "                             columns=['beta_start', 'seed', 'algorithm', 'merged', 'prop_merged', \n",
    "                                      'split', 'prop_split', 'missed', 'recall', 'precision'])\n",
    "local_metrics['f1_score'] = 2 * (local_metrics['precision'] * local_metrics['recall']) / (local_metrics['precision'] + local_metrics['recall'])\n",
    "local_metrics.to_pickle(\"local_metrics_exp2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135f5465-f17f-47d4-94f2-12b4d0498f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_metrics_avg = local_metrics.groupby(['beta_start', 'algorithm'])[['prop_split', 'missed']].mean().reset_index()\n",
    "local_metrics_avg['both'] = local_metrics_avg['prop_split'] + local_metrics_avg['missed']\n",
    "local_metrics_dbscan = local_metrics_avg[local_metrics_avg.algorithm == 'dbscan (eps=12)']\n",
    "local_metrics_lachesis = local_metrics_avg[local_metrics_avg.algorithm == 'lachesis (eps=30)']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4), sharey=True)\n",
    "\n",
    "# Plot for DBSCAN\n",
    "axes[0].plot(local_metrics_dbscan['beta_start'], local_metrics_dbscan['prop_split'], label='Splitting')\n",
    "axes[0].plot(local_metrics_dbscan['beta_start'], local_metrics_dbscan['missed'], label='Missing')\n",
    "axes[0].plot(local_metrics_dbscan['beta_start'], local_metrics_dbscan['both'], \n",
    "             label='Splitting or Missing', linestyle='--', color='gray')\n",
    "axes[0].set_title('DBSCAN: Stops Split and Missed')\n",
    "axes[0].set_xlabel('beta_start : mean minutes between bursts', fontsize=12)\n",
    "axes[0].set_ylabel('Proportion of Stops', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, color='gray', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "\n",
    "# Plot for Lachesis\n",
    "axes[1].plot(local_metrics_lachesis['beta_start'], local_metrics_lachesis['prop_split'], label='Splitting')\n",
    "axes[1].plot(local_metrics_lachesis['beta_start'], local_metrics_lachesis['missed'], label='Missing')\n",
    "axes[1].plot(local_metrics_lachesis['beta_start'], local_metrics_lachesis['both'], \n",
    "             label='Splitting or Missing', linestyle='--', color='gray')\n",
    "axes[1].set_title('Lachesis: Stops Split and Missed')\n",
    "axes[1].set_xlabel('beta_start : mean minutes between bursts', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, color='gray', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"split-missed-comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1415544-2a2d-436a-8448-6f7f50a98013",
   "metadata": {
    "tags": []
   },
   "source": [
    "![title](./example-of-problems.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c01d62-ee41-4ec1-af6b-bcc78204e9f3",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d550feba-2bc3-40df-8fe1-8b8ca8e9f878",
   "metadata": {},
   "source": [
    "- We explain garden city and how we produced ground truth, heavily referencing the Arxiv version\n",
    "- We divide in three sections after that\n",
    "- Possible problems in stop detection\n",
    "- Are these problems present in real (global) trajectories? + covariates\n",
    "- Can the parameterizations address these issues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea05e21c-520a-41eb-8675-09268e6d9fc1",
   "metadata": {},
   "source": [
    "There are regimes in which certain problems disappear.\n",
    "- E.g. going from extremely complete signals to very complete signals, might not make a difference in terms of quality---they are both good enough. This analysis could be visualized in a plot with q in the x-axis, showing that for high q and very low q things break down. \n",
    "- E.g. a very bad parameterization of DBScan for retail, might obscure a small effect of beta ping on retail. \n",
    "- Some problems might mostly affect users that explore a lot (this is application-relevant). \n",
    "- Maybe the conclusion is that you need a hierarchical DBScan\n",
    "\n",
    "We show problems. Can we THEN show solutions? Then maybe we can hint at solutions while describing problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459fbcc6-cb76-4e25-a607-8b6584faa570",
   "metadata": {},
   "source": [
    "## Global problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a619daca-fa35-418c-9cfb-78bb33d76734",
   "metadata": {},
   "source": [
    "Related to the intensity and clustering of the bursts. It can affect if stops are missed (recall), it can also mess with accuracy by building type (typically long/short dwells).\n",
    "\n",
    "- Missingness is more severe for home and work compared to retail and park. Maybe a ratio statistic alongside the magnitude decreases. Pick a \"reasonable\" DBScan (based on recall). Parameters of a typical user in a typical dataset (avoid a regime where nothing/everything works). \n",
    "- Obvious plot is you have less signal (gray rectangles) then you miss more stops/time-at-right-stop overall. Show regimes? Specially for exploration-prone users? + nuance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fdbab3-915b-4463-b1c2-528a850ef497",
   "metadata": {},
   "source": [
    "## Parameterization problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8204a114-9b40-46b6-a761-62834642b437",
   "metadata": {},
   "source": [
    "You can truly choose the wrong parameters, and you might want to incorporate user signal parameters and building areas, and whatnot. Quality of algorithms and how to choose. x-axis is parameter of DBScan and y-axis is quality of clusters.\n",
    "- What would you do with a complete and regular signal? Maybe a \"reasonable\" DBScan would fail miserably and Lachesis would succeed? min_pts would save the day? \n",
    "- Time parameter ranges from 1 hour to 16 hours. Long-dwell \"bridging\" of huge gaps would increase (obviously) but, the nuance is overestimating time at work or at home. Absolute vs Relative. \n",
    "- Epsilon affects splitting and merging. Pick a \"default and reasonable\" beta_ping and change epsilon. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fedb635",
   "metadata": {},
   "source": [
    "Initialization of city and population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698353df-7777-445e-bff1-204a39af27d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "city_geojson = gpd.read_file('../garden_city.geojson')\n",
    "city = cg.load('../garden-city.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fcaf01-13ec-4980-b474-823871d026f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Simulate 1000 Agents*\n",
    "\n",
    "Initialize 1000 agents and generate one week of ground-truth trajectory data for each.\n",
    "\n",
    "USE SYNTHETIC DATA!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846e2874",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "population_1000 = Population(city)\n",
    "population_1000.generate_agents(N = N, \n",
    "                                start_time = datetime(2024, 1, 1, hour=7, minute=0),\n",
    "                                seed = 100)\n",
    "\n",
    "for i, agent_id in enumerate(population_1000.roster):\n",
    "    agent = population_1000.roster[agent_id]\n",
    "    population_1000.generate_trajectory(agent, \n",
    "                                        T = datetime(2024, 1, 8, hour=7, minute=0),\n",
    "                                        seed=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a7903-4257-4932-9bee-42dd1ebaf2c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "N = 10\n",
    "\n",
    "population_1000 = Population(city)\n",
    "population_1000.generate_agents(N = N, \n",
    "                                start_time = datetime(2024, 1, 1, hour=7, minute=0),\n",
    "                                seed = 100)\n",
    "\n",
    "def generate_agent_trajectory(agent_id, shared_roster, seed):\n",
    "    agent = shared_roster[agent_id]\n",
    "    population_1000.generate_trajectory(agent, \n",
    "                                   T = datetime(2024, 1, 8, hour=7, minute=0),\n",
    "                                   seed=seed)\n",
    "    shared_roster[agent_id] = agent\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "shared_roster = manager.dict(population_1000.roster)\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(generate_agent_trajectory, agent_id, shared_roster, i)\n",
    "        for i, agent_id in enumerate(shared_roster)\n",
    "    ]\n",
    "\n",
    "    with tqdm(total=len(futures), desc=\"Generating agent trajectories\") as pbar:\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "                pbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Task failed with exception: {e}\")\n",
    "                pbar.update(1)\n",
    "\n",
    "population_1000.roster = dict(shared_roster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e17381b-d21a-41e0-b5b2-af0beed7663b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Global Problems\n",
    "\n",
    "Related to the intensity and clustering of the bursts. It can affect if stops are missed (recall), it can also mess with accuracy by building type (typically long/short dwells).\n",
    "\n",
    "- Missingness is more severe for home and work compared to retail and park. Maybe a ratio statistic alongside the magnitude decreases. Pick a \"reasonable\" DBScan (based on recall). Parameters of a typical user in a typical dataset (avoid a regime where nothing/everything works). \n",
    "- Obvious plot is you have less signal (gray rectangles) then you miss more stops/time-at-right-stop overall. Show regimes? Specially for exploration-prone users? + nuance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5900509-f6fd-4820-8a56-36698cde1d21",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EPR, recall and precision*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cadd3e-b521-49b6-9a75-e83e21bdf378",
   "metadata": {},
   "source": [
    "fix the agent and their ground-truth trajectory. vary sampling parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b13de7-7423-4334-b21d-4d5b56383177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beta_starts = range(120, 1600, 40)\n",
    "beta_durs = range(30, 400, 10)\n",
    "beta_ping = 10\n",
    "sparsity_params = [(start, dur, beta_ping) for start, dur in zip(beta_starts, beta_durs)]\n",
    "\n",
    "dbscan_params = [(120, 0.5, 4), (180, 2, 2)] \n",
    "lachesis_params = [(5, 120, 3)] # [(5, 120, 2), (5, 120, 1.5)] #dur_min, dt_max, and delta_roam\n",
    "\n",
    "N = 1  # how many times to run each agent-sparsity pair\n",
    "\n",
    "traj_cols = {'user_id':'identifier',\n",
    "             'latitude':'x',\n",
    "             'longitude':'y',\n",
    "             'datetime':'local_timestamp'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ee7091-f41b-4735-bdfe-c60e67c5bb08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# run without parallelization\n",
    "\n",
    "agent = population_1000.roster['quirky_chebyshev']\n",
    "\n",
    "def process_sample(sparsity_param, seed, agent, city, shared_metrics):\n",
    "    agent.sample_traj_hier_nhpp(*sparsity_param, seed=seed, ha=3/4, replace_sparse_traj=True)\n",
    "    qstat = q_stat(agent)\n",
    "\n",
    "    # Run DBScan\n",
    "    for j, dbscan_param in enumerate(dbscan_params):\n",
    "        dbscan_out = sd._temporal_dbscan_labels(agent.sparse_traj, *dbscan_param, traj_cols)\n",
    "        stop_table = sd.temporal_dbscan(agent.sparse_traj, *dbscan_param, traj_cols, complete_output=False)\n",
    "        stop_table = identify_stop(dbscan_out, agent.sparse_traj, stop_table, city, method='mode')\n",
    "        _, metrics = cluster_metrics(stop_table, agent)\n",
    "        merged = metrics['Stops Merged']\n",
    "        split = metrics['Split']\n",
    "        recall = metrics['Recall']\n",
    "        precision = metrics['Precision']\n",
    "        # Save metrics\n",
    "        shared_metrics.append([qstat, seed, agent.identifier, ['dbscan', 'dbscan2'][j], \n",
    "                               merged, split, recall, precision])\n",
    "\n",
    "    # Run Lachesis\n",
    "    for j, lachesis_param in enumerate(lachesis_params):\n",
    "        lachesis_out = sd._lachesis_labels(agent.sparse_traj, *lachesis_param, traj_cols)\n",
    "        stop_table = sd.lachesis(agent.sparse_traj, *lachesis_param, traj_cols, complete_output=False)\n",
    "        stop_table = identify_stop(lachesis_out, agent.sparse_traj, stop_table, city, method='mode')\n",
    "        _, metrics = cluster_metrics(stop_table, agent)\n",
    "        merged = metrics['Stops Merged']\n",
    "        split = metrics['Split']\n",
    "        recall = metrics['Recall']\n",
    "        precision = metrics['Precision']\n",
    "        # Save metrics\n",
    "        shared_metrics.append([qstat, seed, agent.identifier, ['lachesis', 'lachesis2'][j], \n",
    "                               merged, split, recall, precision])\n",
    "\n",
    "def sequential_work(sparsity_params, N, city):\n",
    "    shared_metrics = []\n",
    "\n",
    "    total_tasks = N*len(sparsity_params)\n",
    "    task_counter = 0\n",
    "\n",
    "    with tqdm(total=total_tasks, desc=\"Processing tasks\") as pbar:\n",
    "        for sparsity_param in sparsity_params:\n",
    "            for seed in range(N):\n",
    "                process_sample(sparsity_param, seed, Chebyshev, city, shared_metrics)\n",
    "                task_counter += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "    return shared_metrics\n",
    "\n",
    "epr_metrics = sequential_work(sparsity_params, N, city)\n",
    "epr_metrics = pd.DataFrame(\n",
    "    epr_metrics,\n",
    "    columns=['q', 'seed', 'agent', 'algorithm', 'merged', 'split', 'recall', 'precision']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f042e1-7bff-4cf6-bdb7-3afa4e7c2ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Run with parallelization\n",
    "\n",
    "def process_sample(sparsity_param, seed, agent_id, city, shared_metrics):\n",
    "    agent = population_1000.roster[agent_id]\n",
    "    agent.sample_traj_hier_nhpp(*sparsity_param, seed=seed, ha=3/4, replace_sparse_traj=True)\n",
    "    qstat = q_stat(agent)\n",
    "\n",
    "    # Run DBScan\n",
    "    for j, dbscan_param in enumerate(dbscan_params):\n",
    "        dbscan_out = sd._temporal_dbscan_labels(agent.sparse_traj, *dbscan_param, traj_cols)\n",
    "        stop_table = sd.temporal_dbscan(agent.sparse_traj, *dbscan_param, traj_cols, complete_output=False)\n",
    "        stop_table = identify_stop(dbscan_out, agent.sparse_traj, stop_table, city, method='mode')\n",
    "        _, metrics = cluster_metrics(stop_table, agent)\n",
    "        merged = metrics['Stops Merged']\n",
    "        prop_merged = metrics['Prop Stops Merged']\n",
    "        split = metrics['Split']\n",
    "        recall = metrics['Recall']\n",
    "        precision = metrics['Precision']\n",
    "        # Save metrics\n",
    "        shared_metrics.append([qstat, seed, agent_id, ['dbscan-fine', 'dbscan-coarse'][j], \n",
    "                               merged, prop_merged, split, recall, precision])\n",
    "\n",
    "    # Run Lachesis\n",
    "    for j, lachesis_param in enumerate(lachesis_params):\n",
    "        lachesis_out = sd._lachesis_labels(agent.sparse_traj, *lachesis_param, traj_cols)\n",
    "        stop_table = sd.lachesis(agent.sparse_traj, *lachesis_param, traj_cols, complete_output=False)\n",
    "        stop_table = identify_stop(lachesis_out, agent.sparse_traj, stop_table, city, method='mode')\n",
    "        _, metrics = cluster_metrics(stop_table, agent)\n",
    "        merged = metrics['Stops Merged']\n",
    "        prop_merged = metrics['Prop Stops Merged']\n",
    "        split = metrics['Split']\n",
    "        recall = metrics['Recall']\n",
    "        precision = metrics['Precision']\n",
    "        # Save metrics\n",
    "        shared_metrics.append([qstat, seed, agent_id, ['lachesis', 'lachesis2'][j],\n",
    "                               merged, prop_merged, split, recall, precision])\n",
    "    \n",
    "def parallelize_work(sparsity_params, N, city):\n",
    "    with multiprocessing.Manager() as manager:\n",
    "        shared_metrics = manager.list()\n",
    "\n",
    "        # Using ProcessPoolExecutor for parallelism\n",
    "        with ProcessPoolExecutor(max_workers=36) as executor:\n",
    "            tasks = [\n",
    "                executor.submit(\n",
    "                    process_sample,\n",
    "                    sparsity_param,\n",
    "                    seed,\n",
    "                    agent_id,\n",
    "                    city,\n",
    "                    shared_metrics\n",
    "                )\n",
    "                for agent_id in population_1000.roster\n",
    "                for sparsity_param in sparsity_params\n",
    "                for seed in range(N)\n",
    "            ]\n",
    "\n",
    "            total_tasks = N * len(population_1000.roster) * len(sparsity_params)\n",
    "\n",
    "            with tqdm(total=total_tasks, desc=\"Processing tasks\") as pbar:\n",
    "                for future in concurrent.futures.as_completed(tasks):\n",
    "                    try:\n",
    "                        future.result()\n",
    "                        pbar.update(1)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Task failed with exception: {e}\")\n",
    "                        pbar.update(1)\n",
    "\n",
    "        return list(shared_metrics)\n",
    "\n",
    "epr_metrics = parallelize_work(sparsity_params, N, city)\n",
    "epr_metrics = pd.DataFrame(list(epr_metrics),\n",
    "                           columns=['q', 'seed', 'agent', 'algorithm', 'merged', 'prop_merged', 'split', 'recall', 'precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f31cf42-f9b7-42ec-92a9-86d5a7258da9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"epr_metrics.pkl\", \"rb\") as f:\n",
    "    epr_metrics = pickle.load(f)\n",
    "\n",
    "# epr_metrics['f1_score'] = 2 * (epr_metrics['precision'] * epr_metrics['recall']) / (epr_metrics['precision'] + epr_metrics['recall'])\n",
    "# epr_metrics.to_pickle(\"epr_metrics.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978942b5-a81c-4aaa-81b9-bd2a1f88a80a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epr_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9fe0dc-855f-4161-95b2-5de88133fb3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epr_metrics_avg = (\n",
    "    epr_metrics.groupby(['q', 'algorithm'])['merged']\n",
    "    .agg(lambda x: x.mean() if len(x) > 10 else None)  # Only take mean if more than 10\n",
    "    .reset_index()\n",
    ")\n",
    "pivoted = epr_metrics_avg.pivot(index='q', columns='algorithm', values='merged')\n",
    "\n",
    "ax = pivoted.plot(\n",
    "    kind='line',\n",
    "    figsize=(5, 4)\n",
    ")\n",
    "\n",
    "ax.set_title('Effect of Sparsity on Stop Merging', fontsize=16)\n",
    "ax.set_xlabel('Sparsity (q)', fontsize=12)\n",
    "ax.set_ylabel('Number of Stops Merged', fontsize=12)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"epr-merging.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dbab71-a87b-46ae-aca1-020660e86ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epr_metrics_avg = (\n",
    "    epr_metrics.groupby(['q', 'algorithm'])['precision']\n",
    "    .agg(lambda x: x.mean() if len(x) > 10 else None)  # Only take mean if more than 10\n",
    "    .reset_index()\n",
    ")\n",
    "pivoted = epr_metrics_avg.pivot(index='q', columns='algorithm', values='precision')\n",
    "\n",
    "ax = pivoted.plot(\n",
    "    kind='line',\n",
    "    figsize=(5, 4)\n",
    ")\n",
    "\n",
    "ax.set_title('Effect of Sparsity on Precision', fontsize=16)\n",
    "ax.set_xlabel('Sparsity (q)', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"epr-precision.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948cfc2a-057e-479b-9007-487d0faa800c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epr_metrics_avg = (\n",
    "    epr_metrics.groupby(['q', 'algorithm'])['recall']\n",
    "    .agg(lambda x: x.mean() if len(x) > 10 else None)  # Only take mean if more than 10\n",
    "    .reset_index()\n",
    ")\n",
    "pivoted = epr_metrics_avg.pivot(index='q', columns='algorithm', values='recall')\n",
    "\n",
    "ax = pivoted.plot(\n",
    "    kind='line',\n",
    "    figsize=(5, 4)\n",
    ")\n",
    "\n",
    "ax.set_title('Effect of Sparsity on Recall', fontsize=16)\n",
    "ax.set_xlabel('Sparsity (q)', fontsize=12)\n",
    "ax.set_ylabel('Recall', fontsize=12)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"epr-recall.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd6598f-f654-4931-84c1-306a3a7695fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epr_metrics_avg = (\n",
    "    epr_metrics.groupby(['q', 'algorithm'])['f1_score']\n",
    "    .agg(lambda x: x.mean() if len(x) > 10 else None)  # Only take mean if more than 10\n",
    "    .reset_index()\n",
    ")\n",
    "pivoted = epr_metrics_avg.pivot(index='q', columns='algorithm', values='f1_score')\n",
    "\n",
    "ax = pivoted.plot(\n",
    "    kind='line',\n",
    "    figsize=(5, 4)\n",
    ")\n",
    "\n",
    "ax.set_title('Effect of Sparsity on F1', fontsize=16)\n",
    "ax.set_xlabel('Sparsity (q)', fontsize=12)\n",
    "ax.set_ylabel('F1 Score', fontsize=12)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"epr-f1.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ffb88-b182-47ff-8881-f7334a7b3e61",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Proportion of stops missed v.s. amount of signal\n",
    "\n",
    "For each agent, sample their trajectory with different parameterizations. Compute some measure of completeness for their sampled trajectory (proportion of pings sampled, q, etc.). Compute the proportion of stops missed (recall, or we could go hour by hour?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb72b63c-c7a1-485e-98ec-583bf9886be4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def sample_agent_trajectory(agent_id, shared_roster, seed):\n",
    "    npr.seed(seed)\n",
    "    agent = shared_roster[agent_id]\n",
    "\n",
    "    # We could try different ranges and different distributions\n",
    "    beta_start = npr.uniform(60, 1200)\n",
    "    beta_duration = npr.uniform(15, 180)\n",
    "    beta_ping = npr.uniform(5, 30)\n",
    "\n",
    "    param = (beta_start, beta_duration, beta_ping)\n",
    "    agent.sample_traj_hier_nhpp(*param, seed=seed)\n",
    "    \n",
    "    # compute completeness\n",
    "    prop_sampled = len(agent.sparse_traj) / len(agent.trajectory)\n",
    "    q = q_stat(agent)\n",
    "    \n",
    "    sampling_stats.append((agent_id, beta_start, beta_duration, beta_ping, prop_sampled, q))\n",
    "\n",
    "    shared_roster[agent_id] = agent\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "shared_roster = manager.dict(population.roster)\n",
    "sampling_stats = manager.list()\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(sample_agent_trajectory, agent_id, shared_roster, i)\n",
    "        for i, agent_id in enumerate(shared_roster)\n",
    "    ]\n",
    "    concurrent.futures.wait(futures)\n",
    "\n",
    "population.roster = dict(shared_roster)\n",
    "sampling_stats = pd.DataFrame(list(sampling_stats), columns=['agent', 'beta_start', 'beta_duration', 'beta_ping', 'prop_sampled', 'q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc688057-01f5-4e11-b56e-f48dddedb7a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampling_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be926c5-2066-4b0b-865a-7049130ec887",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dbscan_params = [(180, 7, 2), (45, 2, 3)]\n",
    "\n",
    "def compute_stop_detection_metrics(agent_id, shared_roster, dbscan_params, seed):\n",
    "    agent = population.roster[agent_id]\n",
    "    combined = []\n",
    "    \n",
    "    for j, dbscan_param in enumerate(dbscan_params):\n",
    "        dbscan_out = sd.temporal_dbscan(agent.sparse_traj, *dbscan_param)\n",
    "        stop_table = sd.generate_stop_table(agent.sparse_traj, dbscan_out)\n",
    "        \n",
    "        if stop_table.empty:\n",
    "            continue\n",
    "        \n",
    "        stop_table = identify_stop(dbscan_out, agent.sparse_traj, stop_table)\n",
    "        _, metrics, _ = cluster_metrics(stop_table, agent)\n",
    "        \n",
    "        sd_metrics.append((agent_id, metrics['Recall'], metrics['Missed']/metrics['Stop Count'], ['coarse', 'fine'][j]))\n",
    "    \n",
    "    return pd.concat(combined, ignore_index=True) if combined else pd.DataFrame()\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "shared_roster = manager.dict(population.roster)\n",
    "sd_metrics = manager.list()\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(compute_stop_detection_metrics, agent_id, shared_roster, dbscan_params, i)\n",
    "        for i, agent_id in enumerate(shared_roster)\n",
    "    ]\n",
    "    concurrent.futures.wait(futures)\n",
    "\n",
    "sd_metrics = pd.DataFrame(list(sd_metrics), columns=['agent', 'recall', 'missed', 'dbscan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725f4c37-bd6c-4479-9962-e44fff61cd4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_df = pd.merge(sampling_stats, sd_metrics, on='agent', how='outer')\n",
    "combined_df\n",
    "\n",
    "with open('global-results.pkl', 'wb') as file:\n",
    "    pickle.dump(combined_df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47356457-8157-4a54-8047-9ebf04497c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('global-results.pkl', 'rb') as file:\n",
    "    combined_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ba53d6-284c-4c0b-aa2d-a2798a52d1b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bin_width = 0.025\n",
    "q_bins = np.arange(0, 1, bin_width)\n",
    "combined_df['q_bin'] = pd.cut(combined_df['q'], bins=q_bins, right=False)\n",
    "count_df = combined_df.groupby('q_bin').size().reset_index(name='count')\n",
    "averaged_df = combined_df.groupby(['dbscan', 'q_bin'])[['recall', 'missed']].mean().reset_index()\n",
    "averaged_df = averaged_df.merge(count_df, on='q_bin')\n",
    "averaged_df = averaged_df[averaged_df['count'] > 10]\n",
    "averaged_df['q_bin_med'] = averaged_df['q_bin'].apply(lambda x: (x.left + x.right) / 2)\n",
    "averaged_df\n",
    "\n",
    "# ADD CONFIDENCE INTERVALS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b668f-dc75-44c8-a44c-0da1cc12c075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dbscan_type, group_data in averaged_df.groupby('dbscan'):\n",
    "    plt.scatter(group_data['q_bin_med'], group_data['recall'], label=dbscan_type)\n",
    "    \n",
    "    # Trend line\n",
    "    sns.regplot(\n",
    "        x='q_bin_med', \n",
    "        y='recall', \n",
    "        data=group_data, \n",
    "        scatter=False, \n",
    "        ci=None\n",
    "    )\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel(\"q\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.title(\"Recall vs. q with Trend Lines\")\n",
    "plt.legend(title=\"DBSCAN Type\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"global.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eda8f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f50db20d",
   "metadata": {},
   "source": [
    "# Possible stop detection errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89288a08-3232-455e-a876-b4a95d08dc86",
   "metadata": {},
   "source": [
    "# other stuff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d62ef7-b869-48b4-bd3e-29c8008c25b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_metrics_avg = local_metrics.groupby(['beta_start', 'algorithm'])['precision'].mean().reset_index()\n",
    "pivoted = local_metrics_avg.pivot(index='beta_ping', columns='algorithm', values='precision')\n",
    "\n",
    "ax = pivoted.plot(\n",
    "    kind='line',\n",
    "    figsize=(5, 4)\n",
    ")\n",
    "\n",
    "ax.set_title('Effect of Ping Latency on Precision', fontsize=16)\n",
    "ax.set_xlabel('Within-burst Ping Latency', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"precision.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b2b058-d1ee-429c-9041-bfc0eb8e0500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_metrics_avg = local_metrics.groupby(['beta_ping', 'algorithm'])['recall'].mean().reset_index()\n",
    "pivoted = local_metrics_avg.pivot(index='beta_ping', columns='algorithm', values='recall')\n",
    "\n",
    "ax = pivoted.plot(\n",
    "    kind='line',\n",
    "    figsize=(5, 4)\n",
    ")\n",
    "\n",
    "ax.set_title('Effect of Ping Latency on Recall', fontsize=16)\n",
    "ax.set_xlabel('Within-burst Ping Latency', fontsize=12)\n",
    "ax.set_ylabel('Recall', fontsize=12)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"recall.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e459b8c7-ccd5-45f4-bfe8-da346778bf0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_metrics_avg = local_metrics.groupby(['beta_ping', 'algorithm'])['f1_score'].mean().reset_index()\n",
    "pivoted = local_metrics_avg.pivot(index='beta_ping', columns='algorithm', values='f1_score')\n",
    "\n",
    "ax = pivoted.plot(\n",
    "    kind='line',\n",
    "    figsize=(5, 4)\n",
    ")\n",
    "\n",
    "ax.set_title('Effect of Ping Latency on F1 Score', fontsize=16)\n",
    "ax.set_xlabel('Within-burst Ping Latency', fontsize=12)\n",
    "ax.set_ylabel('F1 Score', fontsize=12)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"f1.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d3fdbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed=20000\n",
    "beta_ping=2.0\n",
    "sd_param=(5, 120, 1.2)\n",
    "\n",
    "Charlie = Agent(identifier=\"Charlie\",\n",
    "        home='h-x13-y12',\n",
    "        workplace='w-x15-y9',\n",
    "        city=city,\n",
    "        destination_diary=destination,\n",
    "        start_time=datetime(2024, 6, 1, hour=0, minute=0),\n",
    "        dt=0.5)\n",
    "# simulate trajectory and sparsify\n",
    "population.generate_trajectory(Charlie, seed=seed)\n",
    "Charlie.sample_traj_hier_nhpp(beta_start, beta_dur, beta_ping, seed=seed)\n",
    "\n",
    "sd_out = sd._lachesis_labels(Charlie.sparse_traj, *sd_param, traj_cols)\n",
    "sd_out.index = sd_out.index.map(lambda x: int(x.timestamp()))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "num_clusters = sum(sd_out.cluster.unique() > -1)\n",
    "for cid in range(num_clusters):\n",
    "    cpings = sd_out[sd_out.cluster == cid]\n",
    "    cdata = Charlie.sparse_traj.loc[cpings.index]\n",
    "    col = cm.tab20c(cid/(num_clusters+1))\n",
    "    ax.scatter(cdata.x, cdata.y, s=80, color=col, alpha=1, zorder=2)\n",
    "\n",
    "ax.scatter(x=Charlie.sparse_traj.x, \n",
    "           y=Charlie.sparse_traj.y, \n",
    "           s=6, color='black', alpha=1, zorder=2)\n",
    "city.plot_city(ax, doors=True, address=False, zorder=1)\n",
    "\n",
    "ax.plot(Charlie.sparse_traj.x,\n",
    "        Charlie.sparse_traj.y,\n",
    "        linewidth=1, color='blue', alpha=0.2)\n",
    "\n",
    "ax.set_xlim(12, 17)\n",
    "ax.set_ylim(8, 13)\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"clusters.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a0d8c3-6ebd-4f8f-a0cb-2e79b93db5b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## need to check everything below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa64ad-2f7f-40d5-a8bb-c2f5cba35480",
   "metadata": {},
   "source": [
    "## Bar charts from diaries, type vs fraction_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655624ee-04ec-4c7b-a97c-09c671dccf22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_type = []\n",
    "output_size = []\n",
    "output_truehome = []\n",
    "for agent_id, agent in population_n.roster.items():\n",
    "    d = agent.diary.dropna()\n",
    "    d['id'] = agent_id\n",
    "    d['type'] = d['location'].apply(lambda b: city.buildings[b].building_type)\n",
    "    d['size'] = d['location'].apply(lambda b: len(city.buildings[b].blocks) if b in city.buildings else None)\n",
    "    d['true_home'] = d.apply(lambda x: agent.home == x.location, axis=1)\n",
    "    bin_edges = [0, 1, 4, 100]\n",
    "    bin_labels = ['small', 'medium', 'large']\n",
    "    d['size'] = pd.cut(d['size'], bins=bin_edges, labels=bin_labels, include_lowest=True)\n",
    "\n",
    "    d_type_agg = d.groupby(['id', 'type'], observed=False).duration.sum().reset_index()\n",
    "    d_size_agg = d.groupby(['id', 'size'], observed=False).duration.sum().reset_index()\n",
    "    d_truehome_agg = d.groupby(['id', 'true_home'], observed=False).duration.sum().reset_index()\n",
    "    output_type += [d_type_agg]\n",
    "    output_size += [d_size_agg]\n",
    "    output_truehome += [d_truehome_agg]\n",
    "\n",
    "bars_type_gc = pd.concat(output_type)\n",
    "temp = bars_type_gc.groupby('id')['duration'].transform('sum')\n",
    "bars_type_gc['duration'] = bars_type_gc['duration'] / temp\n",
    "\n",
    "bars_size_gc = pd.concat(output_size)\n",
    "temp = bars_size_gc.groupby('id')['duration'].transform('sum')\n",
    "bars_size_gc['duration'] = bars_size_gc['duration'] / temp\n",
    "\n",
    "bars_trhome_gc = pd.concat(output_truehome)\n",
    "temp = bars_trhome_gc.groupby('id')['duration'].transform('sum')\n",
    "bars_trhome_gc['duration'] = bars_trhome_gc['duration'] / temp\n",
    "\n",
    "all_stop_tables['type'] = all_stop_tables['location'].apply(\n",
    "    lambda b: city.buildings[b].building_type if b in city.buildings else None)\n",
    "\n",
    "all_stop_tables['true_home'] = all_stop_tables.apply(\n",
    "    lambda x: population_n.roster[x.id].home == x.location, axis=1)\n",
    "\n",
    "all_stop_tables['size'] = all_stop_tables['location'].apply(\n",
    "    lambda b: len(city.buildings[b].blocks) if b in city.buildings else None)\n",
    "bin_edges = [0, 1, 4, 100]\n",
    "bin_labels = ['small', 'medium', 'large']\n",
    "all_stop_tables['size'] = pd.cut(all_stop_tables['size'], bins=bin_edges, labels=bin_labels, include_lowest=True)\n",
    "\n",
    "bars_type = all_stop_tables.groupby(['id', 'dbscan', 'sparsity', 'type'], observed=False).duration.sum().reset_index()\n",
    "temp = bars_type.groupby(['id', 'dbscan', 'sparsity'])['duration'].transform('sum')\n",
    "bars_type['duration'] = bars_type['duration'] / temp\n",
    "\n",
    "bars_size = all_stop_tables.groupby(['id', 'dbscan', 'sparsity', 'size'], observed=False).duration.sum().reset_index()\n",
    "temp = bars_size.groupby(['id', 'dbscan', 'sparsity'])['duration'].transform('sum')\n",
    "bars_size['duration'] = bars_size['duration'] / temp\n",
    "\n",
    "bars_trhome = all_stop_tables.groupby(['id', 'dbscan', 'sparsity', 'true_home'], observed=False).duration.sum().reset_index()\n",
    "temp = bars_trhome.groupby(['id', 'dbscan', 'sparsity'])['duration'].transform('sum')\n",
    "bars_trhome['duration'] = bars_trhome['duration'] / temp\n",
    "\n",
    "# Ensure all combinations of 'id', 'dbscan', 'sparsity', and 'type' are present\n",
    "unique_ids = bars_type['id'].unique()\n",
    "unique_dbscan = bars_type['dbscan'].unique()\n",
    "unique_sparsity = bars_type['sparsity'].unique()\n",
    "unique_types = bars_type['type'].unique()\n",
    "all_combinations = pd.DataFrame(list(product(unique_ids, unique_dbscan, unique_sparsity, unique_types)),\n",
    "                                columns=['id', 'dbscan', 'sparsity', 'type'])\n",
    "bars_type = all_combinations.merge(bars_type, on=['id', 'dbscan', 'sparsity', 'type'], how='left')\n",
    "bars_type['duration'] = bars_type['duration'].fillna(0)\n",
    "\n",
    "bars_rog = all_stop_tables.groupby(['id', 'dbscan', 'sparsity']).agg(\n",
    "    rog_d=('rog_d', 'mean'),\n",
    "    rog_sd=('rog_sd', 'mean')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8105e09-306a-479c-9d31-c64beefc54e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sparsity_levels = ['low', 'high']\n",
    "dbscan_options = ['coarse', 'fine']\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(sparsity_levels))\n",
    "\n",
    "grouped_gc = bars_trhome_gc.groupby('true_home')['duration']\n",
    "mean_duration_gc = grouped_gc.mean()[True]\n",
    "std_duration_gc = grouped_gc.std()[True]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "\n",
    "colors = [None, 'lightcoral', 'lightgreen']\n",
    "labels = ['Ground Truth', 'Coarse DBSCAN', 'Fine DBSCAN']\n",
    "labelsx = ['Low Sparsity', 'High Sparsity']\n",
    "\n",
    "ax.bar(index, mean_duration_gc, bar_width, yerr=std_duration_gc, capsize=5, label='Ground Truth', color=colors[0])\n",
    "\n",
    "for j, dbscan_param in enumerate(dbscan_options):\n",
    "    df_sparsity = bars_trhome[bars_trhome['dbscan'] == dbscan_param]\n",
    "\n",
    "    mean_duration = df_sparsity.groupby('sparsity')['duration'].mean()\n",
    "    std_duration = df_sparsity.groupby('sparsity')['duration'].std()\n",
    "\n",
    "    mean_duration = mean_duration.sort_index(ascending=False)\n",
    "    std_duration = std_duration.sort_index(ascending=False)\n",
    "\n",
    "    bar_positions = index + (j + 1) * bar_width\n",
    "    ax.bar(bar_positions, mean_duration.values, bar_width, yerr=std_duration.values, \n",
    "           capsize=5, label=labels[j + 1], color=colors[j + 1])\n",
    "    ax.grid(True)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "ax.set_title('True Home')\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(labelsx, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e98b7-59b7-448a-a249-fd1d508f9eb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sparsity_levels = ['low', 'high']\n",
    "dbscan_options = ['coarse', 'fine']\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(sparsity_levels))\n",
    "\n",
    "mean_gc = bars_rog['rog_d'].mean()\n",
    "std_gc = bars_rog['rog_d'].std()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "\n",
    "colors = [None, 'lightcoral', 'lightgreen']\n",
    "labels = ['Ground Truth', 'Coarse DBSCAN', 'Fine DBSCAN']\n",
    "labelsx = ['Low Sparsity', 'High Sparsity']\n",
    "\n",
    "ax.bar(index, mean_gc, bar_width, yerr=std_gc, capsize=5, label='Ground Truth', color=colors[0])\n",
    "\n",
    "for j, dbscan_param in enumerate(dbscan_options):\n",
    "    df_sparsity = bars_rog[bars_rog['dbscan'] == dbscan_param]\n",
    "\n",
    "    mean_duration = df_sparsity.groupby('sparsity')['rog_sd'].mean()\n",
    "    std_duration = df_sparsity.groupby('sparsity')['rog_sd'].std()\n",
    "\n",
    "    mean_duration = mean_duration.sort_index(ascending=False)\n",
    "    std_duration = std_duration.sort_index(ascending=False)\n",
    "\n",
    "    bar_positions = index + (j + 1) * bar_width\n",
    "    ax.bar(bar_positions, mean_duration.values, bar_width, yerr=std_duration.values, \n",
    "           capsize=5, label=labels[j + 1], color=colors[j + 1])\n",
    "    ax.grid(True)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "ax.set_title('Radius of Gyration')\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(labelsx, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0637793-2fc3-4cc3-86b5-e319c4f7da5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sparsity_levels = ['low', 'high']\n",
    "dbscan_options = ['coarse', 'fine']\n",
    "building_types = bars_type_gc['type'].unique()\n",
    "\n",
    "grouped_gc = bars_type_gc.groupby('type')['duration']\n",
    "mean_duration_gc = grouped_gc.mean()\n",
    "std_duration_gc = grouped_gc.std()\n",
    "\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(sparsity_levels))\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(9, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = [None, 'lightcoral', 'lightgreen']\n",
    "labels = ['Ground Truth', 'Coarse DBSCAN', 'Fine DBSCAN']\n",
    "labelsx = ['Low Sparsity', 'High Sparsity']\n",
    "\n",
    "for k, building_type in enumerate(building_types):\n",
    "    ax = axes[k]\n",
    "\n",
    "    mean_gc = mean_duration_gc[building_type]\n",
    "    std_gc = std_duration_gc[building_type]\n",
    "    ax.bar(index, mean_gc, bar_width, yerr=std_gc, capsize=5, label='Ground Truth', color=colors[0])\n",
    "\n",
    "    for j, dbscan_param in enumerate(dbscan_options):\n",
    "        df_sparsity = bars_type[bars_type['dbscan'] == dbscan_param]\n",
    "        df_sparsity = df_sparsity[df_sparsity['type'] == building_type]\n",
    "\n",
    "        mean_duration = df_sparsity.groupby('sparsity')['duration'].mean()\n",
    "        std_duration = df_sparsity.groupby('sparsity')['duration'].std()\n",
    "\n",
    "        mean_duration = mean_duration.sort_index(ascending=False)\n",
    "        std_duration = std_duration.sort_index(ascending=False)\n",
    "\n",
    "        bar_positions = index + (j + 1) * bar_width\n",
    "        ax.bar(bar_positions, mean_duration.values, bar_width, yerr=std_duration.values, \n",
    "               capsize=5, label=labels[j + 1], color=colors[j + 1])\n",
    "\n",
    "    ax.set_title(f'{building_type.capitalize()}', fontsize=16)\n",
    "    ax.set_ylabel('Duration (%)', fontsize=14)\n",
    "    ax.set_xticks(index + bar_width)\n",
    "    ax.set_xticklabels(labelsx, rotation=0, fontsize=14)\n",
    "    ax.set_ylim(0,)\n",
    "    #ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n",
    "    ax.grid(True)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.1), ncol=3, fontsize=14)\n",
    "\n",
    "plt.savefig('bar_charts.svg')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b22105e-17b9-470b-be1e-105bd3522102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sparsity_levels = ['low', 'high']\n",
    "dbscan_options = ['coarse', 'fine']\n",
    "building_sizes = bars_size_gc['size'].unique()\n",
    "\n",
    "grouped_gc = bars_size_gc.groupby('size', observed=False)['duration']\n",
    "mean_duration_gc = grouped_gc.mean()\n",
    "std_duration_gc = grouped_gc.std()\n",
    "\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(sparsity_levels))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = [None, 'lightcoral', 'lightgreen']\n",
    "labels = ['Ground Truth', 'Coarse DBSCAN', 'Fine DBSCAN']\n",
    "labelsx = ['Low Sparsity', 'High Sparsity']\n",
    "\n",
    "for k, building_size in enumerate(building_sizes):\n",
    "    ax = axes[k]\n",
    "\n",
    "    mean_gc = mean_duration_gc[building_size]\n",
    "    std_gc = std_duration_gc[building_size]\n",
    "    ax.bar(index, mean_gc, bar_width, yerr=std_gc, capsize=5, label='Ground Truth', color=colors[0])\n",
    "\n",
    "    for j, dbscan_param in enumerate(dbscan_options):\n",
    "        df_sparsity = bars_size[bars_size['dbscan'] == dbscan_param]\n",
    "        df_sparsity = df_sparsity[df_sparsity['size'] == building_size]\n",
    "\n",
    "        mean_duration = df_sparsity.groupby('sparsity')['duration'].mean()\n",
    "        std_duration = df_sparsity.groupby('sparsity')['duration'].std()\n",
    "\n",
    "        mean_duration = mean_duration.sort_index(ascending=False)\n",
    "        std_duration = std_duration.sort_index(ascending=False)\n",
    "\n",
    "        bar_positions = index + (j + 1) * bar_width\n",
    "        ax.bar(bar_positions, mean_duration.values, bar_width, yerr=std_duration.values, \n",
    "               capsize=5, label=labels[j + 1], color=colors[j + 1])\n",
    "\n",
    "    ax.set_title(f'{building_size.capitalize()} Buildings', fontsize=16)\n",
    "    ax.set_ylabel('Duration (%)', fontsize=14)\n",
    "    ax.set_xticks(index + bar_width)\n",
    "    ax.set_xticklabels(labelsx, rotation=0, fontsize=14)\n",
    "    ax.set_ylim(0,)\n",
    "    #ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n",
    "    ax.grid(True)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.05), ncol=3)\n",
    "\n",
    "plt.savefig('bar_size.svg')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e5a12-9e79-422f-85b1-8b13fb89b610",
   "metadata": {},
   "source": [
    "# Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc4ed8-eed3-4ace-ab00-9a03d35e20e5",
   "metadata": {},
   "source": [
    "### Fix frequency (q). Vary sparsity pattern by changing first two params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a43150b-5a16-48be-97b1-676acbb8a61e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def process_seed(seed, nhpp_params, dbscan_params, lachesis_params, city):\n",
    "    results_dbscan = []\n",
    "    results_lachesis = []\n",
    "    \n",
    "    Bethany = Agent('Bethany',\n",
    "                    'h-x13-y9',\n",
    "                    'w-x3-y13',\n",
    "                    city,\n",
    "                    start_time=datetime(2024, 1, 1, hour=0, minute=0))\n",
    "\n",
    "    population.add_agent(Bethany, verbose=False)\n",
    "    population.generate_trajectory(Bethany, T=datetime(2024, 1, 15, hour=0, minute=0), seed=seed)\n",
    "\n",
    "    for nhpp_param in nhpp_params: \n",
    "        Bethany.sample_traj_hier_nhpp(*nhpp_param, seed=seed)\n",
    "        qstat = q_stat(Bethany)\n",
    "\n",
    "        for k in range(len(dbscan_params)):\n",
    "            dbscan_param = dbscan_params[k]\n",
    "            lachesis_param = lachesis_params[k]\n",
    "\n",
    "            dbscan_out = sd.temporal_dbscan(Bethany.sparse_traj, *dbscan_param)\n",
    "            lachesis_out = sd.lachesis_patches(Bethany.sparse_traj, *lachesis_param)\n",
    "\n",
    "            stop_table_dbscan = sd.generate_stop_table(Bethany.sparse_traj, dbscan_out)\n",
    "            stop_table_lachesis = sd.generate_stop_table(Bethany.sparse_traj, lachesis_out)\n",
    "\n",
    "            result_dbscan = {\n",
    "                'seed': seed,\n",
    "                'alg_param': ['coarse', 'fine'][k],\n",
    "                'beta_dur': nhpp_param[1],\n",
    "                'q': qstat,\n",
    "                'recall': 0,\n",
    "                'precision': 0,\n",
    "                'stop_merging': None,\n",
    "                'trip_merging': None,\n",
    "                'missed': 0,\n",
    "                'split': None,\n",
    "                'rog_diary': 0,\n",
    "                'rog_alg': 0\n",
    "            }\n",
    "\n",
    "            result_lachesis = result_dbscan.copy()\n",
    "\n",
    "            if stop_table_dbscan.empty:\n",
    "                prepared_diary, _ = prepare_diary(Bethany)\n",
    "                n_stops = np.sum(prepared_diary['stop_id'].unique() > 0)\n",
    "                result_dbscan.update({\n",
    "                    'recall': 0,\n",
    "                    'precision': None,\n",
    "                    'stop_merging': None,\n",
    "                    'trip_merging': None,\n",
    "                    'missed': n_stops,\n",
    "                    'split': None,\n",
    "                    'rog_diary': radius_of_gyration(prepared_diary[['x', 'y']]),\n",
    "                    'rog_alg': 0\n",
    "                })\n",
    "            else:\n",
    "                stop_table_dbscan = identify_stop(dbscan_out, Bethany.sparse_traj, stop_table_dbscan)\n",
    "                # stop_table_dbscan['location'] = stop_table_dbscan.apply(\n",
    "                #     lambda row: city.get_block((row.centroid_x, row.centroid_y)).id, axis=1)\n",
    "                _, metrics, rog = cluster_metrics(stop_table_dbscan, Bethany)\n",
    "                result_dbscan.update({\n",
    "                    'recall': metrics['Recall'],\n",
    "                    'precision': metrics['Precision'],\n",
    "                    'stop_merging': metrics['Stops Merged'],#/metrics['Stop Count'],\n",
    "                    'trip_merging': metrics['Trip Merging'],\n",
    "                    'missed': metrics['Missed'],#/metrics['Stop Count'],\n",
    "                    'split': metrics['Split'],#/metrics['Stop Count'],\n",
    "                    'rog_diary': rog['diary'],\n",
    "                    'rog_alg': rog['stop_table']\n",
    "                })\n",
    "\n",
    "            if stop_table_lachesis.empty:\n",
    "                prepared_diary, _ = prepare_diary(Bethany)\n",
    "                n_stops = np.sum(prepared_diary['stop_id'].unique() > 0)\n",
    "                result_lachesis.update({\n",
    "                    'recall': 0,\n",
    "                    'precision': None,\n",
    "                    'stop_merging': None,\n",
    "                    'trip_merging': None,\n",
    "                    'missed': n_stops,\n",
    "                    'split': None,\n",
    "                    'rog_diary': radius_of_gyration(prepared_diary[['x', 'y']]),\n",
    "                    'rog_alg': 0\n",
    "                })\n",
    "            else:\n",
    "                stop_table_lachesis = identify_stop(lachesis_out, Bethany.sparse_traj, stop_table_lachesis)\n",
    "                # stop_table_lachesis['location'] = stop_table_lachesis.apply(\n",
    "                #     lambda row: city.get_block((row.centroid_x, row.centroid_y)).id, axis=1)\n",
    "                _, metrics, rog = cluster_metrics(stop_table_lachesis, Bethany)\n",
    "                result_lachesis.update({\n",
    "                    'recall': metrics['Recall'],\n",
    "                    'precision': metrics['Precision'],\n",
    "                    'stop_merging': metrics['Stops Merged'],#/metrics['Stop Count'],\n",
    "                    'trip_merging': metrics['Trip Merging'],\n",
    "                    'missed': metrics['Missed'],#/metrics['Stop Count'],\n",
    "                    'split': metrics['Split'],#/metrics['Stop Count'],\n",
    "                    'rog_diary': rog['diary'],\n",
    "                    'rog_alg': rog['stop_table']\n",
    "                })\n",
    "\n",
    "                results_dbscan.append(result_dbscan)\n",
    "                results_lachesis.append(result_lachesis)\n",
    "\n",
    "    return pd.DataFrame(results_dbscan), pd.DataFrame(results_lachesis)\n",
    "\n",
    "\n",
    "seeds = range(1, 175)\n",
    "dbscan_params = [(360, 5, 2), (60, 1.5, 3)]\n",
    "lachesis_params = [(10, 480, 6), (10, 60, 1.5)]\n",
    "beta_starts = range(120, 1600, 40)\n",
    "beta_durs = range(30, 400, 10)\n",
    "beta_ping = 10\n",
    "\n",
    "nhpp_params = [(start, dur, beta_ping) for start, dur in zip(beta_starts, beta_durs)]\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    work = partial(process_seed, nhpp_params=nhpp_params,\n",
    "                   dbscan_params=dbscan_params, lachesis_params=lachesis_params,\n",
    "                   city=city)\n",
    "    all_results = list(executor.map(work, seeds))\n",
    "\n",
    "results_dbscan = [result[0] for result in all_results]\n",
    "results_lachesis = [result[1] for result in all_results]\n",
    "\n",
    "results_dbscan = pd.concat(results_dbscan, ignore_index=True)\n",
    "results_lachesis = pd.concat(results_lachesis, ignore_index=True)\n",
    "\n",
    "results_dbscan.to_pickle('results-sparsity-dbscan-nofilter-1.pkl')\n",
    "results_lachesis.to_pickle('results-sparsity-lachesis-nofilter-1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1102e9e4-38ee-4578-9c0b-a16f2da577cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('results-sparsity-dbscan-nofilter.pkl', 'rb') as file:\n",
    "    results_dbscan = pickle.load(file)\n",
    "\n",
    "with open('results-sparsity-lachesis-nofilter.pkl', 'rb') as file:\n",
    "    results_lachesis = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8566733-392e-426d-b456-82bcea9b9d1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create bins for q, size 0.05 from 0 to 1\n",
    "# bins = np.arange(0, 1.05, 0.05)\n",
    "# labels = [i/100+0.025 for i in range(0, 100, 5)]\n",
    "# results_dbscan['q_bin'] = pd.cut(results_dbscan['q'], bins=bins, labels=labels, include_lowest=True)\n",
    "# results_lachesis['q_bin'] = pd.cut(results_lachesis['q'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "results_dbscan_sparsity = results_dbscan.groupby(['beta_dur', 'alg_param'], observed=True).mean().reset_index()\n",
    "counts = results_dbscan.groupby(['beta_dur', 'alg_param'], observed=True).size().reset_index(name='count')\n",
    "results_dbscan_sparsity = results_dbscan_sparsity.merge(counts, on=['beta_dur', 'alg_param'])\n",
    "results_dbscan_sparsity = results_dbscan_sparsity[results_dbscan_sparsity['count'] >= 10]\n",
    "\n",
    "results_lachesis_sparsity = results_lachesis.groupby(['beta_dur', 'alg_param'], observed=True).mean().reset_index()\n",
    "counts = results_lachesis.groupby(['beta_dur', 'alg_param'], observed=True).size().reset_index(name='count')\n",
    "results_lachesis_sparsity = results_lachesis_sparsity.merge(counts, on=['beta_dur', 'alg_param'])\n",
    "results_lachesis_sparsity = results_lachesis_sparsity[results_lachesis_sparsity['count'] >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af8ed55-6773-40a7-acfc-f48492dd1e35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_dbscan_sparsity\n",
    "#results_lachesis_sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28025e93-4cf8-4b02-89af-b7434038524c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 1.5))\n",
    "\n",
    "for i, p in enumerate([2,4,5]):\n",
    "    ax = axes[i]\n",
    "    for j in range(2):\n",
    "        setting = ['coarse', 'fine'][j]\n",
    "        results_df = results_dbscan_sparsity[results_dbscan_sparsity['alg_param']==setting]\n",
    "        ax.plot(results_df['beta_dur'], results_df.iloc[:, p+4],\n",
    "                label=['Coarse', 'Fine'][j],\n",
    "                color=['red', 'darkgreen'][j],\n",
    "                linestyle=['-', ':'][j])\n",
    "        ax.set_title(['Stop Merging', 'Missed', 'Split'][i])\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_xlabel('Expected Burst Durations')\n",
    "\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', fontsize='large',\n",
    "           ncol=3, bbox_to_anchor=(0.51, -0.55))\n",
    "plt.subplots_adjust(hspace=0.8, wspace=0.2)\n",
    "\n",
    "plt.savefig(\"metrics_vs_sparsity-dbscan.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a933407-d294-44e8-883b-ef92b360f0bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(12, 4))\n",
    "\n",
    "for i in range(6):\n",
    "    ax = axes[i % 2, i//2]\n",
    "    for j in range(2):\n",
    "        setting = ['coarse', 'fine'][j]\n",
    "        results_df = results_lachesis_sparsity[results_lachesis_sparsity['alg_param']==setting]\n",
    "        ax.plot(results_df['beta_dur'], results_df.iloc[:, i+4],\n",
    "                label=['Coarse', 'Fine'][j],\n",
    "                color=['black', 'red'][j],\n",
    "                linestyle=['-', ':'][j])\n",
    "        ax.set_title(['Recall', 'Precision', 'Stop Merging', 'Trip Merging', 'Missed', 'Split'][i])\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_xlabel('Expected Burst Durations')\n",
    "\n",
    "handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', fontsize='large',\n",
    "           ncol=3, bbox_to_anchor=(0.51, -0.15))\n",
    "plt.subplots_adjust(hspace=0.8, wspace=0.2)\n",
    "\n",
    "plt.savefig(\"metrics_vs_sparsity-lachesis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a0ce99-9176-4591-91f6-2edd7f28a4ac",
   "metadata": {},
   "source": [
    "### Fix sparsity. Vary DBSCAN parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022f861-0b9d-42a7-8a1e-49a59c5fa3f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seeds = range(100, 150)\n",
    "hier_nhpp_params = [(90, 30, 4), (60, 60, 2)]  # 3pph, 12pph\n",
    "\n",
    "# dbscan_params = (time_thresh, dist_thresh, min_pts)\n",
    "dist_threshs = np.arange(5/15, 200/15, 0.25)\n",
    "\n",
    "results = np.zeros((len(seeds), len(hier_nhpp_params), len(dist_threshs), 8))\n",
    "\n",
    "for i, seed in enumerate(seeds):\n",
    "    Bethany = Agent('Bethany',\n",
    "                    'h-x13-y9',\n",
    "                    'w-x3-y13',\n",
    "                    city,\n",
    "                    destination_diary=destination)\n",
    "    population.add_agent(Bethany, verbose=False)\n",
    "    population.generate_trajectory(Bethany, seed=seed, dt=1)\n",
    "\n",
    "    for j, hier_nhpp_param in enumerate(hier_nhpp_params):\n",
    "        Bethany.sample_traj_hier_nhpp(*hier_nhpp_param, seed=seed)\n",
    "        truepph = round(len(Bethany.sparse_traj) * 2 / 9) / 2\n",
    "        print(truepph)\n",
    "\n",
    "        for k, dist_thresh in enumerate(dist_threshs):\n",
    "            dbscan_param = (240, dist_thresh, 2)\n",
    "            dbscan_out = sd.temporal_dbscan(Bethany.sparse_traj, *dbscan_param)\n",
    "            stop_table = sd.generate_stop_table(Bethany.sparse_traj, dbscan_out)\n",
    "\n",
    "            if stop_table.empty:\n",
    "                prepared_diary, _ = prepare_diary(Bethany)\n",
    "                n_stops = np.sum(prepared_diary['stop_id'].unique() > 0)\n",
    "\n",
    "                results[i, j, k, 0] = dist_thresh\n",
    "                results[i, j, k, 1] = 0  # Recall\n",
    "                results[i, j, k, 2] = 0  # Precision\n",
    "                results[i, j, k, 3] = None  # Weighted Stop Merging\n",
    "                results[i, j, k, 4] = None  # Trip Merging\n",
    "                results[i, j, k, 5] = n_stops  # Stops missed\n",
    "                results[i, j, k, 6] = None  # Stops split\n",
    "                results[i, j, k, 7] = q_stat(Bethany)  # q\n",
    "                continue\n",
    "\n",
    "            stop_table['location'] = stop_table.apply(\n",
    "                lambda row: city.get_block((row.centroid_x, row.centroid_y)).id, axis=1)\n",
    "            metrics_df, metrics = cluster_metrics(stop_table, Bethany)\n",
    "\n",
    "            results[i, j, k, 0] = dist_thresh\n",
    "            results[i, j, k, 1] = metrics['Recall']\n",
    "            results[i, j, k, 2] = metrics['Precision']\n",
    "            #results[i, j, k, 3] = metrics['Weighted Stop Merging']\n",
    "            results[i, j, k, 3] = metrics['Stops Merged']\n",
    "            results[i, j, k, 4] = metrics['Trip Merging']\n",
    "            results[i, j, k, 5] = metrics['Missed']\n",
    "            results[i, j, k, 6] = metrics['Split']\n",
    "            results[i, j, k, 7] = q_stat(Bethany)\n",
    "\n",
    "    print(seed)\n",
    "\n",
    "results_param = np.nanmean(results, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04dff38-ad57-4346-a0aa-2118548d1314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(10, 10))\n",
    "\n",
    "for i in range(6):\n",
    "    ax = axes[i//2, i % 2]\n",
    "    for j in range(2):\n",
    "        results_df = pd.DataFrame(\n",
    "            results_param[j, :, :],\n",
    "            columns=['dist_thresh', 'recall', 'precision', 'weighted_stop_merging', 'trip_merging', 'missed', 'split', 'q_stat'])\n",
    "        ax.plot(results_df['dist_thresh'], results_df.iloc[:, i+1],\n",
    "                label=['3pph', '12pph'][j], color=['black', 'red'][j], linestyle=['-', '--'][j])\n",
    "        ax.set_title(['Recall', 'Precision', 'Stop Merging', 'Trip Merging', 'Missed', 'Split'][i])\n",
    "        ax.set_ylabel('')\n",
    "\n",
    "#fig.suptitle('Metrics v.s. DBSCAN parameters', fontsize=20)\n",
    "handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', fontsize='large',\n",
    "           ncol=2, bbox_to_anchor=(0.5, 0.03))\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.2)\n",
    "\n",
    "plt.savefig(\"metrics_vs_params.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19945c7-4287-4412-abaf-9803724c3a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
