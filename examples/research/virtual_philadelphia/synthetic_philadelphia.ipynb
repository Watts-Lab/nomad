{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf872541-c40e-4f6c-8c41-8b47835d551c",
   "metadata": {},
   "source": [
    "# Synthetic Philadelphia - Production Pipeline\n",
    "\n",
    "Full rasterization pipeline with EPR destination diary generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e13f0a6-9073-42b7-86ea-c8746c909d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import box\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as cx\n",
    "\n",
    "import nomad.map_utils as nm\n",
    "from nomad.city_gen import RasterCity\n",
    "from nomad.traj_gen import Population\n",
    "from nomad.io.base import from_file\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec81266",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d04e8b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "LARGE_BOX = box(-75.1905, 39.9235, -75.1425, 39.9535)\n",
    "MEDIUM_BOX = box(-75.1665, 39.9385, -75.1425, 39.9535)\n",
    "\n",
    "USE_FULL_CITY = False\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if USE_FULL_CITY:\n",
    "    BOX_NAME = \"full\"\n",
    "    POLY = \"Philadelphia, Pennsylvania, USA\"\n",
    "else:\n",
    "    BOX_NAME = \"large\"\n",
    "    POLY = LARGE_BOX\n",
    "\n",
    "SANDBOX_GPKG = OUTPUT_DIR / f\"spatial_data_{BOX_NAME}.gpkg\"\n",
    "REGENERATE_DATA = False  # Set to True to regenerate data with rotation metadata\n",
    "\n",
    "config = {\n",
    "    \"box_name\": BOX_NAME,\n",
    "    \"block_side_length\": 15.0,\n",
    "    \"hub_size\": 100,\n",
    "    \"N\": 200,\n",
    "    \"name_seed\": 42,\n",
    "    \"name_count\": 2,\n",
    "    \"epr_params\": {\n",
    "        \"datetime\": \"2025-05-23 00:00-05:00\",\n",
    "        \"end_time\": \"2025-07-01 00:00-05:00\",\n",
    "        \"epr_time_res\": 15,\n",
    "        \"rho\": 0.4,\n",
    "        \"gamma\": 0.3,\n",
    "        \"seed_base\": 100\n",
    "    },\n",
    "    \"traj_params\": {\n",
    "        \"dt\": 0.5,\n",
    "        \"seed_base\": 200\n",
    "    },\n",
    "    \"sampling_params\": {\n",
    "        \"beta_ping\": 7,\n",
    "        \"beta_start\": 300,\n",
    "        \"beta_durations\": 55,\n",
    "        \"ha\": 11.5/15,\n",
    "        \"seed_base\": 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be88c3",
   "metadata": {},
   "source": [
    "## Data Generation (OSM Download + Rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0185669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing data from output\\spatial_data_large.gpkg\n"
     ]
    }
   ],
   "source": [
    "if REGENERATE_DATA or not SANDBOX_GPKG.exists():\n",
    "    print(\"=\"*50)\n",
    "    print(\"DATA GENERATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    buildings = nm.download_osm_buildings(\n",
    "        POLY,\n",
    "        crs=\"EPSG:3857\",\n",
    "        schema=\"garden_city\",\n",
    "        clip=True,\n",
    "        infer_building_types=True,\n",
    "        explode=True,\n",
    "    )\n",
    "    download_buildings_time = time.time() - t0\n",
    "    print(f\"Buildings download: {download_buildings_time:>6.2f}s ({len(buildings):,} buildings)\")\n",
    "    \n",
    "    if USE_FULL_CITY:\n",
    "        boundary_polygon = nm.get_city_boundary_osm(POLY, simplify=True)[0]\n",
    "        boundary_polygon = gpd.GeoSeries([boundary_polygon], crs=\"EPSG:4326\").to_crs(\"EPSG:3857\").iloc[0]\n",
    "    else:\n",
    "        boundary_polygon = gpd.GeoDataFrame(geometry=[POLY], crs=\"EPSG:4326\").to_crs(\"EPSG:3857\").geometry.iloc[0]\n",
    "    \n",
    "    outside_mask = ~buildings.geometry.within(boundary_polygon)\n",
    "    if outside_mask.any():\n",
    "        buildings = gpd.clip(buildings, gpd.GeoDataFrame(geometry=[boundary_polygon], crs=\"EPSG:3857\"))\n",
    "    buildings = nm.remove_overlaps(buildings).reset_index(drop=True)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    streets = nm.download_osm_streets(\n",
    "        POLY,\n",
    "        crs=\"EPSG:3857\",\n",
    "        clip=True,\n",
    "        explode=True,\n",
    "        graphml_path=OUTPUT_DIR / \"streets_consolidated.graphml\",\n",
    "    )\n",
    "    download_streets_time = time.time() - t1\n",
    "    print(f\"Streets download:   {download_streets_time:>6.2f}s ({len(streets):,} streets)\")\n",
    "    \n",
    "    streets = streets.reset_index(drop=True)\n",
    "    \n",
    "    t2 = time.time()\n",
    "    rotated_streets, rotation_deg = nm.rotate_streets_to_align(streets, k=200)\n",
    "    rotation_time = time.time() - t2\n",
    "    print(f\"Grid rotation:      {rotation_time:>6.2f}s ({rotation_deg:.2f}°)\")\n",
    "    \n",
    "    # Get rotation origin (centroid of original streets before rotation)\n",
    "    all_streets = streets.geometry.union_all()\n",
    "    rotation_origin = (all_streets.centroid.x, all_streets.centroid.y)\n",
    "    \n",
    "    rotated_buildings = nm.rotate(buildings, rotation_deg=rotation_deg, origin=rotation_origin)\n",
    "    rotated_boundary = nm.rotate(\n",
    "        gpd.GeoDataFrame(geometry=[boundary_polygon], crs=\"EPSG:3857\"),\n",
    "        rotation_deg=rotation_deg,\n",
    "        origin=rotation_origin\n",
    "    )\n",
    "    \n",
    "    if SANDBOX_GPKG.exists():\n",
    "        SANDBOX_GPKG.unlink()\n",
    "    \n",
    "    rotated_buildings.to_file(SANDBOX_GPKG, layer=\"buildings\", driver=\"GPKG\")\n",
    "    rotated_streets.to_file(SANDBOX_GPKG, layer=\"streets\", driver=\"GPKG\", mode=\"a\")\n",
    "    rotated_boundary.to_file(SANDBOX_GPKG, layer=\"boundary\", driver=\"GPKG\", mode=\"a\")\n",
    "    \n",
    "    # Store rotation_deg and rotation_origin in metadata JSON for later retrieval\n",
    "    rotation_metadata_path = OUTPUT_DIR / f\"rotation_metadata_{BOX_NAME}.json\"\n",
    "    with open(rotation_metadata_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'rotation_deg': rotation_deg,\n",
    "            'rotation_origin': rotation_origin\n",
    "        }, f)\n",
    "    \n",
    "    data_gen_time = download_buildings_time + download_streets_time + rotation_time\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Data generation:    {data_gen_time:>6.2f}s\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "else:\n",
    "    print(f\"Loading existing data from {SANDBOX_GPKG}\")\n",
    "    data_gen_time = 0.0\n",
    "\n",
    "buildings = gpd.read_file(SANDBOX_GPKG, layer=\"buildings\")\n",
    "streets = gpd.read_file(SANDBOX_GPKG, layer=\"streets\")\n",
    "boundary = gpd.read_file(SANDBOX_GPKG, layer=\"boundary\")\n",
    "\n",
    "# Load rotation_deg and rotation_origin from metadata if available\n",
    "rotation_metadata_path = OUTPUT_DIR / f\"rotation_metadata_{BOX_NAME}.json\"\n",
    "if rotation_metadata_path.exists():\n",
    "    with open(rotation_metadata_path, 'r') as f:\n",
    "        rotation_metadata = json.load(f)\n",
    "        rotation_deg = rotation_metadata.get('rotation_deg', 0.0)\n",
    "        rotation_origin = rotation_metadata.get('rotation_origin', None)\n",
    "else:\n",
    "    # Fallback: try to compute from streets (will be ~0 if already rotated)\n",
    "    _, rotation_deg = nm.rotate_streets_to_align(streets, k=200)\n",
    "    if abs(rotation_deg) < 0.1:\n",
    "        rotation_deg = 0.0\n",
    "    # Use boundary centroid as fallback rotation origin\n",
    "    rotation_origin = (boundary.geometry.iloc[0].centroid.x, boundary.geometry.iloc[0].centroid.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7953dba",
   "metadata": {},
   "source": [
    "## Rasterization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d294a739",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "RASTERIZATION PIPELINE\n",
      "==================================================\n",
      "Generated 104,202 blocks (in 4.44s)\n",
      "Assigning block types...\n",
      "Block types assigned (in 0.21s)\n",
      "Assigning streets...\n",
      "Verifying street connectivity...\n",
      "  Streets: 28,313 kept, 7 discarded (in 0.27s)\n",
      "Adding buildings to city...\n",
      "  Added 20036 buildings, skipped 23587 due to overlap (adding took 42.96s)\n",
      "City generation:     63.93s\n",
      "Street graph:         0.30s\n",
      "Hub network:          3.01s\n",
      "Gravity computation:   9.09s\n",
      "Shortest paths:       0.00s\n",
      "--------------------------------------------------\n",
      "Rasterization:       76.32s\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"RASTERIZATION PIPELINE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "t0 = time.time()\n",
    "city = RasterCity(\n",
    "    boundary.geometry.iloc[0],\n",
    "    streets,\n",
    "    buildings,\n",
    "    block_side_length=config[\"block_side_length\"],\n",
    "    resolve_overlaps=True,\n",
    "    other_building_behavior=\"filter\",\n",
    "    rotation_deg=rotation_deg,\n",
    "    rotation_origin=rotation_origin\n",
    ")\n",
    "gen_time = time.time() - t0\n",
    "print(f\"City generation:    {gen_time:>6.2f}s\")\n",
    "\n",
    "t1 = time.time()\n",
    "G = city.get_street_graph()\n",
    "graph_time = time.time() - t1\n",
    "print(f\"Street graph:       {graph_time:>6.2f}s\")\n",
    "\n",
    "t2 = time.time()\n",
    "city._build_hub_network(hub_size=config[\"hub_size\"])\n",
    "hub_time = time.time() - t2\n",
    "print(f\"Hub network:        {hub_time:>6.2f}s\")\n",
    "\n",
    "t3 = time.time()\n",
    "city.compute_gravity(exponent=2.0, callable_only=True)\n",
    "grav_time = time.time() - t3\n",
    "print(f\"Gravity computation: {grav_time:>6.2f}s\")\n",
    "\n",
    "t4 = time.time()\n",
    "city.compute_shortest_paths(callable_only=True)\n",
    "paths_time = time.time() - t4\n",
    "print(f\"Shortest paths:     {paths_time:>6.2f}s\")\n",
    "\n",
    "raster_time = gen_time + graph_time + hub_time + grav_time + paths_time\n",
    "print(\"-\"*50)\n",
    "print(f\"Rasterization:      {raster_time:>6.2f}s\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if data_gen_time > 0:\n",
    "    total_time = data_gen_time + raster_time\n",
    "    print(f\"\\nTotal (with data):  {total_time:>6.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf59b2ac",
   "metadata": {},
   "source": [
    "## Summary: City Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f620081f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "         Component     Count/Shape Memory (MB)\n",
      "            Blocks         104,202        16.6\n",
      "           Streets          28,313         3.6\n",
      "         Buildings          20,036         6.1\n",
      "       Graph Nodes          28,313         4.6\n",
      "       Graph Edges          31,384           -\n",
      "       Hub Network         100×100         0.1\n",
      "          Hub Info         20036×2         1.4\n",
      "      Nearby Doors 1,021,774 pairs         0.0\n",
      "Gravity (callable)        function        <0.1\n",
      "building_type\n",
      "home         19380\n",
      "retail         439\n",
      "workplace      123\n",
      "park            94\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def get_size_mb(obj):\n",
    "    if isinstance(obj, (pd.DataFrame, gpd.GeoDataFrame)):\n",
    "        return obj.memory_usage(deep=True).sum() / 1024**2\n",
    "    elif hasattr(obj, 'nodes') and hasattr(obj, 'edges'):\n",
    "        return (len(obj.nodes) * 64 + len(obj.edges) * 96) / 1024**2\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Component': ['Blocks', 'Streets', 'Buildings', 'Graph Nodes', 'Graph Edges', 'Hub Network', 'Hub Info', 'Nearby Doors', 'Gravity (callable)'],\n",
    "    'Count/Shape': [\n",
    "        f\"{len(city.blocks_gdf):,}\",\n",
    "        f\"{len(city.streets_gdf):,}\",\n",
    "        f\"{len(city.buildings_gdf):,}\",\n",
    "        f\"{len(G.nodes):,}\",\n",
    "        f\"{len(G.edges):,}\",\n",
    "        f\"{city.hub_df.shape[0]}×{city.hub_df.shape[1]}\",\n",
    "        f\"{city.grav_hub_info.shape[0]}×{city.grav_hub_info.shape[1]}\",\n",
    "        f\"{len(city.mh_dist_nearby_doors):,} pairs\",\n",
    "        \"function\"\n",
    "    ],\n",
    "    'Memory (MB)': [\n",
    "        f\"{get_size_mb(city.blocks_gdf):.1f}\",\n",
    "        f\"{get_size_mb(city.streets_gdf):.1f}\",\n",
    "        f\"{get_size_mb(city.buildings_gdf):.1f}\",\n",
    "        f\"{get_size_mb(G):.1f}\",\n",
    "        \"-\",\n",
    "        f\"{get_size_mb(city.hub_df):.1f}\",\n",
    "        f\"{get_size_mb(city.grav_hub_info):.1f}\",\n",
    "        f\"{get_size_mb(city.mh_dist_nearby_doors):.1f}\",\n",
    "        \"<0.1\"\n",
    "    ]\n",
    "})\n",
    "print(\"\\n\" + summary_df.to_string(index=False))\n",
    "print(city.buildings_gdf.building_type.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286b2970",
   "metadata": {},
   "source": [
    "## Generate Population and Destination Diaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc82034a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DESTINATION DIARY GENERATION\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▋                                                                                | 4/200 [01:02<51:26, 15.75s/it]"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DESTINATION DIARY GENERATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "config_path = OUTPUT_DIR / f\"config_{BOX_NAME}.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "population = Population(city)\n",
    "population.generate_agents(\n",
    "    N=config[\"N\"],\n",
    "    seed=config[\"name_seed\"],\n",
    "    name_count=config[\"name_count\"],\n",
    "    datetimes=config[\"epr_params\"][\"datetime\"]\n",
    ")\n",
    "\n",
    "end_time = pd.Timestamp(config[\"epr_params\"][\"end_time\"])\n",
    "\n",
    "t1 = time.time()\n",
    "for i, agent in tqdm(enumerate(population.roster.values()), total=config[\"N\"]):\n",
    "    agent.generate_dest_diary(\n",
    "        end_time=end_time,\n",
    "        epr_time_res=config[\"epr_params\"][\"epr_time_res\"],\n",
    "        rho=config[\"epr_params\"][\"rho\"],\n",
    "        gamma=config[\"epr_params\"][\"gamma\"],\n",
    "        seed=config[\"epr_params\"][\"seed_base\"] + i\n",
    "    )\n",
    "\n",
    "diary_gen_time = time.time() - t1\n",
    "print(f\"Diary generation:   {diary_gen_time:>6.2f}s\")\n",
    "\n",
    "total_entries = sum(len(agent.destination_diary) for agent in population.roster.values())\n",
    "print(f\"Total entries:      {total_entries:,}\")\n",
    "\n",
    "dest_diaries_path = OUTPUT_DIR / f\"dest_diaries_{BOX_NAME}\"\n",
    "t2 = time.time()\n",
    "population.save_pop(\n",
    "    dest_diaries_path=dest_diaries_path,\n",
    "    partition_cols=[\"date\"],\n",
    "    fmt='parquet',\n",
    "    traj_cols={'geohash': 'location'}\n",
    ")\n",
    "persist_time = time.time() - t2\n",
    "print(f\"Persistence:        {persist_time:>6.2f}s\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Total EPR:          {diary_gen_time:>6.2f}s\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nConfig saved to {config_path}\")\n",
    "print(f\"Destination diaries saved to {dest_diaries_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5583fb57",
   "metadata": {},
   "source": [
    "## Generate Full Trajectories from Destination Diaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5b42b4-93a6-4e90-9b08-6065418a56dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAJECTORY GENERATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "t1 = time.time()\n",
    "failed_agents = []\n",
    "for i, agent in tqdm(enumerate(population.roster.values()), total=config[\"N\"], desc=\"Generating trajectories\"):\n",
    "    try:\n",
    "        agent.generate_trajectory(\n",
    "            dt=config[\"traj_params\"][\"dt\"],\n",
    "            seed=config[\"traj_params\"][\"seed_base\"] + i\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        failed_agents.append((agent.identifier, str(e)))\n",
    "        continue\n",
    "\n",
    "traj_gen_time = time.time() - t1\n",
    "print(f\"Trajectory generation: {traj_gen_time:>6.2f}s\")\n",
    "if failed_agents:\n",
    "    print(f\"Warning: {len(failed_agents)} agents failed trajectory generation\")\n",
    "\n",
    "total_points = sum(len(agent.trajectory) for agent in population.roster.values() if agent.trajectory is not None)\n",
    "print(f\"Total trajectory points: {total_points:,}\")\n",
    "print(f\"Points per second: {total_points/traj_gen_time:.1f}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Total trajectory:   {traj_gen_time:>6.2f}s\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a7e06",
   "metadata": {},
   "source": [
    "## Sample Sparse Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800fdb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SPARSE TRAJECTORY SAMPLING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "t1 = time.time()\n",
    "for i, agent in tqdm(enumerate(population.roster.values()), total=config[\"N\"], desc=\"Sampling trajectories\"):\n",
    "    if agent.trajectory is None:\n",
    "        continue\n",
    "    agent.sample_trajectory(\n",
    "        beta_ping=config[\"sampling_params\"][\"beta_ping\"],\n",
    "        beta_durations=config[\"sampling_params\"][\"beta_durations\"],\n",
    "        beta_start=config[\"sampling_params\"][\"beta_start\"],\n",
    "        ha=config[\"sampling_params\"][\"ha\"],\n",
    "        seed=config[\"sampling_params\"][\"seed_base\"] + i,\n",
    "        replace_sparse_traj=True\n",
    "    )\n",
    "\n",
    "sampling_time = time.time() - t1\n",
    "print(f\"Sparse sampling:    {sampling_time:>6.2f}s\")\n",
    "\n",
    "total_sparse_points = sum(len(agent.sparse_traj) for agent in population.roster.values() if agent.sparse_traj is not None)\n",
    "print(f\"Total sparse points: {total_sparse_points:,}\")\n",
    "print(f\"Sparsity ratio: {total_sparse_points/total_points:.2%}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Total sampling:     {sampling_time:>6.2f}s\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaed5f3",
   "metadata": {},
   "source": [
    "## Reproject to Mercator and Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd750880",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"REPROJECTION AND PERSISTENCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Build POI data for diary reprojection\n",
    "cent = city.buildings_gdf['door_point'] if 'door_point' in city.buildings_gdf.columns else city.buildings_gdf.geometry.centroid\n",
    "poi_data = pd.DataFrame({\n",
    "    'building_id': city.buildings_gdf['id'].values,\n",
    "    'x': (city.buildings_gdf['door_cell_x'].astype(float) + 0.5).values if 'door_cell_x' in city.buildings_gdf.columns else cent.x.values,\n",
    "    'y': (city.buildings_gdf['door_cell_y'].astype(float) + 0.5).values if 'door_cell_y' in city.buildings_gdf.columns else cent.y.values\n",
    "})\n",
    "\n",
    "print(\"Reprojecting sparse trajectories to Web Mercator...\")\n",
    "population.reproject_to_mercator(sparse_traj=True, full_traj=False, diaries=True, poi_data=poi_data)\n",
    "\n",
    "print(\"Saving sparse trajectories and diaries...\")\n",
    "population.save_pop(\n",
    "    sparse_path=OUTPUT_DIR / f\"sparse_traj_{BOX_NAME}\",\n",
    "    diaries_path=OUTPUT_DIR / f\"diaries_{BOX_NAME}\",\n",
    "    partition_cols=[\"date\"],\n",
    "    fmt='parquet'\n",
    ")\n",
    "print(\"-\"*50)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef0339",
   "metadata": {},
   "source": [
    "## Visualize Sparse Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n\" + \"=\"*50)\n",
    "# print(\"VISUALIZATION\")\n",
    "# print(\"=\"*50)\n",
    "\n",
    "# # Read sparse trajectories\n",
    "# sparse_traj_df = from_file(OUTPUT_DIR / f\"sparse_traj_{BOX_NAME}\", format=\"parquet\")\n",
    "# print(f\"Loaded {len(sparse_traj_df):,} sparse trajectory points for {config['N']} agents\")\n",
    "\n",
    "# # Plot with contextily basemap\n",
    "# fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# # Plot each agent with different color\n",
    "# for agent_id in sparse_traj_df['user_id'].unique():\n",
    "#     agent_traj = sparse_traj_df[sparse_traj_df['user_id'] == agent_id]\n",
    "#     ax.scatter(agent_traj['x'], agent_traj['y'], s=1, alpha=0.5, label=agent_id)\n",
    "\n",
    "# # Add basemap\n",
    "# cx.add_basemap(ax, crs=\"EPSG:3857\", source=cx.providers.CartoDB.Positron)\n",
    "\n",
    "# ax.set_xlabel('X (Web Mercator)')\n",
    "# ax.set_ylabel('Y (Web Mercator)')\n",
    "# ax.set_title(f'Sparse Trajectories - {config[\"N\"]} Agents, 7 Days')\n",
    "# ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', markerscale=10)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(OUTPUT_DIR / f\"sparse_trajectories_{BOX_NAME}.png\", dpi=150, bbox_inches='tight')\n",
    "# print(f\"Saved plot to {OUTPUT_DIR / f'sparse_trajectories_{BOX_NAME}.png'}\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f1ebb6-89bb-4a2a-aa42-340f45fe368c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
