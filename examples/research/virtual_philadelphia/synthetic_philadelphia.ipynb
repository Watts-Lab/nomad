{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf872541-c40e-4f6c-8c41-8b47835d551c",
   "metadata": {},
   "source": [
    "# Synthetic Philadelphia - Production Pipeline\n",
    "\n",
    "Full rasterization pipeline with EPR destination diary generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e13f0a6-9073-42b7-86ea-c8746c909d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import box\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as cx\n",
    "\n",
    "import nomad.map_utils as nm\n",
    "from nomad.city_gen import RasterCity\n",
    "from nomad.traj_gen import Population\n",
    "from nomad.io.base import from_file\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec81266",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d04e8b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "LARGE_BOX = box(-75.1905, 39.9235, -75.1425, 39.9535)\n",
    "MEDIUM_BOX = box(-75.1665, 39.9385, -75.1425, 39.9535)\n",
    "\n",
    "USE_FULL_CITY = False\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if USE_FULL_CITY:\n",
    "    BOX_NAME = \"full\"\n",
    "    POLY = \"Philadelphia, Pennsylvania, USA\"\n",
    "else:\n",
    "    BOX_NAME = \"large\"\n",
    "    POLY = LARGE_BOX\n",
    "\n",
    "SANDBOX_GPKG = OUTPUT_DIR / f\"spatial_data_{BOX_NAME}.gpkg\"\n",
    "REGENERATE_DATA = False  # Set to True to regenerate data with rotation metadata\n",
    "\n",
    "config = {\n",
    "    \"box_name\": BOX_NAME,\n",
    "    \"block_side_length\": 15.0,\n",
    "    \"hub_size\": 100,\n",
    "    \"N\": 200,\n",
    "    \"name_seed\": 42,\n",
    "    \"name_count\": 2,\n",
    "    \"epr_params\": {\n",
    "        \"datetime\": \"2025-05-23 00:00-05:00\",\n",
    "        \"end_time\": \"2025-07-01 00:00-05:00\",\n",
    "        \"epr_time_res\": 15,\n",
    "        \"rho\": 0.4,\n",
    "        \"gamma\": 0.3,\n",
    "        \"seed_base\": 100\n",
    "    },\n",
    "    \"traj_params\": {\n",
    "        \"dt\": 0.5,\n",
    "        \"seed_base\": 200\n",
    "    },\n",
    "    \"sampling_params\": {\n",
    "        \"beta_ping\": 7,\n",
    "        \"beta_start\": 300,\n",
    "        \"beta_durations\": 55,\n",
    "        \"ha\": 11.5/15,\n",
    "        \"seed_base\": 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be88c3",
   "metadata": {},
   "source": [
    "## Data Generation (OSM Download + Rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0185669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATA GENERATION\n",
      "==================================================\n",
      "Buildings download:  12.23s (53,816 buildings)\n",
      "Streets download:     6.06s (5,209 streets)\n",
      "Grid rotation:        0.28s (10.03°)\n",
      "--------------------------------------------------\n",
      "Data generation:     18.57s\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if REGENERATE_DATA or not SANDBOX_GPKG.exists():\n",
    "    print(\"=\"*50)\n",
    "    print(\"DATA GENERATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    buildings = nm.download_osm_buildings(\n",
    "        POLY,\n",
    "        crs=\"EPSG:3857\",\n",
    "        schema=\"garden_city\",\n",
    "        clip=True,\n",
    "        infer_building_types=True,\n",
    "        explode=True,\n",
    "    )\n",
    "    download_buildings_time = time.time() - t0\n",
    "    print(f\"Buildings download: {download_buildings_time:>6.2f}s ({len(buildings):,} buildings)\")\n",
    "    \n",
    "    if USE_FULL_CITY:\n",
    "        boundary_polygon = nm.get_city_boundary_osm(POLY, simplify=True)[0]\n",
    "        boundary_polygon = gpd.GeoSeries([boundary_polygon], crs=\"EPSG:4326\").to_crs(\"EPSG:3857\").iloc[0]\n",
    "    else:\n",
    "        boundary_polygon = gpd.GeoDataFrame(geometry=[POLY], crs=\"EPSG:4326\").to_crs(\"EPSG:3857\").geometry.iloc[0]\n",
    "    \n",
    "    outside_mask = ~buildings.geometry.within(boundary_polygon)\n",
    "    if outside_mask.any():\n",
    "        buildings = gpd.clip(buildings, gpd.GeoDataFrame(geometry=[boundary_polygon], crs=\"EPSG:3857\"))\n",
    "    buildings = nm.remove_overlaps(buildings).reset_index(drop=True)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    streets = nm.download_osm_streets(\n",
    "        POLY,\n",
    "        crs=\"EPSG:3857\",\n",
    "        clip=True,\n",
    "        explode=True,\n",
    "        graphml_path=OUTPUT_DIR / \"streets_consolidated.graphml\",\n",
    "    )\n",
    "    download_streets_time = time.time() - t1\n",
    "    print(f\"Streets download:   {download_streets_time:>6.2f}s ({len(streets):,} streets)\")\n",
    "    \n",
    "    streets = streets.reset_index(drop=True)\n",
    "    \n",
    "    t2 = time.time()\n",
    "    rotated_streets, rotation_deg = nm.rotate_streets_to_align(streets, k=200)\n",
    "    rotation_time = time.time() - t2\n",
    "    print(f\"Grid rotation:      {rotation_time:>6.2f}s ({rotation_deg:.2f}°)\")\n",
    "    \n",
    "    # Get rotation origin (centroid of original streets before rotation)\n",
    "    all_streets = streets.geometry.union_all()\n",
    "    rotation_origin = (all_streets.centroid.x, all_streets.centroid.y)\n",
    "    \n",
    "    rotated_buildings = nm.rotate(buildings, rotation_deg=rotation_deg, origin=rotation_origin)\n",
    "    rotated_boundary = nm.rotate(\n",
    "        gpd.GeoDataFrame(geometry=[boundary_polygon], crs=\"EPSG:3857\"),\n",
    "        rotation_deg=rotation_deg,\n",
    "        origin=rotation_origin\n",
    "    )\n",
    "    \n",
    "    if SANDBOX_GPKG.exists():\n",
    "        SANDBOX_GPKG.unlink()\n",
    "    \n",
    "    rotated_buildings.to_file(SANDBOX_GPKG, layer=\"buildings\", driver=\"GPKG\")\n",
    "    rotated_streets.to_file(SANDBOX_GPKG, layer=\"streets\", driver=\"GPKG\", mode=\"a\")\n",
    "    rotated_boundary.to_file(SANDBOX_GPKG, layer=\"boundary\", driver=\"GPKG\", mode=\"a\")\n",
    "    \n",
    "    # Store rotation_deg and rotation_origin in metadata JSON for later retrieval\n",
    "    rotation_metadata_path = OUTPUT_DIR / f\"rotation_metadata_{BOX_NAME}.json\"\n",
    "    with open(rotation_metadata_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'rotation_deg': rotation_deg,\n",
    "            'rotation_origin': rotation_origin\n",
    "        }, f)\n",
    "    \n",
    "    data_gen_time = download_buildings_time + download_streets_time + rotation_time\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Data generation:    {data_gen_time:>6.2f}s\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "else:\n",
    "    print(f\"Loading existing data from {SANDBOX_GPKG}\")\n",
    "    data_gen_time = 0.0\n",
    "\n",
    "buildings = gpd.read_file(SANDBOX_GPKG, layer=\"buildings\")\n",
    "streets = gpd.read_file(SANDBOX_GPKG, layer=\"streets\")\n",
    "boundary = gpd.read_file(SANDBOX_GPKG, layer=\"boundary\")\n",
    "\n",
    "# Load rotation_deg and rotation_origin from metadata if available\n",
    "rotation_metadata_path = OUTPUT_DIR / f\"rotation_metadata_{BOX_NAME}.json\"\n",
    "if rotation_metadata_path.exists():\n",
    "    with open(rotation_metadata_path, 'r') as f:\n",
    "        rotation_metadata = json.load(f)\n",
    "        rotation_deg = rotation_metadata.get('rotation_deg', 0.0)\n",
    "        rotation_origin = rotation_metadata.get('rotation_origin', None)\n",
    "else:\n",
    "    # Fallback: try to compute from streets (will be ~0 if already rotated)\n",
    "    _, rotation_deg = nm.rotate_streets_to_align(streets, k=200)\n",
    "    if abs(rotation_deg) < 0.1:\n",
    "        rotation_deg = 0.0\n",
    "    # Use boundary centroid as fallback rotation origin\n",
    "    rotation_origin = (boundary.geometry.iloc[0].centroid.x, boundary.geometry.iloc[0].centroid.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7953dba",
   "metadata": {},
   "source": [
    "## Rasterization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d294a739",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "RASTERIZATION PIPELINE\n",
      "==================================================\n",
      "Generated 104,189 blocks (in 4.72s)\n",
      "Assigning block types...\n",
      "Block types assigned (in 0.25s)\n",
      "Assigning streets...\n",
      "Verifying street connectivity...\n",
      "  Streets: 28,218 kept, 0 discarded (in 0.31s)\n",
      "Adding buildings to city...\n",
      "  Added 19894 buildings, skipped 23742 due to overlap (adding took 45.84s)\n",
      "City generation:     67.39s\n",
      "Street graph:         0.16s\n",
      "Hub network:          3.21s\n",
      "Gravity computation:  13.44s\n",
      "Shortest paths:       0.00s\n",
      "--------------------------------------------------\n",
      "Rasterization:       84.21s\n",
      "==================================================\n",
      "\n",
      "Total (with data):  102.78s\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"RASTERIZATION PIPELINE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "t0 = time.time()\n",
    "city = RasterCity(\n",
    "    boundary.geometry.iloc[0],\n",
    "    streets,\n",
    "    buildings,\n",
    "    block_side_length=config[\"block_side_length\"],\n",
    "    resolve_overlaps=True,\n",
    "    other_building_behavior=\"filter\",\n",
    "    rotation_deg=rotation_deg,\n",
    "    rotation_origin=rotation_origin\n",
    ")\n",
    "gen_time = time.time() - t0\n",
    "print(f\"City generation:    {gen_time:>6.2f}s\")\n",
    "\n",
    "t1 = time.time()\n",
    "G = city.get_street_graph()\n",
    "graph_time = time.time() - t1\n",
    "print(f\"Street graph:       {graph_time:>6.2f}s\")\n",
    "\n",
    "t2 = time.time()\n",
    "city._build_hub_network(hub_size=config[\"hub_size\"])\n",
    "hub_time = time.time() - t2\n",
    "print(f\"Hub network:        {hub_time:>6.2f}s\")\n",
    "\n",
    "t3 = time.time()\n",
    "city.compute_gravity(exponent=2.0, callable_only=True)\n",
    "grav_time = time.time() - t3\n",
    "print(f\"Gravity computation: {grav_time:>6.2f}s\")\n",
    "\n",
    "t4 = time.time()\n",
    "city.compute_shortest_paths(callable_only=True)\n",
    "paths_time = time.time() - t4\n",
    "print(f\"Shortest paths:     {paths_time:>6.2f}s\")\n",
    "\n",
    "raster_time = gen_time + graph_time + hub_time + grav_time + paths_time\n",
    "print(\"-\"*50)\n",
    "print(f\"Rasterization:      {raster_time:>6.2f}s\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if data_gen_time > 0:\n",
    "    total_time = data_gen_time + raster_time\n",
    "    print(f\"\\nTotal (with data):  {total_time:>6.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf59b2ac",
   "metadata": {},
   "source": [
    "## Summary: City Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f620081f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "         Component     Count/Shape Memory (MB)\n",
      "            Blocks         104,189        16.6\n",
      "           Streets          28,218         3.6\n",
      "         Buildings          19,894         6.0\n",
      "       Graph Nodes          28,218         4.6\n",
      "       Graph Edges          31,320           -\n",
      "       Hub Network         100×100         0.1\n",
      "          Hub Info         19894×2         1.4\n",
      "      Nearby Doors 1,010,883 pairs         0.0\n",
      "Gravity (callable)        function        <0.1\n",
      "building_type\n",
      "home         19243\n",
      "retail         436\n",
      "workplace      123\n",
      "park            92\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def get_size_mb(obj):\n",
    "    if isinstance(obj, (pd.DataFrame, gpd.GeoDataFrame)):\n",
    "        return obj.memory_usage(deep=True).sum() / 1024**2\n",
    "    elif hasattr(obj, 'nodes') and hasattr(obj, 'edges'):\n",
    "        return (len(obj.nodes) * 64 + len(obj.edges) * 96) / 1024**2\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Component': ['Blocks', 'Streets', 'Buildings', 'Graph Nodes', 'Graph Edges', 'Hub Network', 'Hub Info', 'Nearby Doors', 'Gravity (callable)'],\n",
    "    'Count/Shape': [\n",
    "        f\"{len(city.blocks_gdf):,}\",\n",
    "        f\"{len(city.streets_gdf):,}\",\n",
    "        f\"{len(city.buildings_gdf):,}\",\n",
    "        f\"{len(G.nodes):,}\",\n",
    "        f\"{len(G.edges):,}\",\n",
    "        f\"{city.hub_df.shape[0]}×{city.hub_df.shape[1]}\",\n",
    "        f\"{city.grav_hub_info.shape[0]}×{city.grav_hub_info.shape[1]}\",\n",
    "        f\"{len(city.mh_dist_nearby_doors):,} pairs\",\n",
    "        \"function\"\n",
    "    ],\n",
    "    'Memory (MB)': [\n",
    "        f\"{get_size_mb(city.blocks_gdf):.1f}\",\n",
    "        f\"{get_size_mb(city.streets_gdf):.1f}\",\n",
    "        f\"{get_size_mb(city.buildings_gdf):.1f}\",\n",
    "        f\"{get_size_mb(G):.1f}\",\n",
    "        \"-\",\n",
    "        f\"{get_size_mb(city.hub_df):.1f}\",\n",
    "        f\"{get_size_mb(city.grav_hub_info):.1f}\",\n",
    "        f\"{get_size_mb(city.mh_dist_nearby_doors):.1f}\",\n",
    "        \"<0.1\"\n",
    "    ]\n",
    "})\n",
    "print(\"\\n\" + summary_df.to_string(index=False))\n",
    "print(city.buildings_gdf.building_type.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286b2970",
   "metadata": {},
   "source": [
    "## Generate Population and Destination Diaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc82034a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DESTINATION DIARY GENERATION\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                               | 2/200 [00:42<1:09:43, 21.13s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, agent \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(population\u001b[38;5;241m.\u001b[39mroster\u001b[38;5;241m.\u001b[39mvalues()), total\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m---> 21\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_dest_diary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepr_time_res\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepr_params\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepr_time_res\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepr_params\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrho\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepr_params\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgamma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepr_params\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed_base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m diary_gen_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t1\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDiary generation:   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiary_gen_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>6.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\Brain\\Code Development\\nomad\\nomad\\traj_gen.py:677\u001b[0m, in \u001b[0;36mAgent.generate_dest_diary\u001b[1;34m(self, end_time, epr_time_res, stay_probs, rho, gamma, seed, verbose)\u001b[0m\n\u001b[0;32m    675\u001b[0m curr_type \u001b[38;5;241m=\u001b[39m visit_freqs\u001b[38;5;241m.\u001b[39mloc[curr, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuilding_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m curr \u001b[38;5;129;01min\u001b[39;00m visit_freqs\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhome\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    676\u001b[0m allowed \u001b[38;5;241m=\u001b[39m allowed_buildings(start_time_local)\n\u001b[1;32m--> 677\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mvisit_freqs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisit_freqs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbuilding_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallowed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisit_freqs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfreq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    679\u001b[0m S \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;66;03m# probability of exploring\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1413\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_slice_axis(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   1412\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 1413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getbool_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;66;03m# an iterable multi-selection\u001b[39;00m\n\u001b[0;32m   1416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(labels, MultiIndex)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1210\u001b[0m, in \u001b[0;36m_LocationIndexer._getbool_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1208\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1209\u001b[0m key \u001b[38;5;241m=\u001b[39m check_bool_indexer(labels, key)\n\u001b[1;32m-> 1210\u001b[0m inds \u001b[38;5;241m=\u001b[39m \u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_take_with_is_copy(inds, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DESTINATION DIARY GENERATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "config_path = OUTPUT_DIR / f\"config_{BOX_NAME}.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "population = Population(city)\n",
    "population.generate_agents(\n",
    "    N=config[\"N\"],\n",
    "    seed=config[\"name_seed\"],\n",
    "    name_count=config[\"name_count\"],\n",
    "    datetimes=config[\"epr_params\"][\"datetime\"]\n",
    ")\n",
    "\n",
    "end_time = pd.Timestamp(config[\"epr_params\"][\"end_time\"])\n",
    "\n",
    "t1 = time.time()\n",
    "for i, agent in tqdm(enumerate(population.roster.values()), total=config[\"N\"]):\n",
    "    agent.generate_dest_diary(\n",
    "        end_time=end_time,\n",
    "        epr_time_res=config[\"epr_params\"][\"epr_time_res\"],\n",
    "        rho=config[\"epr_params\"][\"rho\"],\n",
    "        gamma=config[\"epr_params\"][\"gamma\"],\n",
    "        seed=config[\"epr_params\"][\"seed_base\"] + i\n",
    "    )\n",
    "\n",
    "diary_gen_time = time.time() - t1\n",
    "print(f\"Diary generation:   {diary_gen_time:>6.2f}s\")\n",
    "\n",
    "total_entries = sum(len(agent.destination_diary) for agent in population.roster.values())\n",
    "print(f\"Total entries:      {total_entries:,}\")\n",
    "\n",
    "dest_diaries_path = OUTPUT_DIR / f\"dest_diaries_{BOX_NAME}\"\n",
    "t2 = time.time()\n",
    "population.save_pop(\n",
    "    dest_diaries_path=dest_diaries_path,\n",
    "    partition_cols=[\"date\"],\n",
    "    fmt='parquet',\n",
    "    traj_cols={'geohash': 'location'}\n",
    ")\n",
    "persist_time = time.time() - t2\n",
    "print(f\"Persistence:        {persist_time:>6.2f}s\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Total EPR:          {diary_gen_time:>6.2f}s\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nConfig saved to {config_path}\")\n",
    "print(f\"Destination diaries saved to {dest_diaries_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5583fb57",
   "metadata": {},
   "source": [
    "## Generate Full Trajectories from Destination Diaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5b42b4-93a6-4e90-9b08-6065418a56dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAJECTORY GENERATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "t1 = time.time()\n",
    "failed_agents = []\n",
    "for i, agent in tqdm(enumerate(population.roster.values()), total=config[\"N\"], desc=\"Generating trajectories\"):\n",
    "    try:\n",
    "        agent.generate_trajectory(\n",
    "            dt=config[\"traj_params\"][\"dt\"],\n",
    "            seed=config[\"traj_params\"][\"seed_base\"] + i\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        failed_agents.append((agent.identifier, str(e)))\n",
    "        continue\n",
    "\n",
    "traj_gen_time = time.time() - t1\n",
    "print(f\"Trajectory generation: {traj_gen_time:>6.2f}s\")\n",
    "if failed_agents:\n",
    "    print(f\"Warning: {len(failed_agents)} agents failed trajectory generation\")\n",
    "\n",
    "total_points = sum(len(agent.trajectory) for agent in population.roster.values() if agent.trajectory is not None)\n",
    "print(f\"Total trajectory points: {total_points:,}\")\n",
    "print(f\"Points per second: {total_points/traj_gen_time:.1f}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Total trajectory:   {traj_gen_time:>6.2f}s\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a7e06",
   "metadata": {},
   "source": [
    "## Sample Sparse Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800fdb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SPARSE TRAJECTORY SAMPLING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "t1 = time.time()\n",
    "for i, agent in tqdm(enumerate(population.roster.values()), total=config[\"N\"], desc=\"Sampling trajectories\"):\n",
    "    if agent.trajectory is None:\n",
    "        continue\n",
    "    agent.sample_trajectory(\n",
    "        beta_ping=config[\"sampling_params\"][\"beta_ping\"],\n",
    "        beta_durations=config[\"sampling_params\"][\"beta_durations\"],\n",
    "        beta_start=config[\"sampling_params\"][\"beta_start\"],\n",
    "        ha=config[\"sampling_params\"][\"ha\"],\n",
    "        seed=config[\"sampling_params\"][\"seed_base\"] + i,\n",
    "        replace_sparse_traj=True\n",
    "    )\n",
    "\n",
    "sampling_time = time.time() - t1\n",
    "print(f\"Sparse sampling:    {sampling_time:>6.2f}s\")\n",
    "\n",
    "total_sparse_points = sum(len(agent.sparse_traj) for agent in population.roster.values() if agent.sparse_traj is not None)\n",
    "print(f\"Total sparse points: {total_sparse_points:,}\")\n",
    "print(f\"Sparsity ratio: {total_sparse_points/total_points:.2%}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Total sampling:     {sampling_time:>6.2f}s\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaed5f3",
   "metadata": {},
   "source": [
    "## Reproject to Mercator and Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd750880",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"REPROJECTION AND PERSISTENCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Build POI data for diary reprojection\n",
    "cent = city.buildings_gdf['door_point'] if 'door_point' in city.buildings_gdf.columns else city.buildings_gdf.geometry.centroid\n",
    "poi_data = pd.DataFrame({\n",
    "    'building_id': city.buildings_gdf['id'].values,\n",
    "    'x': (city.buildings_gdf['door_cell_x'].astype(float) + 0.5).values if 'door_cell_x' in city.buildings_gdf.columns else cent.x.values,\n",
    "    'y': (city.buildings_gdf['door_cell_y'].astype(float) + 0.5).values if 'door_cell_y' in city.buildings_gdf.columns else cent.y.values\n",
    "})\n",
    "\n",
    "print(\"Reprojecting sparse trajectories to Web Mercator...\")\n",
    "population.reproject_to_mercator(sparse_traj=True, full_traj=False, diaries=True, poi_data=poi_data)\n",
    "\n",
    "print(\"Saving sparse trajectories and diaries...\")\n",
    "population.save_pop(\n",
    "    sparse_path=OUTPUT_DIR / f\"sparse_traj_{BOX_NAME}\",\n",
    "    diaries_path=OUTPUT_DIR / f\"diaries_{BOX_NAME}\",\n",
    "    partition_cols=[\"date\"],\n",
    "    fmt='parquet'\n",
    ")\n",
    "print(\"-\"*50)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef0339",
   "metadata": {},
   "source": [
    "## Visualize Sparse Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aa736a-6c59-4937-932e-7546992be24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4efeef5-ba44-403e-a99f-8ed17d41648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_traj_df = from_file(OUTPUT_DIR / f\"sparse_traj_{BOX_NAME}\", format=\"parquet\")\n",
    "sparse_traj_df[\"date\"] = pd.to_datetime(sparse_traj_df[\"timestamp\"], unit='s').dt.date.astype(str)\n",
    "sparse_traj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5200d09-0325-418b-a25c-80becd4e1025",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(sparse_traj_df, preserve_index=False)\n",
    "ds.write_dataset(\n",
    "    table,\n",
    "    base_dir=\"output/device_level\",\n",
    "    format=\"parquet\",\n",
    "    partitioning=[\"date\"],\n",
    "    max_rows_per_group=1_000,\n",
    "    max_rows_per_file=2_000   # adjust so each date naturally spills to multiple files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4deb8-fc05-4873-94ad-052999067409",
   "metadata": {},
   "outputs": [],
   "source": [
    "diaries = from_file(OUTPUT_DIR / f\"diaries_{BOX_NAME}\", format=\"parquet\")\n",
    "diaries[\"date\"] = pd.to_datetime(diaries[\"timestamp\"], unit='s').dt.date.astype(str)\n",
    "diaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7746128-2b88-4bd6-b452-df330d8a1c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(diaries, preserve_index=False)\n",
    "ds.write_dataset(\n",
    "    table,\n",
    "    base_dir=\"output/travel_diaries/\",\n",
    "    format=\"parquet\",\n",
    "    partitioning=[\"date\"],\n",
    "    max_rows_per_group=1_000,\n",
    "    max_rows_per_file=2_000   # adjust so each date naturally spills to multiple files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n\" + \"=\"*50)\n",
    "# print(\"VISUALIZATION\")\n",
    "# print(\"=\"*50)\n",
    "\n",
    "# # Read sparse trajectories\n",
    "# sparse_traj_df = from_file(OUTPUT_DIR / f\"sparse_traj_{BOX_NAME}\", format=\"parquet\")\n",
    "# print(f\"Loaded {len(sparse_traj_df):,} sparse trajectory points for {config['N']} agents\")\n",
    "\n",
    "# # Plot with contextily basemap\n",
    "# fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# # Plot each agent with different color\n",
    "# for agent_id in sparse_traj_df['user_id'].unique():\n",
    "#     agent_traj = sparse_traj_df[sparse_traj_df['user_id'] == agent_id]\n",
    "#     ax.scatter(agent_traj['x'], agent_traj['y'], s=1, alpha=0.5, label=agent_id)\n",
    "\n",
    "# # Add basemap\n",
    "# cx.add_basemap(ax, crs=\"EPSG:3857\", source=cx.providers.CartoDB.Positron)\n",
    "\n",
    "# ax.set_xlabel('X (Web Mercator)')\n",
    "# ax.set_ylabel('Y (Web Mercator)')\n",
    "# ax.set_title(f'Sparse Trajectories - {config[\"N\"]} Agents, 7 Days')\n",
    "# ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', markerscale=10)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(OUTPUT_DIR / f\"sparse_trajectories_{BOX_NAME}.png\", dpi=150, bbox_inches='tight')\n",
    "# print(f\"Saved plot to {OUTPUT_DIR / f'sparse_trajectories_{BOX_NAME}.png'}\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f1ebb6-89bb-4a2a-aa42-340f45fe368c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
