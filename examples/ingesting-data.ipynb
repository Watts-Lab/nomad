{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18b18c53",
   "metadata": {
    "id": "460ff464-7812-41fb-bc5b-bc4f24e16499"
   },
   "source": [
    "# Loading Trajectory Data\n",
    "\n",
    "Mobility data comes in many formats: timestamps as unix integers or ISO strings (with timezones), \n",
    "coordinates in lat/lon or projected, files as single CSVs or partitioned directories.\n",
    "\n",
    "`nomad.io.from_file` handles these cases with a single function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3245095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import nomad.io.base as loader\n",
    "import nomad.data as data_folder\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(data_folder.__file__).parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b37e7",
   "metadata": {},
   "source": [
    "## Pandas vs nomad.io for partitioned data\n",
    "\n",
    "Partitioned directories (e.g., `date=2024-01-01/`, `date=2024-01-02/`, ...) require a loop with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a2a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = glob.glob(str(data_dir / \"partitioned_csv\" / \"*\" / \"*.csv\"))\n",
    "df_list = []\n",
    "for f in csv_files:\n",
    "    df_list.append(pd.read_csv(f))\n",
    "df_pandas = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "print(f\"Pandas: {len(df_pandas)} rows\")\n",
    "print(df_pandas.dtypes)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_pandas.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb4656",
   "metadata": {},
   "source": [
    "`nomad.io.from_file` handles partitioned directories in one line, plus automatic type casting and column mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48bf128",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_cols = {\"user_id\": \"user_id\",\n",
    "             \"latitude\": \"dev_lat\",\n",
    "             \"longitude\": \"dev_lon\",\n",
    "             \"datetime\": \"local_datetime\"}\n",
    "\n",
    "df = loader.from_file(data_dir / \"partitioned_csv\", format=\"csv\", traj_cols=traj_cols, parse_dates=True)\n",
    "print(f\"nomad.io: {len(df)} rows\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head(3))\n",
    "print(\"\\nNote: 'local_datetime' is now datetime64[ns], not object!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c187a2",
   "metadata": {},
   "source": [
    "The same pattern works for Parquet files, with the type casting and processing relying on passing to the functions which columns correspond to the default \"typical\" spatio-temporal column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca035ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_cols = {\"user_id\": \"uid\", \"timestamp\": \"timestamp\", \n",
    "             \"latitude\": \"latitude\", \"longitude\": \"longitude\", \"date\": \"date\"}\n",
    "\n",
    "df = loader.from_file(data_dir / \"partitioned_parquet\", format=\"parquet\", traj_cols=traj_cols, parse_dates=True)\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f4479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the default canonical columnn names\n",
    "from nomad.constants import DEFAULT_SCHEMA\n",
    "print(DEFAULT_SCHEMA.keys())"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
