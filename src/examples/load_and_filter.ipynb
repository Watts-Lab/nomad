{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dce96c8",
   "metadata": {},
   "source": [
    "# Example notebook to load and filter a raw data in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b92f25a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a2aec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"...\")))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e5559b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import core.daphmeIO as loader\n",
    "import core.filters as filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d86549",
   "metadata": {},
   "source": [
    "We load a sample of Gravy trajectory data for the Philadelphia area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b21b84c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "part_path = '../data/sample3/date=2024-01-07/aad4a23f7a90441aa0f55f06e5e4313d-0.parquet'\n",
    "part_path = \"s3://phl-pings/gravy_clean/date=2019-11-01/part-00007-a7eb387d-1b0c-4aa7-b6a1-47023f1940bd.c000.snappy.parquet\"\n",
    "\n",
    "traj_cols =  {\"user_id\":\"identifier\",\n",
    "              \"x\":\"x\",\n",
    "              \"y\":\"y\",\n",
    "              \"datetime\":\"local_timestamp\",\n",
    "              \"timestamp\":\"timestamp\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f791d2b6",
   "metadata": {},
   "source": [
    "### Get a sample of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc616658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "u_sample = loader.sample_users(part_path, format='parquet', frac_users=0.2, user_id='identifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d2b2b0",
   "metadata": {},
   "source": [
    "### Load data for users in u_sample for 3 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3048fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filepath = ['s3://phl-pings/gravy_clean/date=2019-11-01/',\n",
    "            's3://phl-pings/gravy_clean/date=2019-11-02/',\n",
    "            's3://phl-pings/gravy_clean/date=2019-11-03/',\n",
    "            's3://phl-pings/gravy_clean/date=2019-11-04/',\n",
    "            's3://phl-pings/gravy_clean/date=2019-11-05/',\n",
    "            's3://phl-pings/gravy_clean/date=2019-11-06/']\n",
    "\n",
    "data = loader.sample_from_file(part_path, users=u_sample, format='parquet', traj_cols=traj_cols, user_id = 'identifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d40c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['timestamp'] = data[traj_cols['datetime']].astype(int) // 10**9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f35fd",
   "metadata": {},
   "source": [
    "### Project coordinates to Web Mercator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bda4b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = filters.to_projection(data, x='x', y='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3effec07",
   "metadata": {},
   "source": [
    "### Compute the q-statistic for the users in this sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8cd8a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, Point\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DoubleType\n",
    "\n",
    "\n",
    "# user can pass latitude and longitude as kwargs, user can pass x and y, OR traj_cols (prioritizing latitude, longitude). \n",
    "def to_projection(df: pd.DataFrame,\n",
    "                  latitude: str,\n",
    "                  longitude: str,\n",
    "                  from_crs: str = \"EPSG:4326\",\n",
    "                  to_crs: str = \"EPSG:3857\",\n",
    "                  spark_session: SparkSession = None):\n",
    "    \"\"\"\n",
    "    Projects latitude and longitude columns from one CRS to another.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame containing latitude and longitude columns.\n",
    "    latitude : str\n",
    "        Name of the latitude column.\n",
    "    longitude : str\n",
    "        Name of the longitude column.\n",
    "    from_crs : str, optional\n",
    "        EPSG code for the original CRS (default is \"EPSG:4326\").\n",
    "    to_crs : str, optional\n",
    "        EPSG code for the target CRS (default is \"EPSG:3857\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with new 'x' and 'y' columns representing projected coordinates.\n",
    "    \"\"\"\n",
    "    if spark_session:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        if latitude not in df.columns or longitude not in df.columns:\n",
    "            raise ValueError(f\"Latitude or longitude columns '{latitude}', '{longitude}' not found in DataFrame.\")\n",
    "\n",
    "        proj_cols = _to_projection(df[latitude],\n",
    "                                   df[longitude],\n",
    "                                   from_crs,\n",
    "                                   to_crs)\n",
    "\n",
    "        df['x'] = proj_cols['x']\n",
    "        df['y'] = proj_cols['y']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _to_projection(lat_col,\n",
    "                   long_col,\n",
    "                   from_crs: str,\n",
    "                   to_crs: str):\n",
    "    \"\"\"\n",
    "    Helper function to project latitude/longitude columns to a new CRS.\n",
    "    \"\"\"\n",
    "    gdf = gpd.GeoSeries(gpd.points_from_xy(long_col, lat_col),\n",
    "                        crs=from_crs)\n",
    "    projected = gdf.to_crs(to_crs)\n",
    "    output = pd.DataFrame({'x': projected.x, 'y': projected.y})\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def filter_to_box(df: pd.DataFrame,\n",
    "                  latitude: str,\n",
    "                  longitude: str,\n",
    "                  polygon: Polygon,\n",
    "                  spark_session: SparkSession = None):\n",
    "    '''\n",
    "    Filters DataFrame to keep points within a specified polygon's bounds.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame with latitude and longitude columns.\n",
    "    polygon : shapely.geometry.Polygon\n",
    "        Polygon defining the area to retain points within.\n",
    "    latitude : str\n",
    "        Name of the latitude column.\n",
    "    longitude : str\n",
    "        Name of the longitude column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Filtered DataFrame with points inside the polygon's bounds.\n",
    "    '''\n",
    "    if spark_session:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        if not isinstance(polygon, Polygon):\n",
    "            raise TypeError(\"Polygon parameter must be a Shapely Polygon object.\")\n",
    "\n",
    "        if latitude not in df.columns or longitude not in df.columns:\n",
    "            raise ValueError(f\"Latitude or longitude columns '{latitude}', '{longitude}' not found in DataFrame.\")\n",
    "\n",
    "        min_x, min_y, max_x, max_y = polygon.bounds\n",
    "\n",
    "        # TO DO: handle different column names and/or defaults as in daphmeIO. i.e. traj_cols as parameter\n",
    "\n",
    "        return df[(df[longitude].between(min_y, max_y)) & (df[latitude].between(min_x, max_x))]\n",
    "\n",
    "\n",
    "def _filter_to_box_spark(df: pd.DataFrame,\n",
    "                         bounding_wkt: str,\n",
    "                         spark: SparkSession,\n",
    "                         longitude_col: str,\n",
    "                         latitude_col: str,\n",
    "                         id_col: str):\n",
    "    \"\"\"Filters a DataFrame based on whether geographical points\n",
    "    (defined by longitude and latitude) fall within a specified geometry.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The Spark DataFrame to be filtered. It should contain columns\n",
    "        corresponding to longitude and latitude values, as well as an id column.\n",
    "\n",
    "    bounding_wkt : str\n",
    "        The Well-Known Text (WKT) string representing the bounding geometry\n",
    "        within which points are tested for inclusion. The WKT should define\n",
    "        a polygon in the EPSG:4326 coordinate reference system.\n",
    "\n",
    "    spark : SparkSession\n",
    "        The active SparkSession instance used to execute Spark operations.\n",
    "\n",
    "    longitude_col : str, default \"longitude\"\n",
    "        The name of the column in 'df' containing longitude values. Longitude\n",
    "        values should be in the EPSG:4326 coordinate reference system.\n",
    "\n",
    "    latitude_col : str, default \"latitude\"\n",
    "        The name of the column in 'df' containing latitude values. Latitude\n",
    "        values should be in the EPSG:4326 coordinate reference system.\n",
    "\n",
    "    id_col : str, default \"id\"\n",
    "        The name of the column in 'df' containing user IDs.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    DataFrame\n",
    "        A new Spark DataFrame filtered to include only rows where the point\n",
    "        (longitude, latitude) falls within the specified geometric boundary\n",
    "        defined by 'bounding_wkt'. This DataFrame includes all original columns\n",
    "        from 'df' and an additional column 'in_geo' that is true if the point\n",
    "        falls within the specified geometric boundary and false otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.withColumn(\"coordinate\", F.expr(f\"ST_MakePoint({longitude_col}, {latitude_col})\"))\n",
    "    df.createOrReplaceTempView(\"temp_df\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "        WITH temp_df AS (\n",
    "            SELECT *,\n",
    "                   ST_Contains(ST_GeomFromWKT('{bounding_wkt}'), coordinate) AS in_geo\n",
    "            FROM temp_df\n",
    "        ),\n",
    "\n",
    "        UniqueIDs AS (\n",
    "            SELECT DISTINCT {id_col} \n",
    "            FROM temp_df\n",
    "            WHERE in_geo\n",
    "        )\n",
    "\n",
    "        SELECT t.*\n",
    "        FROM temp_df t\n",
    "        WHERE t.{id_col} IN (SELECT {id_col} FROM UniqueIDs)\n",
    "        \"\"\"\n",
    "\n",
    "    return spark.sql(query)\n",
    "\n",
    "\n",
    "def coarse_filter(df: pd.DataFrame):\n",
    "    pass\n",
    "\n",
    "\n",
    "def _filtered_users(df: pd.DataFrame,\n",
    "                    k: int,\n",
    "                    T0: str,\n",
    "                    T1: str,\n",
    "                    polygon: Polygon,\n",
    "                    user_col: str,\n",
    "                    timestamp_col: str,\n",
    "                    latitude_col: str,\n",
    "                    longitude_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Subsets to users who have at least k distinct days with pings in the polygon \n",
    "    within the timeframe T0 to T1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame containing user data with latitude, longitude, and timestamp.\n",
    "    k : int\n",
    "        Minimum number of distinct days with pings inside the polygon for the user to be retained.\n",
    "    T0 : str\n",
    "        Start of the timeframe (as a string, or datetime).\n",
    "    T1 : str\n",
    "        End of the timeframe (as a string, or datetime).\n",
    "    polygon : Polygon\n",
    "        The polygon to check whether pings are inside.\n",
    "    user_col : str\n",
    "        Name of the column containing user identifiers.\n",
    "    timestamp_col : str\n",
    "        Name of the column containing timestamps (as strings or datetime).\n",
    "    latitude : str\n",
    "        Name of the column containing latitude values.\n",
    "    longitude : str\n",
    "        Name of the column containing longitude values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        A Series containing the user IDs for users who have at \n",
    "        least k distinct days with pings inside the polygon.\n",
    "    \"\"\"\n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "    df_filtered = df[(df[timestamp_col] >= T0) & (df[timestamp_col] <= T1)]\n",
    "    df_filtered = _in_geo(df_filtered, latitude_col, longitude_col, polygon)\n",
    "    df_filtered['date'] = df_filtered[timestamp_col].dt.date\n",
    "\n",
    "    filtered_users = (\n",
    "        df_filtered[df_filtered['in_geo'] == 1]\n",
    "        .groupby(user_col)['date']\n",
    "        .nunique()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    filtered_users = filtered_users[filtered_users['date'] >= k][user_col]\n",
    "\n",
    "    return filtered_users\n",
    "\n",
    "\n",
    "def _in_geo(df: pd.DataFrame,\n",
    "            latitude_col: str,\n",
    "            longitude_col: str,\n",
    "            polygon: Polygon) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a new column to the DataFrame indicating whether points are \n",
    "    inside the polygon (1) or not (0).\n",
    "    \"\"\"\n",
    "\n",
    "    def _point_in_polygon(lat, lon):\n",
    "        point = Point(lat, lon)\n",
    "        return 1 if polygon.contains(point) else 0\n",
    "\n",
    "    df['in_geo'] = df.apply(lambda row: _point_in_polygon(row[latitude_col], row[longitude_col]), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def q_filter(df: pd.DataFrame,\n",
    "             qbar: float,\n",
    "             user_col: str,\n",
    "             timestamp_col: str):\n",
    "    \"\"\"\n",
    "    Computes the q statistic for each user as the proportion of unique hours with pings \n",
    "    over the total observed hours (last hour - first hour) and filters users where q > qbar.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        A DataFrame containing user IDs and timestamps.\n",
    "    user_col : str\n",
    "        The name of the column containing user IDs.\n",
    "    timestamp_col : str\n",
    "        The name of the column containing timestamps.\n",
    "    qbar : float\n",
    "        The threshold q value; users with q > qbar will be retained.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        A Series containing the user IDs for users whose q_stat > qbar.\n",
    "    \"\"\"\n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "\n",
    "    user_q_stats = df.groupby(user_col).apply(\n",
    "        lambda group: _compute_q_stat(group, timestamp_col)\n",
    "    ).reset_index(name='q_stat')\n",
    "\n",
    "    # Filter users where q > qbar\n",
    "    filtered_users = user_q_stats[user_q_stats['q_stat'] > qbar][user_col]\n",
    "\n",
    "    return filtered_users\n",
    "\n",
    "def q_stats(df: pd.DataFrame, user_id: str, timestamp: str):\n",
    "    \"\"\"\n",
    "    Computes the q statistic for each user as the proportion of unique hours with pings \n",
    "    over the total observed hours (last hour - first hour).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        A DataFrame containing user IDs and timestamps.\n",
    "    user_id : str\n",
    "        The name of the column containing user IDs.\n",
    "    timestamp_col : str\n",
    "        The name of the column containing timestamps.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing each user and their respective q_stat.\n",
    "    \"\"\"\n",
    "    datetime = pd.to_datetime(df[timestamp])\n",
    "\n",
    "    q_stats = df.groupby(user_id).apply(\n",
    "        lambda group: _compute_q_stat(group, datetime)\n",
    "    ).reset_index(name='q_stat')\n",
    "\n",
    "    return q_stats\n",
    "\n",
    "\n",
    "def _compute_q_stat(user, datetime):\n",
    "    date_hour = datetime.dt.to_period('h')\n",
    "    unique_hours = date_hour.nunique()\n",
    "\n",
    "    # Calculate total observed hours (difference between last and first hour)\n",
    "    first_hour = date_hour.min()\n",
    "    last_hour = date_hour.max()\n",
    "    # maybe it should be + 1 hour\n",
    "    total_hours = (last_hour - first_hour).total_seconds() / 3600\n",
    "\n",
    "    # Compute q as the proportion of unique hours to total hours\n",
    "    q_stat = unique_hours / total_hours if total_hours > 0 else 0\n",
    "    if q_stat>1:\n",
    "        pdb.set_trace()\n",
    "    return q_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352dae5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q = q_stats(data, user_id= traj_cols['user_id'], timestamp= traj_cols['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e1382e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the histogram\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.hist(q_stats['q_stat'], bins=20, edgecolor='black')\n",
    "plt.xlabel('Q Statistic')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.03.10.15"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "conda_py_310_env"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      },
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "conda_py_310_env"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    },
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 5,
           "op": "addrange",
           "valuelist": "6"
          },
          {
           "key": 5,
           "length": 2,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3 (ipykernel)"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      },
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "python3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    },
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
