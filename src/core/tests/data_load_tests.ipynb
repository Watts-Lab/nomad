{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.fs as fs\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.dataset as ds\n",
    "import s3fs as fs3\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import daphmeIO\n",
    "import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "import warnings\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test formats and folder structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import partitioned dataset (data 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_names = pd.read_csv('../../data/sample2/sample2.csv', nrows=0).columns\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = ds.dataset(path, format=\"parquet\", partitioning=\"hive\")\n",
    "df = dataset.to_table().to_pandas()\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions that check integrity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kind of based on scikit-mobility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _update_schema(original, new_labels):\n",
    "    updated_schema = dict(original)\n",
    "    for label in new_labels:\n",
    "        if label in constants.DEFAULT_SCHEMA:\n",
    "            updated_schema[label] = new_labels[label]\n",
    "    return updated_schema\n",
    "\n",
    "def _has_traj_cols(df, traj_cols):\n",
    "    \n",
    "    # Check for sufficient spatial columns\n",
    "    spatial_exists = (\n",
    "        ('latitude' in traj_cols and 'longitude' in traj_cols and \n",
    "         traj_cols['latitude'] in df and traj_cols['longitude'] in df) or\n",
    "        ('x' in traj_cols and 'y' in traj_cols and \n",
    "         traj_cols['x'] in df and traj_cols['y'] in df) or\n",
    "        ('geohash' in traj_cols and traj_cols['geohash'] in df)\n",
    "    )\n",
    "    \n",
    "    # Check for sufficient temporal columns\n",
    "    temporal_exists = (\n",
    "        ('datetime' in traj_cols and traj_cols['datetime'] in df) or\n",
    "        ('timestamp' in traj_cols and traj_cols['timestamp'] in df)\n",
    "    )\n",
    "    \n",
    "    if not spatial_exists:\n",
    "        raise ValueError(\n",
    "            \"Could not find required spatial columns in {}. The dataframe columns must contain or map to at least one of the following sets: \"\n",
    "            \"('latitude', 'longitude'), ('x', 'y'), or 'geohash'.\".format(df.columns.tolist())\n",
    "        )\n",
    "        \n",
    "    if not temporal_exists:\n",
    "        raise ValueError(\n",
    "            \"Could not find required temporal columns in {}. The dataframe columns must contain or map to either 'datetime' or 'timestamp'.\".format(df.columns.tolist())\n",
    "        )\n",
    "    \n",
    "    return spatial_exists and temporal_exists\n",
    "\n",
    "\n",
    "def _cast_traj_cols(df, traj_cols):\n",
    "    if 'datetime' in traj_cols and traj_cols['datetime'] in df:\n",
    "        if not pd.core.dtypes.common.is_datetime64_any_dtype(df[traj_cols['datetime']].dtype):\n",
    "            df[traj_cols['datetime']] = pd.to_datetime(df[traj_cols['datetime']])\n",
    "    if 'timestamp' in traj_cols and traj_cols['timestamp'] in df:\n",
    "        # Coerce to integer if it's not already\n",
    "        if not pd.core.dtypes.common.is_integer_dtype(df[traj_cols['timestamp']].dtype):\n",
    "            df[traj_cols['timestamp']] = df[traj_cols['timestamp']].astype(int)\n",
    "\n",
    "    float_cols = ['latitude', 'longitude', 'x', 'y']\n",
    "    for col in float_cols:\n",
    "        if col in traj_cols and traj_cols[col] in df:\n",
    "            if not pd.core.dtypes.common.is_float_dtype(df[traj_cols[col]].dtype):\n",
    "                df[traj_cols[col]] = df[traj_cols[col]].astype(\"float\")\n",
    "\n",
    "    string_cols = ['user_id', 'geohash']\n",
    "    for col in string_cols:\n",
    "        if col in traj_cols and traj_cols[col] in df:\n",
    "            if not pd.core.dtypes.common.is_string_dtype(df[traj_cols[col]].dtype):\n",
    "                df[traj_cols[col]] = df[traj_cols[col]].astype(\"str\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def _is_traj_df(df, traj_cols = None, **kwargs):\n",
    "    \n",
    "    if not (isinstance(df, pd.DataFrame) or isinstance(df, gpd.GeoDataFrame)):\n",
    "        return False\n",
    "    \n",
    "    if not traj_cols:\n",
    "        traj_cols = {}\n",
    "        traj_cols = _update_schema(traj_cols, kwargs) #kwargs ignored if traj_cols\n",
    "        \n",
    "    traj_cols = _update_schema(constants.DEFAULT_SCHEMA, traj_cols)\n",
    "    \n",
    "    if not _has_traj_cols(df, traj_cols):\n",
    "        return False\n",
    "    \n",
    "    if 'datetime' in traj_cols and traj_cols['datetime'] in df:\n",
    "        if not pd.core.dtypes.common.is_datetime64_any_dtype(df[traj_cols['datetime']].dtype):\n",
    "            return False\n",
    "    elif 'timestamp' in traj_cols and traj_cols['timestamp'] in df:\n",
    "        if not pd.core.dtypes.common.is_integer_dtype(df[traj_cols['timestamp']].dtype):\n",
    "            return False\n",
    "\n",
    "    float_cols = ['latitude', 'longitude', 'x', 'y']\n",
    "    for col in float_cols:\n",
    "        if col in traj_cols and traj_cols[col] in df:\n",
    "            if not pd.core.dtypes.common.is_float_dtype(df[traj_cols[col]].dtype):\n",
    "                return False\n",
    "\n",
    "    string_cols = ['user_id', 'geohash']\n",
    "    for col in string_cols:\n",
    "        if col in traj_cols and traj_cols[col] in df:\n",
    "            if not pd.core.dtypes.common.is_string_dtype(df[traj_cols[col]].dtype):\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def from_object(df, traj_cols = None, spark_enabled=False, **kwargs):\n",
    "\n",
    "    if not (isinstance(df, pd.DataFrame) or isinstance(df, gpd.GeoDataFrame)):\n",
    "        raise TypeError(\n",
    "            \"Expected the data argument to be either a pandas DataFrame or a GeoPandas GeoDataFrame.\"\n",
    "        )\n",
    "    \n",
    "    # valid trajectory column names passed to **kwargs collected\n",
    "    if not traj_cols:\n",
    "        traj_cols = {}\n",
    "        traj_cols = _update_schema(traj_cols, kwargs) #kwargs ignored if traj_cols\n",
    "            \n",
    "    for key, value in traj_cols.items():\n",
    "        if value not in df:\n",
    "            warnings.warn(f\"Trajectory column '{value}' specified for '{key}' not found in df.\")\n",
    "            \n",
    "    # include defaults when missing\n",
    "    traj_cols = _update_schema(constants.DEFAULT_SCHEMA, traj_cols)\n",
    "    \n",
    "    if _has_traj_cols(df, traj_cols):\n",
    "        return _cast_traj_cols(df, traj_cols)\n",
    "\n",
    "\n",
    "def from_file(filepath, format=\"csv\", traj_cols=None, **kwargs):\n",
    "    assert format in [\"csv\", \"parquet\"]\n",
    "    \n",
    "    if format == 'parquet':\n",
    "        dataset = ds.dataset(filepath, format=\"parquet\", partitioning=\"hive\")\n",
    "        df = dataset.to_table().to_pandas()\n",
    "        return from_object(df, traj_cols=traj_cols, **kwargs)\n",
    "\n",
    "    elif format == 'csv':\n",
    "        if os.path.isdir(filepath) or isinstance(filepath, list):\n",
    "            dataset = ds.dataset(filepath, format=\"csv\", partitioning=\"hive\")\n",
    "            df = dataset.to_table().to_pandas()\n",
    "            return from_object(df, traj_cols=traj_cols)\n",
    "        else:\n",
    "            df = pd.read_csv(filepath)\n",
    "            return from_object(df, traj_cols=traj_cols)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def sample_users(filepath, format='csv', frac_users=1.0, traj_cols=None, **kwargs):\n",
    "    \n",
    "    assert format in ['csv', 'parquet']\n",
    "\n",
    "    if not traj_cols:\n",
    "        traj_cols = {}\n",
    "        traj_cols = _update_schema(traj_cols, kwargs)\n",
    "        \n",
    "    uid_col = traj_cols['user_id']\n",
    "\n",
    "    if format == 'parquet':\n",
    "        dataset = ds.dataset(filepath, format=\"parquet\", partitioning=\"hive\")\n",
    "        if uid_col not in dataset.schema.names:\n",
    "            raise ValueError(\n",
    "                \"Could not find required user ID column in {}. The columns must contain or map to '{}'.\".format(\n",
    "                    dataset.schema.names, uid_col)\n",
    "            )\n",
    "        user_ids = pc.unique(dataset.to_table(columns=[uid_col])[uid_col]).to_pandas()\n",
    "\n",
    "    else:\n",
    "        if os.path.isdir(filepath):\n",
    "            dataset = ds.dataset(filepath, format=\"csv\", partitioning=\"hive\")\n",
    "            if uid_col not in dataset.schema.names:\n",
    "                raise ValueError(\n",
    "                    \"Could not find required user ID column in {}. The columns must contain or map to '{}'.\".format(\n",
    "                        dataset.schema.names, uid_col)\n",
    "                )\n",
    "            user_ids = pc.unique(dataset.to_table(columns=[uid_col])[uid_col]).to_pandas()\n",
    "        else:\n",
    "            df = pd.read_csv(filepath, usecols=[uid_col])\n",
    "            if uid_col not in df.columns:\n",
    "                raise ValueError(\n",
    "                    \"Could not find required user ID column in {}. The columns must contain or map to '{}'.\".format(\n",
    "                        df.columns.tolist(), uid_col)\n",
    "                )\n",
    "            user_ids = df[uid_col].unique()\n",
    "\n",
    "    return user_ids.sample(frac=frac_users) if frac_users < 1.0 else user_ids\n",
    "\n",
    "\n",
    "\n",
    "def sample_from_file(filepath, users, format=\"csv\", traj_cols=None, **kwargs):\n",
    "    assert format in [\"csv\", \"parquet\"]\n",
    "    \n",
    "    if not traj_cols:\n",
    "        traj_cols = {}\n",
    "        traj_cols = _update_schema(traj_cols, kwargs)\n",
    "        \n",
    "    uid_col = traj_cols['user_id']\n",
    "\n",
    "    if format == 'parquet':\n",
    "        dataset = ds.dataset(filepath, format=\"parquet\", partitioning=\"hive\")\n",
    "        if uid_col not in dataset.schema.names:\n",
    "            raise ValueError(\n",
    "                \"Could not find required user ID column in {}. The columns must contain or map to '{}'.\".format(\n",
    "                    dataset.schema.names, uid_col)\n",
    "            )\n",
    "        df = dataset.to_table(\n",
    "            filter=ds.field(uid_col).isin(list(users))\n",
    "        ).to_pandas()\n",
    "    \n",
    "    elif format == 'csv':\n",
    "        if os.path.isdir(filepath) or isinstance(filepath, list):\n",
    "            dataset = ds.dataset(filepath, format=\"csv\", partitioning=\"hive\")\n",
    "            if uid_col not in dataset.schema.names:\n",
    "                raise ValueError(\n",
    "                    \"Could not find required user ID column in {}. The columns must contain or map to '{}'.\".format(\n",
    "                        dataset.schema.names, uid_col)\n",
    "                )\n",
    "            df = dataset.to_table(\n",
    "                filter=ds.field(uid_col).isin(list(users))\n",
    "            ).to_pandas()\n",
    "        else:\n",
    "            df = pd.read_csv(filepath)\n",
    "            if uid_col not in df.columns:\n",
    "                raise ValueError(\n",
    "                    \"Could not find required user ID column in {}. The columns must contain or map to '{}'.\".format(\n",
    "                        df.columns.tolist(), uid_col)\n",
    "                )\n",
    "            df = df[df[uid_col].isin(users)]\n",
    "    \n",
    "    return from_object(df, traj_cols=traj_cols, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/sample1/'\n",
    "df = from_file(path, format='parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_sample = sample_users(path, format='parquet', frac_users=0.1, user_id='uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wonderful_swirles</td>\n",
       "      <td>1704121560</td>\n",
       "      <td>38.321017</td>\n",
       "      <td>-36.667869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful_swirles</td>\n",
       "      <td>1704178440</td>\n",
       "      <td>38.320851</td>\n",
       "      <td>-36.667484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wonderful_swirles</td>\n",
       "      <td>1704178800</td>\n",
       "      <td>38.320852</td>\n",
       "      <td>-36.667470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wonderful_swirles</td>\n",
       "      <td>1704179820</td>\n",
       "      <td>38.320832</td>\n",
       "      <td>-36.667579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wonderful_swirles</td>\n",
       "      <td>1704180960</td>\n",
       "      <td>38.320834</td>\n",
       "      <td>-36.667461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>happy_feynman</td>\n",
       "      <td>1705183920</td>\n",
       "      <td>38.320609</td>\n",
       "      <td>-36.666785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>happy_feynman</td>\n",
       "      <td>1705249440</td>\n",
       "      <td>38.321699</td>\n",
       "      <td>-36.667564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>happy_feynman</td>\n",
       "      <td>1705249500</td>\n",
       "      <td>38.321702</td>\n",
       "      <td>-36.667532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>happy_feynman</td>\n",
       "      <td>1705250880</td>\n",
       "      <td>38.321722</td>\n",
       "      <td>-36.667483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>happy_feynman</td>\n",
       "      <td>1705276260</td>\n",
       "      <td>38.321415</td>\n",
       "      <td>-36.666953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1653 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   uid   timestamp   latitude  longitude\n",
       "0    wonderful_swirles  1704121560  38.321017 -36.667869\n",
       "1    wonderful_swirles  1704178440  38.320851 -36.667484\n",
       "2    wonderful_swirles  1704178800  38.320852 -36.667470\n",
       "3    wonderful_swirles  1704179820  38.320832 -36.667579\n",
       "4    wonderful_swirles  1704180960  38.320834 -36.667461\n",
       "..                 ...         ...        ...        ...\n",
       "176      happy_feynman  1705183920  38.320609 -36.666785\n",
       "177      happy_feynman  1705249440  38.321699 -36.667564\n",
       "178      happy_feynman  1705249500  38.321702 -36.667532\n",
       "179      happy_feynman  1705250880  38.321722 -36.667483\n",
       "180      happy_feynman  1705276260  38.321415 -36.666953\n",
       "\n",
       "[1653 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_from_file(path, users = u_sample, format='parquet', user_id='uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame([[1, 39.984094, 116.319236, '2008-10-23 13:53:05'],\n",
    " [1, 39.984198, 116.319322, '2008-10-23 13:53:06'],\n",
    " [1, 39.984224, 116.319402, '2008-10-23 13:53:11'],\n",
    " [1, 39.984211, 116.319389, '2008-10-23 13:53:16']], columns = ['uid', 'latitude', 'longitude', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "traj_cols = {'user_id':'uid',\n",
    "         'latitude':'latitude',\n",
    "         'longitude':'longitude',\n",
    "            'datetime':'time'}\n",
    "df = from_object(data, traj_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_is_traj_df(df, traj_cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
