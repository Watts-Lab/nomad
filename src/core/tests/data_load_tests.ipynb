{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.fs as fs\n",
    "import pyarrow.dataset as ds\n",
    "import s3fs as fs3\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import daphmeIO\n",
    "import constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test formats and folder structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import partitioned dataset (data 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_names = pd.read_csv('../../data/sample2/sample2.csv', nrows=0).columns\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = ds.dataset(path, format=\"parquet\", partitioning=\"hive\")\n",
    "df = dataset.to_table().to_pandas()\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions that check integrity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kind of based on scikit-mobility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "# from . import constants\n",
    "import constants\n",
    "\n",
    "def get_pq_users(path: str, id_string: str):\n",
    "    return pq.read_table(path, columns=[id_string]).column(id_string).unique().to_pandas()\n",
    "\n",
    "def get_pq_user_data(path: str, users: list[str], id_string: str):\n",
    "    return pq.read_table(path, filters=[(id_string, 'in', users)]).to_pandas()\n",
    "\n",
    "def _read_partitioned_pq(path):\n",
    "    return None\n",
    "\n",
    "def _update_schema(original, new_labels):\n",
    "    updated_schema = dict(original)\n",
    "    for label in new_labels:\n",
    "        if label in constants.SCHEMA_NAMES:\n",
    "            updated_schema[label] = new_labels[label]\n",
    "    return updated_schema\n",
    "\n",
    "def _has_traj_cols(df, traj_cols):\n",
    "    \n",
    "    # Check for sufficient spatial columns\n",
    "    spatial_exists = (\n",
    "        ('latitude' in traj_cols and 'longitude' in traj_cols and \n",
    "         traj_cols['latitude'] in df and traj_cols['longitude'] in df) or\n",
    "        ('x' in traj_cols and 'y' in traj_cols and \n",
    "         traj_cols['x'] in df and traj_cols['y'] in df) or\n",
    "        ('geohash' in traj_cols and traj_cols['geohash'] in df)\n",
    "    )\n",
    "    \n",
    "    # Check for sufficient temporal columns\n",
    "    temporal_exists = (\n",
    "        ('datetime' in traj_cols and traj_cols['datetime'] in df) or\n",
    "        ('timestamp' in traj_cols and traj_cols['timestamp'] in df)\n",
    "    )\n",
    "    \n",
    "    if not spatial_exists:\n",
    "        raise ValueError(\n",
    "            \"Could not find required spatial columns in {}. The dataframe columns must contain or map to at least one of the following sets: \"\n",
    "            \"('latitude', 'longitude'), ('x', 'y'), or 'geohash'.\".format(df.columns.tolist())\n",
    "        )\n",
    "        \n",
    "    if not temporal_exists:\n",
    "        raise ValueError(\n",
    "            \"Could not find required temporal columns in {}. The dataframe columns must contain or map to either 'datetime' or 'timestamp'.\".format(df.columns.tolist())\n",
    "        )\n",
    "    \n",
    "    return spatial_exists and temporal_exists\n",
    "\n",
    "\n",
    "def _cast_traj_cols(df, traj_cols):\n",
    "    if 'datetime' in traj_cols and traj_cols['datetime'] in df:\n",
    "        if not pd.core.dtypes.common.is_datetime64_any_dtype(df[traj_cols['datetime']].dtype):\n",
    "            df[traj_cols['datetime']] = pd.to_datetime(df[traj_cols['datetime']])\n",
    "    if 'timestamp' in traj_cols and traj_cols['timestamp'] in df:\n",
    "        # Coerce to integer if it's not already\n",
    "        if not pd.core.dtypes.common.is_integer_dtype(df[traj_cols['timestamp']].dtype):\n",
    "            df[traj_cols['timestamp']] = df[traj_cols['timestamp']].astype(int)\n",
    "\n",
    "    float_cols = ['latitude', 'longitude', 'x', 'y']\n",
    "    for col in float_cols:\n",
    "        if col in traj_cols and traj_cols[col] in df:\n",
    "            if not pd.core.dtypes.common.is_float_dtype(df[traj_cols[col]].dtype):\n",
    "                df[traj_cols[col]] = df[traj_cols[col]].astype(\"float\")\n",
    "\n",
    "    string_cols = ['user_id', 'geohash']\n",
    "    for col in string_cols:\n",
    "        if col in traj_cols and traj_cols[col] in df:\n",
    "            if not pd.core.dtypes.common.is_string_dtype(df[traj_cols[col]].dtype):\n",
    "                df[traj_cols[col]] = df[traj_cols[col]].astype(\"str\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def _is_traj_df(df, traj_cols = None, **kwargs):\n",
    "    \n",
    "    if not (isinstance(df, pd.DataFrame) or isinstance(df, gpd.GeoDataFrame)):\n",
    "        return False\n",
    "    \n",
    "    if not traj_cols:\n",
    "        traj_cols = {}\n",
    "        traj_cols = _update_schema(traj_cols, kwargs) #kwargs ignored if traj_cols\n",
    "        \n",
    "    traj_cols = _update_schema(constants.DEFAULT_SCHEMA, traj_cols)\n",
    "    \n",
    "    if not _has_traj_cols(df, traj_cols):\n",
    "        return False\n",
    "    \n",
    "    if 'datetime' in traj_cols and traj_cols['datetime'] in df:\n",
    "        if not pd.core.dtypes.common.is_datetime64_any_dtype(df[traj_cols['datetime']].dtype):\n",
    "            return False\n",
    "    elif 'timestamp' in traj_cols and traj_cols['timestamp'] in df:\n",
    "        if not pd.core.dtypes.common.is_integer_dtype(df[traj_cols['timestamp']].dtype):\n",
    "            return False\n",
    "\n",
    "    float_cols = ['latitude', 'longitude', 'x', 'y']\n",
    "    for col in float_cols:\n",
    "        if col in traj_cols and traj_cols[col] in df:\n",
    "            if not pd.core.dtypes.common.is_float_dtype(df[traj_cols[col]].dtype):\n",
    "                return False\n",
    "\n",
    "    string_cols = ['user_id', 'geohash']\n",
    "    for col in string_cols:\n",
    "        if col in traj_cols and traj_cols[col] in df:\n",
    "            if not pd.core.dtypes.common.is_string_dtype(df[traj_cols[col]].dtype):\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def from_pandas(df, traj_cols = None, spark_enabled=False, **kwargs):\n",
    "\n",
    "    if not (isinstance(df, pd.DataFrame) or isinstance(df, gpd.GeoDataFrame)):\n",
    "        raise TypeError(\n",
    "            \"Expected the data argument to be either a pandas DataFrame or a GeoPandas GeoDataFrame.\"\n",
    "        )\n",
    "    \n",
    "    # valid trajectory column names passed to **kwargs collected\n",
    "    if not traj_cols:\n",
    "        traj_cols = {}\n",
    "        traj_cols = _update_schema(traj_cols, kwargs) #kwargs ignored if traj_cols\n",
    "            \n",
    "    for key, value in traj_cols.items():\n",
    "        if value not in df:\n",
    "            warnings.warn(f\"Trajectory column '{value}' specified for '{key}' not found in df.\")\n",
    "            \n",
    "    # include defaults when missing\n",
    "    traj_cols = _update_schema(constants.DEFAULT_SCHEMA, traj_cols)\n",
    "    \n",
    "    if _has_traj_cols(df, traj_cols):\n",
    "        return _cast_traj_cols(df, traj_cols)\n",
    "\n",
    "def from_file(filepath, format='csv', traj_cols = None, **kwargs):\n",
    "    assert format in ['parquet', 'csv']\n",
    "\n",
    "    if format=='parquet':\n",
    "        dataset = ds.dataset(filepath, format=\"parquet\", partitioning=\"hive\", **kwargs)\n",
    "        df = dataset.to_table().to_pandas()\n",
    "        return from_pandas(df, traj_cols = None, **kwargs)\n",
    "        \n",
    "    elif format=='csv':\n",
    "        if os.path.isdir(filepath):\n",
    "            dataset = ds.dataset(filepath, format=\"csv\", partitioning=\"hive\", **kwargs)\n",
    "            df = dataset.to_table().to_pandas()\n",
    "            return from_pandas(df, traj_cols = None, **kwargs)\n",
    "    \n",
    "        else:\n",
    "            # Pass only valid kwargs for pandas.read_csv\n",
    "            read_csv_args = inspect.signature(pd.read_csv).parameters\n",
    "            read_csv_kwargs = {k: v for k, v in kwargs.items() if k in read_csv_args}\n",
    "            \n",
    "            df = pd.read_csv(filepath, **read_csv_kwargs)\n",
    "            return from_pandas(df, traj_cols = traj_cols, **kwargs)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame([[1, 39.984094, 116.319236, '2008-10-23 13:53:05'],\n",
    " [1, 39.984198, 116.319322, '2008-10-23 13:53:06'],\n",
    " [1, 39.984224, 116.319402, '2008-10-23 13:53:11'],\n",
    " [1, 39.984211, 116.319389, '2008-10-23 13:53:16']], columns = ['uid', 'latitude', 'longitude', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "traj_cols = {'user_id':'uid',\n",
    "         'latitude':'latitude',\n",
    "         'longitude':'longitude',\n",
    "            'datetime':'time'}\n",
    "df = from_object(data, traj_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "\n",
    "# Creating a sample pyarrow table\n",
    "data = {\n",
    "    'user_id': [1, 2, 2, 3, 3, 3],\n",
    "    'value': [10, 20, 30, 40, 50, 60]\n",
    "}\n",
    "\n",
    "# Creating a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Converting pandas DataFrame to pyarrow Table\n",
    "table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "\n",
    "# Extracting the user_id column and converting it to pandas Series\n",
    "user_id_col = table['user_id', 'value']\n",
    "user_ids = user_id_col.to_pandas()\n",
    "\n",
    "# Displaying the type and first few entries of user_ids\n",
    "print(type(user_ids))  # Should return <class 'pandas.core.series.Series'>\n",
    "print(user_ids.head())  # Displays the first few entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (daphme)",
   "language": "python",
   "name": "daphme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
