{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tutorial 2: Filtering and Common Transformations**\n",
    "\n",
    "In this part, we will learn how to implement basic 'cleaning' and spatio-temporal transformations of the data which are common in an analysis pipeline. We will also cover a few \"gotcha\"s to keep an eye out for. Furthermore, we show some good practices for 'cleaning' the data from unreliable users and/or pings. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Researchers often filter geospatial datasets to focus on users or trajectories that meet specific criteria—such as sufficient activity, coverage within a particular timeframe, or presence within defined geographic areas. For example, a study might analyze “users with at least 14 days of activity in January who visited Central Park.” Beyond identifying qualifying users, researchers often need to retrieve these users’ complete trajectories across a wider region to study questions like “Where else do Central Park visitors travel from or to around the world?”\n",
    "\n",
    "Obviously, this reduces the scope of the analysis and makes the sample small and oftentimes manageable without the need for a cluster. However, to get that sample it might be necessary to implement some filtering at read time. \n",
    "\n",
    "Building such filters typically involves handling polygon operations, coordinate system transformations, and reconciling inconsistent formats from various data sources.\n",
    "\n",
    "Below we show a two step process:\n",
    "\n",
    "1.\tSelect users that meet specified geographical and temporal conditions.  \n",
    "2.\tExtract the complete trajectories from those users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "For this example, we will use a third dataset different than the one used in part 1, which has more users and, occasionally, denser signals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nomad.io.base as loader\n",
    "\n",
    "filepath_root = 'gc_data_long/'\n",
    "tc = {\n",
    "    \"user_id\": \"gc_identifier\",\n",
    "    \"timestamp\": \"unix_ts\",\n",
    "    \"x\": \"dev_x\",\n",
    "    \"y\": \"dev_y\",\n",
    "    \"ha\":\"ha\",\n",
    "    \"date\":\"date\"}\n",
    "\n",
    "traj = loader.from_file(filepath_root, format='parquet', traj_cols=tc)\n",
    "traj.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ping Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can investigate the frequency of these GPS signals, which can be very sparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "total_pings = traj.groupby(tc['user_id']).size()\n",
    "total_hours = (traj[tc['timestamp']].max() - traj[tc['timestamp']].min())//3600\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(10,3), ncols=2, sharey=True)\n",
    "\n",
    "total_pings.hist(ax=ax1, bins= [500*b for b in range(17)])\n",
    "ax1.set_xlabel('total pings')\n",
    "\n",
    "# x-axis restricted to bins\n",
    "(total_pings/total_hours).hist(ax=ax2, bins= [0.25*b for b in range(40)])\n",
    "ax2.set_xlabel('ping frequency (records/hour)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note **heterogeneity in the ping frequencies** and the volume of data. However, a common gotcha in this type of analysis is to assume that *more records* = *more complete signals*. Due to how the data is collected---often in short bursts---there are users with fewer pings but **more evenly distributed** over the study period. Furthermore, many pings might have such high levels of error, that they shouldn't even be considered in the analysis. Some minimal cleaning already changes the picture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nomad import filters\n",
    "\n",
    "# Remove horizontal accuracy over 50m\n",
    "traj_ds = traj.loc[traj[tc['ha']] < 50]\n",
    "print(f\"{((traj[tc['ha']]>50).mean()*100).round(3)}% pings have 'ha' > 50m\")\n",
    "# Downsampling pings to once per minute\n",
    "num_mins=1 # Try 5\n",
    "traj_ds = filters.downsample(traj_ds, periods=num_mins, verbose=True, traj_cols=tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pings_filtered = traj_ds.groupby(tc['user_id']).size()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(10,3), ncols=2, sharey=True)\n",
    "\n",
    "total_pings.hist(ax=ax1, bins= [500*b for b in range(17)], alpha=0.5, label='raw')\n",
    "total_pings_filtered.hist(ax=ax1, bins= [500*b for b in range(17)], color='darkorange', alpha=0.5, label='filtered')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('total pings')\n",
    "\n",
    "(total_pings/total_hours).hist(ax=ax2, bins= [0.25*b for b in range(30)], alpha=0.5, label='raw')\n",
    "(total_pings_filtered/total_hours).hist(ax=ax2, bins= [0.25*b for b in range(30)], color='darkorange', alpha=0.5, label='filtered')\n",
    "ax2.set_xlabel('ping frequency (records/hour)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nomad`'s `filters.downsample` method automatically takes care of grouping by user if the name of a user id column is passed (in this case via `traj_cols`), but it would also work for a single trajectory or on a `pandas.datetime64` datetime column if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "from datetime import datetime\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from nomad.filters import to_projection, filter_users, q_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial filters and Projections\n",
    "\n",
    "Suppose we are interested in an analysis of users who visited a park during multiple days, e.g. **3 days**, of January 2024. In particular, we might want to know which other locations these users tend to visit to try and get some insights. Naturally, we do this through a spatial join of each user's coordinates with the polygon that represents the park. Here are some possible immediate challenges that could make an analysis `ad-hoc`\n",
    "\n",
    "- The polygon could be in **spherical coordinates** (also called **geographic coordinates**) and the data in **projected coordinates**. For example, in `(longitude, latitude)` coordinates with `crs='EPSG:4326'`, while the data has ``` (x, y)``` coordinates from the Web Mercator crs (`'EPSG:3857'`).\n",
    "\n",
    "- The polygon could be represented as a Well-Known-Text string (`wkt`), `shapely` polygon (or multipolygon), or a `Geopandas.GeoSeries`\n",
    "\n",
    "We can get a `shapely.Polygon` representing the park querying from the file `garden_city.geojson` but, for the sake of completeness, we will assume that the polygon is given by a `wkt`. `geopandas` and `shapely` already wrap easy to use methods for spatial joins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add an image of web mercator crs vs geographical coordinates crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the geometry data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pyproj # for coordinate reprojection\n",
    "from shapely.geometry import Polygon, box\n",
    "\n",
    "city = gpd.read_file(\"garden_city.geojson\").set_index('building_id')\n",
    "outer_box = box(*city.total_bounds).buffer(0.00015, join_style='mitre') # \"background\" box\n",
    "# poly = city.geometry.loc['p-x13-y11'] #uncomment for comparing\n",
    "\n",
    "poly_wkt = 'POLYGON ((-38.31767437672955 36.66929877196072, -38.31767437672955 36.66973109167419,'\\\n",
    "                     '-38.31821336590002 36.66973109167419, -38.31821336590002 36.66929877196072,'\\\n",
    "                     '-38.31767437672955 36.66929877196072))'\n",
    "poly = gpd.GeoSeries.from_wkt([poly_wkt])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if a user has pings inside the park \n",
    "\n",
    "We start by analyzing the data from a single user. Checking which pings fall inside a given polygon is trivial with `geopandas.GeoDataFrame.within`. However, the polygon and the passed pings must have the same CRS. Reprojection is also quite simple if we cast our dataset as a `GeoDataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = traj.loc[(traj[tc['user_id']] == 'kind_perlman')]\n",
    "\n",
    "# Cast to GeoDataFrame and reproject (easy)\n",
    "user_df = gpd.GeoDataFrame(user_df,\n",
    "                           geometry=gpd.points_from_xy(user_df['dev_x'], user_df['dev_y']),\n",
    "                           crs='EPSG:3857').to_crs('EPSG:4326')\n",
    "\n",
    "user_df['in_park'] = user_df.within(poly)\n",
    "user_df = gpd.clip(user_df, outer_box) # For plotting purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely.plotting as shp_plt\n",
    "# for pretty plot\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.set_axis_off()\n",
    "\n",
    "shp_plt.plot_polygon(outer_box, ax=ax, add_points=False, color='#0e0e0e')\n",
    "city.plot(ax=ax, edgecolor='white', color='#2c353c')\n",
    "shp_plt.plot_polygon(poly, ax=ax, add_points=False, color='#0fad5a', alpha=0.5)\n",
    "user_df.plot(ax=ax, column='in_park', marker='o', markersize=12, alpha=(0.4+user_df.in_park*0.6), cmap='Reds')\n",
    "ax.set_title(\"Pings inside 'p-x13-y11' (single user)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualification constraints: minimum active days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trajectories are **noisy**, so some of the pings in the park might actually come from the street. Thus, to include our user in the final analysis we want to make sure they have a sufficient number of pings inside the park in each of, at least, 3 days. This could remove any **spurious 'visits'** which are *actually noise*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_df.loc[user_df['in_park']].groupby('date').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again face the issue of pings possibly being concentrated in short bursts, in which case, *even 60 pings could happen in the span of a few minutes*. Properly detecting periods of time in which individuals stopped will be explained in the next notebook. For now, we can deal with spurious visits by using `filters.downsample` again. Our criterion can be that **in at least 3 days there are at least 3 different pings in the park separated by at least 3 minutes** (feel free to change this at your leisure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "park_pings_ds = filters.downsample(user_df.loc[user_df['in_park']], periods=3, traj_cols=tc)\n",
    "park_pings_ds.groupby('date')['in_park'].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. We see that our user makes the cut and can be incorporated in our sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualification constraints: signal completeness (q)\n",
    "\n",
    "As with any dataset, if the data for a user is highly incomplete, then results might not be reliable due to *incompleteness bias*. Imagine trying to analyze whether visitors to the park tend to come back the next day, but one of your 'visitors' only has data every 5 days. We quantify the *completeness* of a user with a statistic $q$ (or, conversely its *sparsity* would be $1-q$) that, given a temporal resolution (typically an hour, or a day) provides the percentage of 'time buckets' with any data at that resolution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters.completeness(user_df, periods=1, freq='h', traj_cols=tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.unix_ts.is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate system projection\n",
    "\n",
    "The first step after loading the dataset is to ensure the dataset is in the expected coordinate system.\n",
    "\n",
    "Many geospatial datasets come in spherical coordiantes latitude/longitude (EPSG:4326). However, spatial analyses---like joins of points in polygons, computing buffers, or clustering pings---might benefit from computing euclidean distances. Thus projected planar coordinates (like EPSG:3857) are commonly used. Nomad's `to_projection` method creates new columns `x` and `y` with projected coordinates in any coordinate reference system (CRS) recognized by PyProj. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project to EPSG:3857 (Web Mercator)\n",
    "projected_x, projected_y = to_projection(traj=traj,\n",
    "                                         input_crs=\"EPSG:4326\",\n",
    "                                         output_crs=\"EPSG:3857\",\n",
    "                                         longitude=\"longitude\",\n",
    "                                         latitude=\"latitude\")\n",
    "\n",
    "traj['x'] = projected_x\n",
    "traj['y'] = projected_y\n",
    "\n",
    "traj.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply desired filters\n",
    "\n",
    "Our library currently allows for three common forms of filtering:\n",
    "- **Temporal Filtering**: Restrict data to a time window of interest (e.g., January). Use the `start_time` and `end_time` arguments. If \n",
    "- **Quantity-Based Filtering**: Keep only users with sufficient activity as measured by a minimum number of pings. Use the `min_active_days` and `min_pings_per_day` arguments.\n",
    "- **Spatial Filtering**: Keep only users with pings that fall within a specific geographic region (e.g., Philadelphia). Use the `polygon` argument.\n",
    "\n",
    "If the aforementioned arguments are not specified, the default arguments ensure that the respective filtering is not performed. I.e., `polygon` defaults to `None`, and so no spatial filtering is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the polygon for the park area (in projected coordinates)\n",
    "polygon_coords = [\n",
    "    (-4265504, 4393111), \n",
    "    (-4265504, 4393171), \n",
    "    (-4265564, 4393171), \n",
    "    (-4265564, 4393111)\n",
    "]\n",
    "polygon = Polygon(polygon_coords)\n",
    "\n",
    "n0 = len(traj)\n",
    "uq0 = traj['uid'].unique()\n",
    "\n",
    "filtered_traj = filter_users(traj=traj, \n",
    "                             start_time=pd.Timestamp(\"2024-01-01 00:00:00\", tz='America/New_York'),\n",
    "                             end_time=pd.Timestamp(\"2024-01-07 23:59:00\", tz='America/New_York'),\n",
    "                             polygon=polygon,\n",
    "                             min_active_days=2,\n",
    "                             min_pings_per_day=10,\n",
    "                             user_id='uid',\n",
    "                             x='x',\n",
    "                             y='y')\n",
    "\n",
    "n1 = len(filtered_traj)\n",
    "uq1 = filtered_traj['uid'].unique()\n",
    "print(f\"Number of pings before filtering: {n0}\")\n",
    "print(f\"Number of unique users before filtering: {len(uq0)}\")\n",
    "print(f\"Number of pings after filtering: {n1}\")\n",
    "print(f\"Number of unique users after filtering: {len(uq1)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize completeness and filter by completeness metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here, a discussion on how we want to discard users that have too incomplete a signal to be used. E.g. Q < 80% . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  _generate_Q_matrix(df, traj_cols={'datetime':'event_zoned_datetime', 'user_id':'cuebiq_id'})\n",
    "# num_days = 42\n",
    "\n",
    "# q_hourly = Q.mean(axis=0)*100\n",
    "# q_daily = ((Q>0.0).sum(axis=0)/num_days)*100\n",
    "\n",
    "# # assume Q is your DataFrame and q_daily is the Series of mean completeness per user\n",
    "# sorted_cols = q_daily.sort_values(ascending=False).index\n",
    "# Qs = Q[sorted_cols]\n",
    "# binary = Qs.gt(0).astype(int)\n",
    "# cmap = plt.cm.Blues\n",
    "\n",
    "# fig, axes = plt.subplots(\n",
    "#     2, 2,\n",
    "#     figsize=(8, 8),\n",
    "#     gridspec_kw={'height_ratios': [1, 2.5]},\n",
    "#     sharey='row'\n",
    "# )\n",
    "\n",
    "# # marginal histograms\n",
    "# axes[0,0].hist(q_daily, bins=46)\n",
    "# axes[0,0].set_xlabel('% days with data')\n",
    "\n",
    "# axes[0,1].hist(q_hourly, bins=46)\n",
    "# axes[0,1].set_xlabel('% hours with data')\n",
    "\n",
    "# for ax, data in zip([axes[0,1], axes[0,0]], (q_hourly, q_daily)):\n",
    "#     ax.axvline(np.median(data), linestyle='--', color='red', lw=1)\n",
    "# # binary heatmap\n",
    "# sns.heatmap(\n",
    "#     binary.T,\n",
    "#     ax=axes[1,0],\n",
    "#     cmap=cmap,\n",
    "#     vmin=0, vmax=1,\n",
    "#     cbar=False,\n",
    "#     xticklabels=False,\n",
    "#     yticklabels=False\n",
    "# )\n",
    "# axes[1,0].set_ylabel('User')\n",
    "\n",
    "# # continuous heatmap\n",
    "# sns.heatmap(\n",
    "#     Qs.T,\n",
    "#     ax=axes[1, 1],\n",
    "#     cmap=cmap,\n",
    "#     vmin=0, vmax=1,\n",
    "#     cbar_kws={'label': 'Completeness'},\n",
    "#     xticklabels=False,\n",
    "#     yticklabels=False\n",
    "# )\n",
    "# axes[1,1].set_ylabel('')\n",
    "\n",
    "# # set date ticks on bottom row\n",
    "# dates = pd.to_datetime(Qs.index)\n",
    "# tick_locs = np.linspace(0, len(dates) - 1, 6, dtype=int)\n",
    "# tick_lbls = dates.strftime('%m-%d')[tick_locs]\n",
    "# for ax in [axes[1,0], axes[1,1]]:\n",
    "#     ax.set_xticks(tick_locs)\n",
    "#     ax.set_xticklabels(tick_lbls, rotation=45, ha='right')\n",
    "\n",
    "# #plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code constructs a daily $Q$ matrix, which represents the completeness of user activity for each day. $q$ represents the proportion of hours in a day during which a specific user has recorded activity. A sliding window of 3 days is then applied to compute a mean completeness metric for each date in the sliding window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct daily Q matrix\n",
    "Q = filters._generate_Q_matrix(traj, tc)\n",
    "Q\n",
    "\n",
    "# no need for sliding window\n",
    "# visualize (1) distribution of q=0, (2) barcode 0/1 for each hour for each user plt.imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window = 3  # in days\n",
    "SW_dates = Q.index[:-sliding_window]  # dates involved in the sliding window computation\n",
    "Q_window = pd.DataFrame(\n",
    "    [filters._compute_mean_q(Q, date, sliding_window) for date in SW_dates],\n",
    "    index=SW_dates\n",
    "    )\n",
    "\n",
    "Q_window.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualize the number of complete users in each day for different values of completeness ($\\bar{q} = 1-\\epsilon$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_eps = [0.3,0.5]\n",
    "Colors_eps = ['red', 'blue']\n",
    "X = range(len(SW_dates))\n",
    "\n",
    "DICT_legend= {'classes': [int(e*100) for e in LIST_eps], \n",
    "              'colors': Colors_eps, \n",
    "              'loc': 'lower right', \n",
    "              'fontsize':10,\n",
    "              'title': '$\\\\epsilon \\\\quad (\\\\%)$', \n",
    "              'title_fontsize':10}\n",
    "\n",
    "DICT_ticks = {'t': X, \n",
    "              'tl': SW_dates, \n",
    "              'rot': 90, \n",
    "              'size':10}\n",
    "\n",
    "DICT_label_titles = {'xlabel': 'Date', \n",
    "                     'ylabel': 'Count of complete users', \n",
    "                     'title': f'Daily time-series of complete users \\n over {sliding_window} days sliding window with $\\\\epsilon$ tolerance', \n",
    "                     'label_size': 15, \n",
    "                     'title_size': 15}\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15,5))\n",
    "\n",
    "for eps,color in zip(LIST_eps, Colors_eps):\n",
    "    #Counts of complete users for each date - based on eps tolerance\n",
    "    I_complete_eps = (Q_window > (1-eps)).sum(axis=1)\n",
    "    ax.plot(X, I_complete_eps.values, color = color, label = eps)\n",
    "\n",
    "filters._ax_visual_legend(ax, DICT_legend)\n",
    "filters._ax_visual_ticklabel(ax, DICT_ticks, axis = 'x')\n",
    "filters._ax_visual_labeltitles(ax, DICT_label_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use $q$ to filter users by completeness, retaining only the users who exhibit a mean completeness of over `qbar` in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0 = len(traj)\n",
    "uq0 = traj['uid'].unique()\n",
    "\n",
    "complete_users = q_filter(traj=traj,\n",
    "                          qbar=0.7,\n",
    "                          sliding_window=3,\n",
    "                          traj_cols=tc)\n",
    "filtered_traj_q = traj[traj['uid'].isin(complete_users)]\n",
    "\n",
    "n1 = len(filtered_traj_q)\n",
    "uq1 = filtered_traj_q['uid'].unique()\n",
    "\n",
    "print(f\"Number of pings before filtering: {n0}\")\n",
    "print(f\"Number of unique users before filtering: {len(uq0)}\")\n",
    "print(f\"Number of pings after filtering: {n1}\")\n",
    "print(f\"Number of unique users after filtering: {len(uq1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying filters on read\n",
    "\n",
    "If your dataset is too large. Loading it all in memory to then apply the filters can be intractable. Nomad implements these functionalities to filter in this way on read..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
