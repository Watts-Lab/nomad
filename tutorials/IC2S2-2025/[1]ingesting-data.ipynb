{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e43946-4af5-4cfe-8b33-21aa16a5cbd0",
   "metadata": {},
   "source": [
    "### Outline\n",
    "explain how nomad can address different challenges in the data ingestion process:\n",
    "1) the data can be partitioned, complicating things for users most familiar with pandas, a simple wrapper function from_file simplifies things, same function for simple csvs or partitioned data\n",
    "2) column names and formats might vary from dataset to dataset, but spatial analysis functions require spatial and temporal columns, sometimes the time is expected as a unix timestamp, sometimes as a datetime with the local timezone. Similarly, an algorithm might require the latitude and longitude. Users always have the alternative of renaming the data so that those column names match the defaults of those functions, or they can input the right column names (or pass the relevant columns) on functions that have such flexibility. Nothing wrong with that. However, it could be preferrable to not alter the data permanently for analysis, specially if one will perform some filtering or produce a derivative dataset that is expected to be joined later on with the original data. Passing the correct column names every time to processing functions can be burdensome and verbose, and makes code less reusable when applied to a dataset with different column names. nomad addresses this by using an auxiliary dictionary of column names which other processing methods can use to find the appropriate columns. This is somewhat equivalent to passing the column names as keyword arguments, but functions also have a fallback to default column names for certain expected columns (like latitude, longitude, user_id, timestamp, etc).\n",
    "3) We can demonstrate the flexibiilty that this auxiliary dictionary offers, by loading some device-level data fond in `gc-data.csv`. Beyond being a wrapper for the pandas or pyarrow reader functions, the `io` reader method, `from_file`, also ensures the trajectory columns (coordinates and time columns) are cast to the correct data types, issues warnings when unix timestamps are not in seconds, and raises errors when the data seems to lack spatial or temporal columns that will likely be required in downstream processing. This can be evidenced by comparing the output of simply using `pandas.read_csv` with that of `from_file`, where we see that the right columns have been cast to the right data types:\n",
    "\n",
    "4) Of particular importance is the standardized handling of datetime strings in iso8601 formats. These can be timezone naive, have timezone information, and even have mixed timezones. For instance, when a trajectory spans multiple regions, or when there are daylight savings changes. nomad tries to simplify the parsing of dates in such cases, with three cases: [code explaining]\n",
    "\n",
    "5) This last case is important because distributed algorithms relying on Spark do not store timezone information in the timestamp format. This presents a challenge in which analysis related to local datetime is required, but this information is lost. Switching to utc time is always an option which makes naive datetimes comparable, but it makes analysis of day-time, night-time behaviors more complicated when there are mixed timezones. A standard way to deal with timezone data is to strip the timezone information from timestamps and represent it in a separate column as the offset from UTC time in seconds. Thus, for compatibility with Spark workflows, setting `mixed_timezone_behavior = \"naive\"` will create a `tz_offset` column (when one does not already exist).\n",
    "\n",
    "6) The flexibility provided by nomad to easily switch between small analyses using a small example of data, which could be stored in a .csv file, for testing code, and then using the same (or similar functions) to scale up in a distributed environment, facilitates a common (and recommended) workflow in which users can easily read data from some users from a large dataset and use standard pandas functionalities, benchmark their code, etc, and then scale up using more resources once they are certain their code is in good shape. This can easily be done with io methods like `sample_users`, `sample_from_file` (which may optionally take a sample of users drawn from somewhere else). This is shown as follows:\n",
    "\n",
    "7) Finally, a user might want to persist such a sample with care for the data types and, perhaps, recovering the date string format with timezone, which is possible even when this information was saved in the tz_offset column. Notice that this writer function can also seamlessly switch between csv and parquet formats, leveraging pyarrow and pandas. FOr example: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e74357-703f-4a47-b365-74e6beac612b",
   "metadata": {},
   "source": [
    "# **Tutorial 1: Loading and Sampling Trajectory Data**\n",
    "\n",
    "## Getting started\n",
    "\n",
    "Real-world mobility files vary widely in structure and formatting. Timestamps may be recorded as UNIX integers or ISO-formatted strings, with or without timezone offsets. Coordinate columns may follow different naming conventions, and files may be stored either as flat CSVs or as partitioned Parquet directories. This notebook demonstrates how `nomad.io.base` standardizes data loading across these variations using two example datasets: a CSV file (`gc-data.csv`) and a partitioned Parquet directory (`gc-data/`). For visualization, we will also use a dataset with building geometries underlying the synthetic data in these examples. Namely, the file `garden_city.geojson`.\n",
    "\n",
    "## Inspecting schemas\n",
    "Let's start by inspecting the schemas of the datasets we will use with the nomad helper function `table_columns` from the `io` module. This method reports column names for both flat files and partitioned datasets without reading the full content into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d2a70-4f52-44e9-8e8f-106426addea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nomad.io import base as loader\n",
    "\n",
    "print(loader.table_columns(\"gc-data.csv\", format=\"csv\"))\n",
    "print(loader.table_columns(\"gc-data/\", format=\"parquet\")) # <<< SHOULD BE A PARTITIONED CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e81f2-bcf3-4cc5-8c26-b6111484df73",
   "metadata": {},
   "source": [
    "## Typical processing with `pandas`, `geopandas`\n",
    "\n",
    "When analyzing a manageable sample of data, pandas and geopandas provide excellent functionalities that allow you to do preliminary analysis and plotting without many additional tools. Suppose we want to perform some preliminary analysis of the data in (`gc-data.csv`). Suppose we would like to\n",
    "- Load the trajectory and geometry data from disk. (using `pandas.read_csv()` and `geopandas.read_file()`)\n",
    "- Plot the data of a user for a given day. (using `geopandas.plot()` and `matplotlib.pyplot.plot()`)\n",
    "- Create a heatmap of certain areas with a lot of pings. (for this we could use a tessellation, for example `h3`).\n",
    "- Analyze if there are gaps in the user's signals. (likely with a simple histogram from `matplotlib`)\n",
    "\n",
    "For example, we can do it like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904bf840-4253-41e3-a1d3-d54874072613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import h3\n",
    "from shapely import Polygon\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv(\"gc-data.csv\")\n",
    "# FIX THIS AND DELETE ME!\n",
    "df.loc[:, ['device_lon', 'device_lat']] = -1 * df.loc[:, ['device_lat', 'device_lon']].values + 0.003 #FIX THIS!!!\n",
    "city = gpd.read_file(\"garden_city.geojson\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c85efaa-9010-49a2-b502-bdd0c35896ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trajectory data of a single user\n",
    "user = df['identifier'].iloc[0]\n",
    "user_df = df.loc[(df['identifier'] == user) & (df['date'] == '2024-01-04')]\n",
    "\n",
    "# Plot trajectory\n",
    "fig, ax1 = plt.subplots(figsize=(4,4))\n",
    "ax1.set_axis_off()\n",
    "\n",
    "city.plot(ax=ax1, column='type', edgecolor='black', linewidth=0.75, cmap='Set3')\n",
    "ax1.scatter(user_df['device_lon'], user_df['device_lat'], s=6, alpha=0.75, color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5363ef05-24a2-4c8d-8c38-840e36f76b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot an h3 heatmap\n",
    "#  For this task we can leverage `h3.latlng_to_cell` and `h3.cell_to_boundary` to get a cell's polygon\n",
    "#  'Sharp edge # 1' the order of latitude and longitude depends on the library. We need to switch the order!\n",
    "\n",
    "# Switch lat, lon to lon, lat and pass to shapely Polygon\n",
    "def h3_cell_to_polygon(cell):\n",
    "    coords = h3.cell_to_boundary(cell)\n",
    "    lat, lon = zip(*coords)\n",
    "    return Polygon(zip(lon, lat))    \n",
    "\n",
    "# Cell for each row\n",
    "def row_to_h3_cell(row, res):\n",
    "    return h3.latlng_to_cell(lat=row['device_lat'], lng=row['device_lon'], res=res)\n",
    "\n",
    "df['cell'] = df.apply(row_to_h3_cell, res=12, axis=1)\n",
    "\n",
    "pings_per_cell = df.groupby('cell').agg(pings=('unix_timestamp', 'count')).reset_index()\n",
    "pings_per_cell['geometry'] = pings_per_cell['cell'].apply(h3_cell_to_polygon)\n",
    "\n",
    "h3_gdf = gpd.GeoDataFrame(pings_per_cell, geometry='geometry')\n",
    "\n",
    "fig, ax2 = plt.subplots(figsize=(5,4))\n",
    "city.plot(ax=ax2, column='type', edgecolor='black', linewidth=0.75, cmap='Set3')\n",
    "h3_gdf.plot(column=h3_gdf.pings, cmap='Reds', alpha=0.75, ax=ax2, legend=True)\n",
    "ax2.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f136dd-de20-4190-b96c-943c7b193463",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's find the maximum temporal gap in the trajectory of each user\n",
    "# A simple pandas groupby on the unix_timestamp column (in seconds since epoch)\n",
    "def get_max_gap_minutes(times):\n",
    "    shifted_times = times.shift(1, fill_value=0) # shift gives previous value\n",
    "    gaps = (times.iloc[1:] - shifted_times.iloc[1:])//60 # gaps in minutes\n",
    "    return gaps.max()\n",
    "\n",
    "max_gap = df.groupby('identifier')['unix_timestamp'].apply(get_max_gap_minutes)\n",
    "\n",
    "fig, ax3 = plt.subplots(figsize=(4,3))\n",
    "max_gap.hist(ax=ax3, bins=24, color='#8dd3c7')\n",
    "ax3.set_xlabel('minutes')\n",
    "ax3.set_title('max temporal gap per user')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f802bd-950f-4c64-a309-766590d7e4e9",
   "metadata": {},
   "source": [
    "## Data ingestion with `nomad` — a reusable pipeline\n",
    "\n",
    "Here, we explore which advantages (if any) we can get from using `nomad` for the same preliminary analysis. In the case of reading a single csv file, the reader function `nomad.io.base.from_file` is basically a `pandas` wrapper, except that it facilitates the parsing of spatiotemporal columns which are known to follow specific formatting, for instance:\n",
    "\n",
    "- dates and datetimes in ISO format are cast to `pandas.datetime64`\n",
    "- unix timestamps are cast to integers\n",
    "- user identifiers are cast to strings\n",
    "- coordinates are cast to float\n",
    "\n",
    "For such typecasting and other methods, `nomad` relies on a user provided mapping from \"default\" column names, to the column names in the data, namely, the dictionary `traj_cols`. This prevents having to rename columns *ad hoc* to reuse code, and simplifies the number of arguments passed to different methods. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341ed5b6-b375-4069-9a51-0f6d6ef069e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the possible default column names that could be mapped to data columns to methods in `nomad` \n",
    "from nomad.constants import DEFAULT_SCHEMA\n",
    "DEFAULT_SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b7bf33-48a1-4d75-bd95-ae05fb7f9357",
   "metadata": {},
   "source": [
    "<a id='hidden-cell'></a>\n",
    "\n",
    "A problem that can arise when analyzing data using just `pandas` is that the geospatial data is often partitioned, e.g. stored in smaller csv chunks in partitioned directories (e.g. `date=2024-01-01/`). Rather than reading the data with a for loop (and turning the partitioning directories into variables), `nomad`'s `from_file` can read a whole directory with the same function call, by warpping `PyArrow`'s file reader.\n",
    "\n",
    "Let's replicate the previous analysis starting with `nomad`'s file reader on the partitioned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf535a-b1f7-41e2-993c-56c40c768b32",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For the single csv dataset\n",
    "traj_cols = {\"user_id\": \"identifier\",\n",
    "             \"timestamp\": \"unix_timestamp\",\n",
    "             \"latitude\": \"device_lat\",\n",
    "             \"longitude\": \"device_lon\",\n",
    "             \"datetime\": \"local_datetime\",\n",
    "             \"date\": \"date\"}\n",
    "file_path = \"gc-data.csv\"\n",
    "\n",
    "df = loader.from_file(file_path, format=\"csv\", traj_cols=traj_cols)\n",
    "# check data types\n",
    "#print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33de9d2-ee49-46ac-96a1-56784674d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the partitioned dataset\n",
    "traj_cols = {\"user_id\": \"user_id\",\n",
    "             \"timestamp\": \"timestamp\",\n",
    "             \"latitude\": \"latitude\",\n",
    "             \"longitude\": \"longitude\",\n",
    "             \"datetime\": \"datetime\",\n",
    "             \"date\": \"date\"} # the dataset has default column names\n",
    "file_path = \"gc-data/\" # partitioned\n",
    "\n",
    "# Try traj_cols=None. It should work because of default names\n",
    "# Try mixed_timezone_behavior=\"utc\" or \"object\", or parse_dates = False and inspect df\n",
    "df = loader.from_file(file_path, format=\"parquet\", traj_cols=traj_cols, parse_dates=True)\n",
    "# FIX THIS AND DELETE ME!\n",
    "df.loc[:, ['longitude', 'latitude']] = -1 * df.loc[:, ['latitude', 'longitude']].values + 0.003 #FIX THIS!!!\n",
    "# check data types\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d82bfd-b126-4346-81f7-e0b738ccfe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute all three statistics as before \n",
    "\n",
    "# Trajectory of a single user\n",
    "user = df[traj_cols['user_id']].iloc[0]\n",
    "user_df = df.loc[(df[traj_cols['user_id']] == user) & (df[traj_cols['date']] == '2024-01-04')]\n",
    "\n",
    "# Pings per cell geodataframe\n",
    "df[\"cell\"] = df.apply(\n",
    "    lambda r: h3.latlng_to_cell(lat=r[traj_cols[\"latitude\"]], lng=r[traj_cols[\"longitude\"]], res=12),\n",
    "    axis=1)\n",
    "\n",
    "pings_per_cell = df.groupby('cell').agg(pings=(traj_cols['timestamp'], 'count')).reset_index()\n",
    "h3_gdf = gpd.GeoDataFrame(pings_per_cell, geometry=pings_per_cell['cell'].apply(h3_cell_to_polygon))\n",
    "\n",
    "# Maximum gap for each user\n",
    "max_gap = df.groupby(traj_cols['user_id'])[traj_cols['timestamp']].apply(get_max_gap_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b55ae-51cf-4e89-bf53-918ee80b735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(figsize=(12,3), ncols=3)\n",
    "\n",
    "# trajectory of a single user\n",
    "city.plot(ax=ax1, column='type', edgecolor='black', linewidth=0.75, cmap='Set3')\n",
    "ax1.scatter(user_df[traj_cols[\"longitude\"]], user_df[traj_cols[\"latitude\"]], s=6, alpha=0.75, color='black')\n",
    "ax1.set_axis_off()\n",
    "# heatmap\n",
    "city.plot(ax=ax2, column='type', edgecolor='black', linewidth=0.75, cmap='Set3')\n",
    "h3_gdf.plot(column=h3_gdf.pings, cmap='Reds', alpha=0.75, ax=ax2, legend=True)\n",
    "ax2.set_axis_off()\n",
    "# gaps\n",
    "max_gap.hist(ax=ax3, bins=24, color='#8dd3c7')\n",
    "ax3.set_xlabel('minutes')\n",
    "ax3.set_title('max temporal gap per user')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4514fdd-d74a-4c69-bb68-73a091292d52",
   "metadata": {},
   "source": [
    "Now, go back to (hidden) [cell 7](#hidden-cell) and try simply changing the file path and column name mapping (traj_cols). The rest of the code works the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbfe5f4-c481-4816-8136-bca26e48cf3c",
   "metadata": {},
   "source": [
    "## A good practice: prototype on a small sample, scale up later\n",
    "\n",
    "While a researcher is still exploring and designing their experiments, it can be impractical, time consuming, or outright intractable, to use the entire dataset. Thus, it is recommended to work on a sample of the data, either by selecting some users at random, some records at random, or both. `nomad`'s `io.base.sample_users` selects a reproducible subset of user IDs, while `io.base.sample_from_file` reads the data of only those users, optionally sampling records. The resulting sample can be written to disk using `io.base.to_file`. \n",
    "\n",
    "When persisting the sample, we partition by `date` again to preserve the likeness with the original dataset. Reading the output back with `from_file` confirms that the sample was saved correctly and remains compatible with the same loading functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565115f0-1e3c-4b34-8eca-3abeb1695f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = \"gc-data/\" # has default names\n",
    "fmt = \"parquet\"\n",
    "\n",
    "# full data\n",
    "df = loader.from_file(file_path, format=fmt)\n",
    "\n",
    "# sample users\n",
    "users = loader.sample_users(file_path, format=fmt, size=12, seed=314) # change if user_id has other name\n",
    "# sample data, pass users\n",
    "sample_df = loader.sample_from_file(file_path, users=users, format=fmt, frac_records=0.30, seed=314)\n",
    "\n",
    "## optionally try uncommenting this line\n",
    "# sample_df = loader.sample_from_file(file_path, users=users, format=fmt, frac_records=0.30, frac_users=0.12, seed=314)\n",
    "\n",
    "# persist\n",
    "loader.to_file(sample_df, \"/tmp/nomad_sample\", format=fmt, partition_by=[\"date\"], existing_data_behavior='overwrite_or_ignore')\n",
    "round_trip = loader.from_file(\"/tmp/nomad_sample\", format=fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd42c69-c932-428e-ab9c-30b46a1aecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- Value counts for sample of data:\\n\")\n",
    "print(round_trip.user_id.value_counts())\n",
    "print(\"\\n---------------------------------\\n\")\n",
    "print(\"- Value counts for original data:\\n\")\n",
    "print(df.user_id.value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (daphme)",
   "language": "python",
   "name": "daphme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "conda_py_310_env"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      },
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "conda_py_310_env"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    },
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 5,
           "op": "addrange",
           "valuelist": "6"
          },
          {
           "key": 5,
           "length": 2,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3 (ipykernel)"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      },
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "python3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    },
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
