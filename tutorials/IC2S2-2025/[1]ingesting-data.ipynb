{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fd5cdb-2f87-4b30-99f9-21dda2ff3ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/Watts-Lab/nomad.git@IC2S2-tutorial\n",
    "\n",
    "import gdown\n",
    "gdown.cached_download(\n",
    "    \"https://drive.google.com/uc?id=1wk3nrNsmAiBoTtWznHjjjPkmWAZfxk0P\",\n",
    "    path=\"IC2S2_2025.zip\",\n",
    "    quiet=False,\n",
    "    postprocess=gdown.extractall,  # auto-unzip\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e74357-703f-4a47-b365-74e6beac612b",
   "metadata": {},
   "source": [
    "# **Tutorial 1: Loading and Sampling Trajectory Data**\n",
    "\n",
    "## Getting started\n",
    "\n",
    "Real-world mobility files vary widely in structure and formatting. Timestamps may be recorded as UNIX integers or ISO-formatted strings, with or without timezone offsets. Coordinate columns may follow different naming conventions, and files may be stored either as flat CSVs or as partitioned Parquet directories. This notebook demonstrates how `nomad.io.base` standardizes data loading across these variations using two example datasets: a CSV file (`gc-data.csv`) and a partitioned Parquet directory (`gc-data/`). For visualization, we will also use a dataset with building geometries underlying the synthetic data in these examples. Namely, the file `garden_city.geojson`.\n",
    "\n",
    "## Inspecting schemas\n",
    "Let's start by inspecting the schemas of the datasets we will use with the nomad helper function `table_columns` from the `io` module. This method reports column names for both flat files and partitioned datasets without reading the full content into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d2a70-4f52-44e9-8e8f-106426addea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nomad.io import base as loader\n",
    "\n",
    "print(loader.table_columns(\"gc_data.csv\", format=\"csv\"))\n",
    "print(loader.table_columns(\"gc_data/\", format=\"csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e81f2-bcf3-4cc5-8c26-b6111484df73",
   "metadata": {},
   "source": [
    "## Typical processing with `pandas`, `geopandas`\n",
    "\n",
    "When analyzing a manageable sample of data, pandas and geopandas provide excellent functionalities that allow you to do preliminary analysis and plotting without many additional tools. Suppose we want to perform some preliminary analysis of the data in (`gc-data.csv`). Suppose we would like to\n",
    "- Load the trajectory and geometry data from disk. (using `pandas.read_csv()` and `geopandas.read_file()`)\n",
    "- Plot the data of a user for a given day. (using `geopandas.plot()` and `matplotlib.pyplot.plot()`)\n",
    "- Create a heatmap of certain areas with a lot of pings. (for this we could use a tessellation, for example `h3`).\n",
    "- Analyze if there are gaps in the user's signals. (likely with a simple histogram from `matplotlib`)\n",
    "\n",
    "For example, we can do it like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904bf840-4253-41e3-a1d3-d54874072613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv(\"gc_data.csv\")\n",
    "city = gpd.read_file(\"garden_city.geojson\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c85efaa-9010-49a2-b502-bdd0c35896ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot trajectory data of a single user\n",
    "user = df['identifier'].iloc[0]\n",
    "user_df = df.loc[(df['identifier'] == user) & (df['date'] == '2024-01-04')]\n",
    "\n",
    "# Plot trajectory\n",
    "fig, ax1 = plt.subplots(figsize=(4,4))\n",
    "ax1.set_axis_off()\n",
    "\n",
    "city.plot(ax=ax1, column='type', edgecolor='black', linewidth=0.75, cmap='Set3')\n",
    "ax1.scatter(user_df['device_lon'], user_df['device_lat'], s=5, alpha=0.75, color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5363ef05-24a2-4c8d-8c38-840e36f76b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot an h3 heatmap of ping hotspots.\n",
    "#  For this task we can leverage `h3.latlng_to_cell` and `h3.cell_to_boundary` to get a cell's polygon\n",
    "#  'Sharp edge # 1' the order of latitude and longitude depends on the library. We need to switch the order!\n",
    "import h3\n",
    "from shapely import Polygon\n",
    "\n",
    "# Switch lat, lon to lon, lat and pass to shapely Polygon\n",
    "def h3_cell_to_polygon(cell):\n",
    "    coords = h3.cell_to_boundary(cell)\n",
    "    lat, lon = zip(*coords)\n",
    "    return Polygon(zip(lon, lat))\n",
    "\n",
    "# Cell for each row\n",
    "def row_to_h3_cell(row, res):\n",
    "    return h3.latlng_to_cell(lat=row['device_lat'], lng=row['device_lon'], res=res)\n",
    "\n",
    "df['cell'] = df.apply(row_to_h3_cell, res=12, axis=1)\n",
    "\n",
    "pings_per_cell = df.groupby('cell').agg(pings=('unix_timestamp', 'count')).reset_index()\n",
    "cell_geometries = pings_per_cell['cell'].apply(h3_cell_to_polygon)\n",
    "\n",
    "h3_gdf = gpd.GeoDataFrame(pings_per_cell, geometry=cell_geometries)\n",
    "h3_gdf = h3_gdf.loc[h3_gdf.pings>5] # Try 800\n",
    "\n",
    "fig, ax2 = plt.subplots(figsize=(5,4))\n",
    "city.plot(ax=ax2, column='type', edgecolor='black', linewidth=0.75, cmap='Set3') \n",
    "h3_gdf.plot(column=h3_gdf.pings, cmap='inferno', alpha=0.75, ax=ax2, legend=True, legend_kwds={'shrink': 0.5}) # try cmap='inferno' or 'plasma'\n",
    "ax2.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f136dd-de20-4190-b96c-943c7b193463",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's find the maximum temporal gap in the trajectory of each user\n",
    "# A simple pandas groupby on the unix_timestamp column (in seconds since epoch)\n",
    "def get_max_gap_minutes(times):\n",
    "    shifted_times = times.shift(1, fill_value=0) # shift gives previous value\n",
    "    gaps = (times.iloc[1:] - shifted_times.iloc[1:])//60 # gaps in minutes\n",
    "    return gaps.max()\n",
    "\n",
    "max_gap = df.groupby('identifier')['unix_timestamp'].apply(get_max_gap_minutes)\n",
    "\n",
    "fig, ax3 = plt.subplots(figsize=(4,3))\n",
    "max_gap.hist(ax=ax3, bins=24, color='#8dd3c7')\n",
    "ax3.set_xlabel('minutes')\n",
    "ax3.set_title('max temporal gap per user')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f802bd-950f-4c64-a309-766590d7e4e9",
   "metadata": {},
   "source": [
    "## Data ingestion with `nomad` â€” default type casting and a reusable pipeline\n",
    "\n",
    "Here, we explore which advantages (if any) we can get from using `nomad` for the same preliminary analysis. In the case of reading a single csv file, the reader function `nomad.io.base.from_file` is basically a `pandas` wrapper, except that it facilitates the parsing of spatiotemporal columns which are known to follow specific formatting, for instance:\n",
    "\n",
    "- dates and datetimes in ISO format are cast to `pandas.datetime64`\n",
    "- unix timestamps are cast to integers\n",
    "- user identifiers are cast to strings\n",
    "- coordinates are cast to float\n",
    "\n",
    "For such typecasting and other methods, `nomad` relies on a user provided mapping from \"default\" column names, to the column names in the data, namely, the dictionary `traj_cols`. This prevents having to rename columns *ad hoc* to reuse code, and simplifies the number of arguments passed to different methods. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341ed5b6-b375-4069-9a51-0f6d6ef069e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the possible default column names that could be mapped to data columns to methods in `nomad` \n",
    "from nomad.constants import DEFAULT_SCHEMA\n",
    "DEFAULT_SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b7bf33-48a1-4d75-bd95-ae05fb7f9357",
   "metadata": {},
   "source": [
    "<a id='hidden-cell'></a>\n",
    "\n",
    "A problem that can arise when analyzing data using just `pandas` is that the geospatial data is often partitioned, e.g. stored in smaller csv chunks in partitioned directories (e.g. `date=2024-01-01/`). Rather than reading the data with a for loop (and turning the partitioning directories into variables), `nomad`'s `from_file` can read a whole directory with the same function call, by warpping `PyArrow`'s file reader.\n",
    "\n",
    "Let's replicate the previous analysis starting with `nomad`'s file reader on the partitioned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf535a-b1f7-41e2-993c-56c40c768b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the single csv dataset\n",
    "traj_cols = {\"user_id\": \"identifier\",\n",
    "             \"timestamp\": \"unix_timestamp\",\n",
    "             \"latitude\": \"device_lat\",\n",
    "             \"longitude\": \"device_lon\",\n",
    "             \"datetime\": \"local_datetime\",\n",
    "             \"date\": \"date\"}\n",
    "file_path = \"gc_data.csv\"\n",
    "\n",
    "df = loader.from_file(file_path, format=\"csv\", traj_cols=traj_cols)\n",
    "# check data types\n",
    "#print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33de9d2-ee49-46ac-96a1-56784674d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the partitioned dataset\n",
    "traj_cols = {\"user_id\": \"user_id\",\n",
    "             \"timestamp\": \"timestamp\",\n",
    "             \"latitude\": \"latitude\",\n",
    "             \"longitude\": \"longitude\",\n",
    "             \"datetime\": \"datetime\",\n",
    "             \"date\": \"date\"} # the dataset has default column names\n",
    "\n",
    "file_path = \"gc_data/\" # partitioned\n",
    "\n",
    "# Try traj_cols=None. It should work because of default names\n",
    "# Try mixed_timezone_behavior=\"utc\" or \"object\", or parse_dates = False and inspect df\n",
    "df = loader.from_file(file_path, format=\"csv\", traj_cols=traj_cols, parse_dates=True)\n",
    "# check data types\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d82bfd-b126-4346-81f7-e0b738ccfe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute all three statistics as before \n",
    "\n",
    "# Trajectory of a single user\n",
    "user = df[traj_cols['user_id']].iloc[0]\n",
    "user_df = df.loc[(df[traj_cols['user_id']] == user) & (df[traj_cols['date']] == '2024-01-04')]\n",
    "\n",
    "# Pings per cell geodataframe # TO DO: filters.to_h3\n",
    "df[\"cell\"] = df.apply(\n",
    "    lambda r: h3.latlng_to_cell(lat=r[traj_cols[\"latitude\"]], lng=r[traj_cols[\"longitude\"]], res=12),\n",
    "    axis=1)\n",
    "\n",
    "pings_per_cell = df.groupby('cell').agg(pings=(traj_cols['timestamp'], 'count')).reset_index()\n",
    "h3_gdf = gpd.GeoDataFrame(pings_per_cell, geometry=pings_per_cell['cell'].apply(h3_cell_to_polygon))\n",
    "h3_gdf = h3_gdf.loc[h3_gdf.pings>5]\n",
    "\n",
    "# Maximum gap for each user\n",
    "max_gap = df.groupby(traj_cols['user_id'])[traj_cols['timestamp']].apply(get_max_gap_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b55ae-51cf-4e89-bf53-918ee80b735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(figsize=(14,4), ncols=3, width_ratios=[4.5, 7, 4.5])\n",
    "\n",
    "# trajectory of a single user\n",
    "city.plot(ax=ax1, column='type', edgecolor='black', linewidth=0.75, cmap='Set3') \n",
    "ax1.scatter(user_df[traj_cols[\"longitude\"]], user_df[traj_cols[\"latitude\"]], s=6, alpha=0.75, color='black') # WHY NO PINGS!\n",
    "ax1.set_axis_off()\n",
    "# heatmap\n",
    "city.plot(ax=ax2, column='type', edgecolor='black', linewidth=0.75, cmap='Set3')\n",
    "h3_gdf.plot(column=h3_gdf.pings, cmap='Reds', alpha=0.75, ax=ax2, legend=True, legend_kwds={'shrink': 0.5})\n",
    "ax2.set_axis_off()\n",
    "# gaps\n",
    "max_gap.hist(ax=ax3, bins=24, color='#8dd3c7')\n",
    "ax3.set_xlabel('minutes')\n",
    "ax3.set_title('max temporal gap per user')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4514fdd-d74a-4c69-bb68-73a091292d52",
   "metadata": {},
   "source": [
    "Now, go back to (hidden) [cell 7](#hidden-cell) and try simply changing the file path and column name mapping (traj_cols). The rest of the code works the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbfe5f4-c481-4816-8136-bca26e48cf3c",
   "metadata": {},
   "source": [
    "## A good practice: prototype on a small sample, scale up later\n",
    "\n",
    "While a researcher is still exploring and designing their experiments, it can be impractical, time consuming, or outright intractable, to use the entire dataset. Thus, it is recommended to work on a sample of the data, either by selecting some users at random, some records at random, or both. `nomad`'s `io.base.sample_users` selects a reproducible subset of user IDs, while `io.base.sample_from_file` reads the data of only those users, optionally sampling records. The resulting sample can be written to disk using `io.base.to_file`. \n",
    "\n",
    "When persisting the sample, we partition by `date` again to preserve the likeness with the original dataset. Reading the output back with `from_file` confirms that the sample was saved correctly and remains compatible with the same loading functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565115f0-1e3c-4b34-8eca-3abeb1695f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = \"gc_data/\" # has default names\n",
    "fmt = \"csv\"\n",
    "\n",
    "# full data\n",
    "df = loader.from_file(file_path, format=fmt)\n",
    "\n",
    "# sample users\n",
    "users = loader.sample_users(file_path, format=fmt, size=12, seed=314) # change if user_id has other name\n",
    "# sample data, pass users\n",
    "sample_df = loader.sample_from_file(file_path, users=users, format=fmt, frac_records=0.30, seed=314)\n",
    "\n",
    "## optionally try uncommenting this line\n",
    "# sample_df = loader.sample_from_file(file_path, frac_users=0.2, format=fmt, frac_records=0.30, frac_users=0.12, seed=314)\n",
    "\n",
    "# persist to parquet\n",
    "\n",
    "loader.to_file(sample_df, \"/tmp/nomad_sample2/\", format=fmt, partition_by=[\"date\"], existing_data_behavior='overwrite_or_ignore')\n",
    "round_trip = loader.from_file(\"/tmp/nomad_sample2/\", format=fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd42c69-c932-428e-ab9c-30b46a1aecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- Value counts for sample of data:\\n\")\n",
    "print(round_trip.user_id.value_counts())\n",
    "print(\"\\n---------------------------------\\n\")\n",
    "print(\"- Value counts for original data:\\n\")\n",
    "print(df.user_id.value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (daphme)",
   "language": "python",
   "name": "daphme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "conda_py_310_env"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      },
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "conda_py_310_env"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    },
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 5,
           "op": "addrange",
           "valuelist": "6"
          },
          {
           "key": 5,
           "length": 2,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3 (ipykernel)"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      },
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "python3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    },
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
