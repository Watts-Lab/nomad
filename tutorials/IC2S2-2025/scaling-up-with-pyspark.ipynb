{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b2c711-25cc-4f00-9012-d2b9f3f798b2",
   "metadata": {},
   "source": [
    "# **Tutorial 5: Scaling up with Pyspark** \n",
    "\n",
    "In this section of the tutorial, we demonstrate how to process a much larger mobility dataset using nomad's software in a Spark cluster. Our target application will be to produce mobility metrics, aggregated at the neighborhood level. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb5fa2-d3ed-4b0c-9807-eb6fd97c626d",
   "metadata": {},
   "source": [
    "## Configure your Spark cluster with SparkMagic\n",
    "\n",
    "The EMR cluster for this demonstration has 1 master node (`m5.xlarge`, 4 vCPU, 16 GiB RAM) and 4 core nodes (`c5.4xlarge`, each with 16 vCPU and 32 GiB RAM). That gives us a total of 48 vCPUs across the workers.\n",
    "\n",
    "These resources are divided between a **driver** and multiple **executors**:\n",
    "- This jupyter notebook is inside the **driver** (or master). It coordinates the jobs, helps shuffle data around, and collects results.\n",
    "\n",
    "- The **executors** (or slaves) are distributed processes that perform actual computations on the worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f0cd2-d17f-4df9-8d04-a01db49206a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"conf\":\n",
    "     {\"spark.pyspark.python\":\"/home/hadoop/nomad-venv/bin/python3\",\n",
    "      \"spark.pyspark.virtualenv.bin.path\":\"/home/hadoop/nomad-venv/bin\",\n",
    "      \"spark.driver.memory\": \"6g\",\n",
    "      \"spark.driver.maxResultSize\": \"4g\",\n",
    "      \"spark.executor.memory\": \"4900m\",\n",
    "      \"spark.executor.cores\": \"4\", \n",
    "      \"spark.dynamicAllocation.enabled\": \"true\",\n",
    "      \"spark.dynamicAllocation.minExecutors\": \"4\",\n",
    "      \"maximizeResourceAllocation\": \"true\",\n",
    "      \"spark.sql.execution.arrow.pyspark.enabled\": \"true\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d8655-7cd2-459b-acd2-4c6e2e4403e9",
   "metadata": {},
   "source": [
    "## A not-that-off-topic example: count intersecting time intervals\n",
    "\n",
    "A large number of events := `(event_id, start_datetime, end_datetime)` need to be compared to find the pairs that intersect. \n",
    "- Events are all under 15 min long\n",
    "- Simple sorting can be a bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081369dc-5608-47a8-b219-35244c69324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30a77d4-ce38-4c21-b11e-f59cbb24783a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N        = 1_500_000  # records\n",
    "parts    = 200  # initial partition of the data\n",
    "DUR_MAX  = 900  # <= 15 min\n",
    "totally_normal_day  = int(dt.datetime(2021, 1, 6).timestamp())\n",
    "sec_in_5_weeks   = 5 * 7 * 24 * 3600  # 5â€¯weeks\n",
    "\n",
    "events = (spark.range(N, numPartitions=parts)\n",
    "          .withColumn(\"offset\", (F.rand() * sec_in_5_weeks).cast(\"int\"))\n",
    "          .withColumn(\"duration\", (F.rand() * DUR_MAX + 1).cast(\"int\"))\n",
    "          .withColumn(\"start\", F.from_unixtime(totally_normal_day + F.col(\"offset\")).cast(\"timestamp\"))\n",
    "          .withColumn(\"end\",   F.from_unixtime(totally_normal_day + F.col(\"offset\") + F.col(\"duration\")).cast(\"timestamp\"))\n",
    "          .select(\"id\", \"start\", \"end\")\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e86ff-ac12-4e1b-993e-1a2e1e9e09f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "events.show(5, truncate=False)\n",
    "print(f\"Initial partitions: {events.rdd.getNumPartitions()}\")\n",
    "\n",
    "print(\"Rows in first ten partitions:\",\n",
    "      events.rdd.mapPartitions(lambda it: [sum(1 for _ in it)]).take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf4144-2a64-4989-9c50-40c8a8aab8f4",
   "metadata": {},
   "source": [
    "### How to parallelize counting overlaps? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d10908-7fc9-484b-9e3a-860046606b54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### [click to reveal]\n",
    "bucketed = (events\n",
    "            .withColumn(\"start_bucket\", # 30 minute interval of the start time\n",
    "                        F.floor(F.col(\"start\").cast(\"int\")/1800)) # 30 minute buckets\n",
    "            .withColumn(\"bucket\",\n",
    "                        F.explode(\n",
    "                            F.sequence(\n",
    "                                F.col(\"start_bucket\"),\n",
    "                                F.floor(F.col(\"end\").cast(\"int\")/1800)\n",
    "                            ))))\n",
    "\n",
    "\n",
    "# No execution yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19db2baa-095f-4f2c-bd7d-00e8a0f259c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### [click to reveal]\n",
    "bucketed.groupby(\"bucket\").count().show(10) # triggers execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d38e7a4-3896-4fe3-baea-fb39abf7514b",
   "metadata": {},
   "source": [
    "### *Pandas user defined functions (pandas_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1f6ec9-7277-4b41-a92e-a9c97f879e47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "def count_overlaps(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    pdf = pdf.sort_values(\"start\")\n",
    "    starts      = pdf[\"start\"].values\n",
    "    ends        = pdf[\"end\"].values\n",
    "    start_bs    = pdf[\"start_bucket\"].values\n",
    "    cur_bucket  = pdf[\"bucket\"].iat[0]\n",
    "    \n",
    "    cnt = 0\n",
    "    n   = len(starts)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if starts[j] > ends[i]:\n",
    "                break\n",
    "            # only count if this bucket is the \"canonical\" one\n",
    "            if cur_bucket == max(start_bs[i], start_bs[j]):\n",
    "                cnt += 1\n",
    "    \n",
    "    return pd.DataFrame({\"cnt\": [cnt]})\n",
    "\n",
    "# applyInPandas and sum across buckets\n",
    "overlap_counts = (\n",
    "    bucketed\n",
    "      .groupby(\"bucket\")\n",
    "      .applyInPandas(count_overlaps, schema=\"cnt long\")\n",
    ")\n",
    "\n",
    "\n",
    "total = overlap_counts.select(F.sum(\"cnt\")).collect()[0][0]\n",
    "\n",
    "print(\"distinct overlap pairs =\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13465253-8511-4b2d-bfc9-57a96a2f1981",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_counts.toPandas() # Will trigger all the execution again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda04fd3-977c-45b9-b8e7-d13eabaa6ae0",
   "metadata": {},
   "source": [
    "# Large scale mobility dataset (Philadelphia, PA, USA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d7167d-fb55-4b3d-a8fe-3bb0e91ba29f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nomad.io.spark import table_columns\n",
    "\n",
    "data_path = \"s3://catalog-csslab/tutorial-large-data/\"\n",
    "table_columns(data_path, include_schema=False) # try True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c748700-b9b0-403a-8c5f-df2d312c227b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = spark.read.parquet(data_path)\n",
    "data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b53aaa-53c8-4f71-98be-8a50cb254699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.select(F.min(data[\"date\"]), F.max(data[\"date\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835d25b4-aead-46b8-be40-dd2e2811c7e7",
   "metadata": {},
   "source": [
    "## How many users and records are there in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e7096-9ead-4699-85c5-b21065e40736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "approx_total_records = data.rdd.countApprox(timeout=100,\n",
    "                                                confidence=0.80) \n",
    "# better than data.count( ).collect()[0]\n",
    "print(f\"Approximate total records: {approx_total_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d6c54f-527c-405f-be00-19f8cf1d8687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "approx_num_users = (\n",
    "    data\n",
    "    .agg(F.approx_count_distinct(\"user_id\", rsd=0.15).alias(\"approx_num_users\"))\n",
    "    .collect()[0][\"approx_num_users\"]\n",
    ")\n",
    "# better than count_distinct()\n",
    "print(f\"Approximate unique devices (user_id): {approx_num_users}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04210954-ade1-49d7-a66f-1c51cbea673e",
   "metadata": {},
   "source": [
    "### We will focus on a smaller box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ec88f-4f18-41ce-bbfb-8597e410b380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import shapely as shp\n",
    "from shapely.geometry import box, LineString, Point\n",
    "import contextily as cx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# center city Philadelphia\n",
    "bbox = box(-75.1680, 39.9400, -75.1440, 39.9557)\n",
    "old_city = gpd.GeoSeries([bbox], crs=\"EPSG:4326\").to_crs(\"EPSG:3857\").iloc[0]\n",
    "\n",
    "cbgs = gpd.read_file(\"s3://ic2s2-emr-setup/tutorial-notebooks/Census_Block_Groups_2010.geojson\").to_crs(\"EPSG:3857\")\n",
    "ax = cbgs.clip(old_city).plot(figsize=(4, 4), alpha=0.4, facecolor=\"none\", linewidth=2)\n",
    "ax.set_axis_off()\n",
    "cx.add_basemap(ax, source=cx.providers.CartoDB.Positron)\n",
    "\n",
    "plt.title(\"Centery City, Philadelphia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71763718-e509-4d3d-be4d-d910c15348ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplot plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c774336b-4bf4-4567-8ef7-e372f386289d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nomad.filters import completeness\n",
    "\n",
    "@F.pandas_udf(\"double\")\n",
    "def completeness_udf(local_dt: pd.Series) -> float:\n",
    "    # local_dt is all the local_datetime values for one user\n",
    "    return float(completeness(\n",
    "        data=pd.to_datetime(local_dt, utc=False),\n",
    "        periods=1,\n",
    "        freq='d',\n",
    "        start=\"2020-02-01\",\n",
    "        end=\"2020-05-01\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1136343-2cac-42b9-928c-28bcfc4e865e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_x, min_y, max_x, max_y = old_city.bounds\n",
    "date_from, date_to = [F.to_date(F.lit(s)) for s in dates]\n",
    "filtered = (\n",
    "    data\n",
    "    .filter((F.col(\"x\") >= min_x) & (F.col(\"x\") <= max_x))\n",
    "    .filter((F.col(\"y\") >= min_y) & (F.col(\"y\") <= max_y))\n",
    "    .filter(F.col(\"date\").between(\"2020-02-01\", \"2020-05-01\"))\n",
    "    .groupBy(\"user_id\")\n",
    "      .agg(completeness_udf(\"local_datetime\").alias(\"completeness\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc75fa-c6ab-488d-98e7-7a417043087f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "daily_q = filtered.select(filtered.completeness).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba9293b-ec4a-4e0b-b733-8639c8796ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "ax1.hist(daily_q, bins=40)\n",
    "ax1.set_title('Completeness (d) restricted to Center City')\n",
    "ax1.set_ylabel('Number of users')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5939e649-8e0d-421d-a95c-9921c27572e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fe9ac4-65a8-437c-a4a6-5d133523bf1a",
   "metadata": {},
   "source": [
    "## Let's persist the final sample of data we will work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c500ab05-d4a0-4d22-bbfd-babbc84035a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_users = (\n",
    "    filtered\n",
    "      .filter(F.col(\"completeness\") > 0.1)\n",
    "      .select(\"user_id\")\n",
    ")\n",
    "\n",
    "sample_data = (\n",
    "    data\n",
    "      .filter(F.col(\"date\").between(\"2020-02-01\", \"2020-05-01\"))\n",
    "      .join(sample_users, on=\"user_id\", how=\"inner\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f6e8a-601b-4049-8e73-2eafbd7d27b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_path = \"/tmp/temp_data/\"\n",
    "(\n",
    "    sample_data.write\n",
    "      .option(\"hiveStylePartitioning\", \"true\")\n",
    "      .partitionBy(\"date\")\n",
    "      .mode(\"overwrite\")\n",
    "      .parquet(out_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f54d7df-923b-4397-9e75-9d26e29e973d",
   "metadata": {},
   "source": [
    "## **Radius of gyration** based on stops for this small sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503cd2dd-a2ec-4f79-be69-5bdb29a17653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = spark.read.parquet(out_path)\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af0c9f-3900-4ba2-bd45-31eeccd6bbd4",
   "metadata": {},
   "source": [
    "### Like previously, wrap the stop_detection function in a pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf481ca-4e65-470d-a4cf-3f3a74cd68a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nomad.stop_detection.lachesis import lachesis\n",
    "\n",
    "def _lachesis(pdf):\n",
    "    pdf = pdf.sort_values(by=['unix_timestamp'])\n",
    "    stops = lachesis(pdf, dt_max= 240, delta_roam=35,\n",
    "        complete_output = True, keep_col_names=False, timestamp = 'unix_timestamp',\n",
    "        x=\"x\", y=\"y\", user_id=\"user_id\", passthrough_cols=['tz_offset', 'local_datetime'])    \n",
    "    \n",
    "    schema_cols = [\"user_id\",\"start_timestamp\", \"end_timestamp\",\n",
    "                   \"x\", \"y\",\"n_pings\", \"max_gap\", \"duration\",\n",
    "                   \"cluster\",\"diameter\",\"local_datetime\", \"tz_offset\"]\n",
    "    if stops.empty:\n",
    "        pd.DataFrame(columns=schema_cols, dtype=object)\n",
    "    else:\n",
    "        return stops\n",
    "\n",
    "# For grouped map udfs, the syntax uses applyInPandas on a regular pandas function\n",
    "schema = (\n",
    "    f\"user_id string, \"\n",
    "    \"start_timestamp long, end_timestamp long, \"\n",
    "    \"x double, y double, \"\n",
    "    \"n_pings long, max_gap long, \"\n",
    "    \"duration long, cluster long, \"\n",
    "    \"diameter float, \"\n",
    "    \"local_datetime string, tz_offset long\"\n",
    ")\n",
    "\n",
    "stops_data = (\n",
    "    data\n",
    "    .groupBy('user_id')\n",
    "    .applyInPandas(_lachesis, schema)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af46244f-b1ca-484c-8767-cf02c35c4b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_path = \"/tmp/temp_stops/\"\n",
    "(\n",
    "    stops_data.write\n",
    "      .option(\"hiveStylePartitioning\", \"true\")\n",
    "      .mode(\"overwrite\")\n",
    "      .parquet(out_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef7b0f-674f-41cf-9a75-132221aa772f",
   "metadata": {},
   "source": [
    "### Next we compute the radius of gyration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785f5988-d481-4c48-a7ed-19a7336c59ab",
   "metadata": {},
   "source": [
    "At this point the data is much smaller, and we can uncomplicate our lives by simply parallelizing a single pandas_udf computing RoGs and aggregating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e3c1aa-1ccf-4be9-9f97-b58ef67af598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nomad.io.base as loader\n",
    "traj_cols = {\"datetime\":\"local_datetime\", \"user_id\":\"user_id\", \"timestamp\":\"unix_timestamp\"}\n",
    "loader.sample_from_file(out_path, format=\"parquet\", frac_users=0.2, traj_cols=traj_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696b4e0a-3181-4159-b1f6-2e64ed35fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbgs = gpd.read_file(\"s3://ic2s2-emr-setup/tutorial-notebooks/Census_Block_Groups_2010.geojson\").to_crs(\"EPSG:3857\")\n",
    "cbgs = cbgs.rename(columns={\"GEOID10\":\"cbg\"})\n",
    "cbgs = cbgs.set_index(\"cbg\", drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f1ab18-b21d-4025-9887-a8614cd2fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops[\"cbg\"] = visits.point_in_polygon(\n",
    "                         data=stops,\n",
    "                         poi_table=cbgs,\n",
    "                         max_distance=0,\n",
    "                         x='x',\n",
    "                         y='y',\n",
    "                         method='centroid',\n",
    "                         data_crs='EPSG:3857')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa318230-7fe3-47db-8f53-c99aa8a5635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_homes = homes.compute_candidate_homes(stops,\n",
    "                                           datetime=\"local_datetime\",\n",
    "                                           location_id=\"cbgs\",\n",
    "                                           user_id=\"user_id\")\n",
    "\n",
    "last_date = date(year=2020, month=6, day=1) \n",
    "home_table = homes.select_home(cand_homes, min_days=3, min_weeks=2, last_date=last_date, user_id='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed10178-6a81-4de0-9e97-3882740076b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute ROG join with home_table[[\"user_id\", \"cbg\"]] on 'user_id'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
